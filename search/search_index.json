{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PrismToolBox Documentation PrismToolBox is a comprehensive Python library for histopathology image analysis, providing powerful tools for processing whole slide images (WSI), feature extraction, and nuclei segmentation. \ud83d\udd2c What is PrismToolBox? PrismToolBox is designed for researchers and practitioners working with digital pathology images. It offers a complete pipeline for: WSI Preprocessing : Handle tissue contouring, patch extraction, and visualization Feature Extraction : Extract embeddings from pretrained models from slide or patch datasets Nuclei Segmentation : Advanced deep learning models for cell detection and segmentation \ud83d\udee0\ufe0f Installation Basic Installation pip install prismtoolbox Installation with Optional Dependencies pip install prismtoolbox [ emb, seg ] Development Installation git clone https://github.com/gustaveroussy/PrismToolBox.git cd PrismToolBox pip install -e . \ud83d\ude80 Key Features Core Modules wsicore : Core functionality for WSI handling and preprocessing wsiemb : Feature extraction nucleiseg : Deep learning-based nuclei segmentation Command Line Interface PrismToolBox comes with a powerful CLI that makes it easy to process large datasets: # Use the CLI ptb --help \ud83d\udccb Quick Start Python API from prismtoolbox import WSI # Initialize the reader WSI_object = WSI ( slide_path = \"path_to_your_slide\" , engine = \"openslide\" ) # Extract tissue contours params_detect_tissue = { \"seg_level\" : 2 , \"window_avg\" : 30 , \"window_eng\" : 3 , \"thresh\" : 120 , \"area_min\" : 6e3 } WSI_object . detect_tissue ( ** params_detect_tissue ) # Extract patches and save them as jpg images params_patches = { \"patch_size\" : 256 , \"patch_level\" : 0 , \"overlap\" : 0 , \"contours_mode\" : \"four_pt\" } WSI_object . extract_patches ( ** params_patches ) WSI_object . save_patches ( \"path_to_folder\" , file_format = \"jpg\" ) Command Line Interface # Extract tissue contours ptb preprocessing contouring slides/ results/ --visualize # Extract patches ptb preprocessing patching slides/ results/ --contours-directory results/contours/ \ud83d\udcda Documentation Structure API Reference : Detailed documentation of all classes and functions CLI Reference : Complete guide to the command-line interface Examples : Practical examples and tutorials \ud83e\udd1d Contributing We welcome contributions! Please see our contribution guidelines for more information. \ud83d\udcc4 License This project is licensed under the BSD-3-Clause license - see the LICENSE file for details. \ud83d\udd17 Links GitHub Repository PyPI Package Issue Tracker","title":"Home"},{"location":"#prismtoolbox-documentation","text":"PrismToolBox is a comprehensive Python library for histopathology image analysis, providing powerful tools for processing whole slide images (WSI), feature extraction, and nuclei segmentation.","title":"PrismToolBox Documentation"},{"location":"#what-is-prismtoolbox","text":"PrismToolBox is designed for researchers and practitioners working with digital pathology images. It offers a complete pipeline for: WSI Preprocessing : Handle tissue contouring, patch extraction, and visualization Feature Extraction : Extract embeddings from pretrained models from slide or patch datasets Nuclei Segmentation : Advanced deep learning models for cell detection and segmentation","title":"\ud83d\udd2c What is PrismToolBox?"},{"location":"#installation","text":"","title":"\ud83d\udee0\ufe0f Installation"},{"location":"#basic-installation","text":"pip install prismtoolbox","title":"Basic Installation"},{"location":"#installation-with-optional-dependencies","text":"pip install prismtoolbox [ emb, seg ]","title":"Installation with Optional Dependencies"},{"location":"#development-installation","text":"git clone https://github.com/gustaveroussy/PrismToolBox.git cd PrismToolBox pip install -e .","title":"Development Installation"},{"location":"#key-features","text":"","title":"\ud83d\ude80 Key Features"},{"location":"#core-modules","text":"wsicore : Core functionality for WSI handling and preprocessing wsiemb : Feature extraction nucleiseg : Deep learning-based nuclei segmentation","title":"Core Modules"},{"location":"#command-line-interface","text":"PrismToolBox comes with a powerful CLI that makes it easy to process large datasets: # Use the CLI ptb --help","title":"Command Line Interface"},{"location":"#quick-start","text":"","title":"\ud83d\udccb Quick Start"},{"location":"#python-api","text":"from prismtoolbox import WSI # Initialize the reader WSI_object = WSI ( slide_path = \"path_to_your_slide\" , engine = \"openslide\" ) # Extract tissue contours params_detect_tissue = { \"seg_level\" : 2 , \"window_avg\" : 30 , \"window_eng\" : 3 , \"thresh\" : 120 , \"area_min\" : 6e3 } WSI_object . detect_tissue ( ** params_detect_tissue ) # Extract patches and save them as jpg images params_patches = { \"patch_size\" : 256 , \"patch_level\" : 0 , \"overlap\" : 0 , \"contours_mode\" : \"four_pt\" } WSI_object . extract_patches ( ** params_patches ) WSI_object . save_patches ( \"path_to_folder\" , file_format = \"jpg\" )","title":"Python API"},{"location":"#command-line-interface_1","text":"# Extract tissue contours ptb preprocessing contouring slides/ results/ --visualize # Extract patches ptb preprocessing patching slides/ results/ --contours-directory results/contours/","title":"Command Line Interface"},{"location":"#documentation-structure","text":"API Reference : Detailed documentation of all classes and functions CLI Reference : Complete guide to the command-line interface Examples : Practical examples and tutorials","title":"\ud83d\udcda Documentation Structure"},{"location":"#contributing","text":"We welcome contributions! Please see our contribution guidelines for more information.","title":"\ud83e\udd1d Contributing"},{"location":"#license","text":"This project is licensed under the BSD-3-Clause license - see the LICENSE file for details.","title":"\ud83d\udcc4 License"},{"location":"#links","text":"GitHub Repository PyPI Package Issue Tracker","title":"\ud83d\udd17 Links"},{"location":"api_index/","text":"API Reference Welcome to the PrismToolBox API documentation. This section provides comprehensive reference for all modules, classes, and functions in the library. Core Modules \ud83d\udd2c WSI Core ( prismtoolbox.wsicore ) The core module for whole slide image handling and preprocessing. Main Classes: - WSI - Primary class for WSI operations, tissue detection, patch extraction, and visualization Key Functionality: - Slide loading and multi-resolution access - Tissue contour detection and analysis - ROI (Region of Interest) management - Patch extraction with various modes - Visualization and stitching capabilities - QuPath integration for annotations \ud83e\udde0 WSI Embeddings ( prismtoolbox.wsiemb ) Feature extraction and embedding generation from WSI patches. Main Classes: - SlideEmbedder - Extract embeddings from slide patches using pretrained models - PatchEmbedder - Extract embeddings from patch datasets - EmbeddingProcessor - Process, analyze, and visualize embeddings Key Functionality: - Model-based embeddings (ResNet, Vision Transformers, Foundation models) - Stain-based feature extraction (color deconvolution) - Cell-based feature extraction (morphological features) - Dimensionality reduction and clustering - Visualization and analysis tools \ud83d\udd2c Nuclei Segmentation ( prismtoolbox.nucleiseg ) Deep learning-based nuclei segmentation and analysis. Main Classes: - NucleiSegmenter - Segment nuclei in WSI patches using deep learning models Key Functionality: - Multiple segmentation models (SOP, custom models) - Batch processing of slide patches - Post-processing and conflict resolution - QuPath export for visualization","title":"Overview"},{"location":"api_index/#api-reference","text":"Welcome to the PrismToolBox API documentation. This section provides comprehensive reference for all modules, classes, and functions in the library.","title":"API Reference"},{"location":"api_index/#core-modules","text":"","title":"Core Modules"},{"location":"api_index/#wsi-core-prismtoolboxwsicore","text":"The core module for whole slide image handling and preprocessing. Main Classes: - WSI - Primary class for WSI operations, tissue detection, patch extraction, and visualization Key Functionality: - Slide loading and multi-resolution access - Tissue contour detection and analysis - ROI (Region of Interest) management - Patch extraction with various modes - Visualization and stitching capabilities - QuPath integration for annotations","title":"\ud83d\udd2c WSI Core (prismtoolbox.wsicore)"},{"location":"api_index/#wsi-embeddings-prismtoolboxwsiemb","text":"Feature extraction and embedding generation from WSI patches. Main Classes: - SlideEmbedder - Extract embeddings from slide patches using pretrained models - PatchEmbedder - Extract embeddings from patch datasets - EmbeddingProcessor - Process, analyze, and visualize embeddings Key Functionality: - Model-based embeddings (ResNet, Vision Transformers, Foundation models) - Stain-based feature extraction (color deconvolution) - Cell-based feature extraction (morphological features) - Dimensionality reduction and clustering - Visualization and analysis tools","title":"\ud83e\udde0 WSI Embeddings (prismtoolbox.wsiemb)"},{"location":"api_index/#nuclei-segmentation-prismtoolboxnucleiseg","text":"Deep learning-based nuclei segmentation and analysis. Main Classes: - NucleiSegmenter - Segment nuclei in WSI patches using deep learning models Key Functionality: - Multiple segmentation models (SOP, custom models) - Batch processing of slide patches - Post-processing and conflict resolution - QuPath export for visualization","title":"\ud83d\udd2c Nuclei Segmentation (prismtoolbox.nucleiseg)"},{"location":"cli/preprocessing/","text":"CLI Reference: Preprocessing The PrismToolBox CLI provides useful preprocessing capabilities for whole slide images through the ptb preprocessing command. Overview The preprocessing module includes two main commands: contour : Extract tissue contours from whole slide images patchify : Extract patches from slides using tissue contours Installation Make sure you have PrismToolBox installed: # Basic installation pip install prismtoolbox Global Options All preprocessing commands support these global options: --verbose, -v : Increase verbosity (can be used multiple times: -v , -vv ) --help : Show help message Commands ptb preprocessing contour Extract tissue contours from whole slide images. Usage ptb preprocessing contour [ OPTIONS ] SLIDE_DIRECTORY RESULTS_DIRECTORY Arguments SLIDE_DIRECTORY : Path to the directory containing the slide files RESULTS_DIRECTORY : Path to the directory where the results will be saved Options Option Type Description Default --engine str Engine for reading slides ( openslide , tiffslide ). openslide --annotations-directory str | None Path to annotations directory None --contours-exts list[str] File extensions for contour annotations ( geojson , pickle ) [pickle] --config-file str Path to configuration file None --visualize bool Visualize the extracted contours False Configuration File You can use a YAML configuration file to specify tissue extraction and visualization parameters: # Default configuration for PrismToolBox contouring # Tissue contour extraction parameters contour_settings : seg_level : 4 # (int) Segmentation level for the tissue contour extraction. window_avg : 30 # (int) Size of the window average for tissue extraction. window_eng : 5 # (int) Size of the window to use for computing energy for tissue extraction. thresh : 190 # (int) Threshold for the tissue extraction algorithm. area_min : 50000 # (int) Minimum area for the tissue contour. # Tissue visualization parameters visualization_settings : vis_level : 4 # (int) Visualization level for the tissue contour extraction. number_contours : false # (bool) Plot the id number for each contour. line_thickness : 50 # (bool) Line thickness for the contour visualization. Examples # Basic contour extraction ptb preprocessing contour slides/ results/ # With visualization ptb preprocessing contour slides/ results/ --visualize # Using custom configuration ptb preprocessing contour slides/ results/ --config-file custom_config.yaml # With annotations and multiple output formats ptb preprocessing contour slides/ results/ --annotations-directory annotations/ --contours-exts pickle geojson --visualize ptb preprocessing patchify Extract patches from slides using tissue contours. Usage ptb preprocessing patchify [ OPTIONS ] SLIDE_DIRECTORY RESULTS_DIRECTORY Arguments SLIDE_DIRECTORY : Path to the directory containing the slide files RESULTS_DIRECTORY : Path to the directory where the results will be saved Options Option Type Description Default --roi-csv str | None Path to the csv file containing the ROIs None --contours-directory str | None Path to directory containing contour annotations None --engine str Engine for reading slides openslide --mode str Extraction mode ( contours , roi , all ) contours --patch-exts list[str] File extensions for patches ( h5 , geojson ) [h5] --config-file str | None Path to configuration file None Configuration File Example configuration for patch extraction: # Default configuration for PrismToolBox patching # Patch extraction parameters patch_settings : patch_level : 0 # (float) Level of the slide to extract patches from. patch_size : 256 # (float) Size of the patches to extract. overlap : 0 # (float) Overlap between the patches. units : [ \"px\" , \"px\" ] # (str, str) Units for the patch size and overlap. Options are 'pixels' or 'micro' for micrometers. contours_mode : \"four_pt\" # (str) The mode to use for the contour checking. Possible values are center, four_pt, and four_pt_hard. rgb_threshs : [ 2 , 240 ] # (int, int) The thresholds for the RGB channels (black threshold, white threshold). percentages : [ 0.6 , 0.9 ] # (float, float) The percentages of pixels below/above the thresholds to consider the patch as black/white. # Patch stitching parameters stitching_settings : vis_level : 4 # (int) Level of the slide to stitch the patches at. draw_grid : false # (bool) Whether to draw a grid on the stitched image. Examples # Basic patch extraction ptb preprocessing patchify slides/ results/ # Patch extraction within a ROI ptb preprocessing patchify slides/ results/ --mode roi --roi-directory results/rois.csv # Within previously extracted tissue contours and custom configuration ptb preprocessing patchify slides/ results/ --mode contours --contours-directory results/contours/ --config-file patch_config.yaml # Extract patches in multiple formats ptb preprocessing patchify slides/ results/ --contours-directory results/contours/ --patch-exts h5 geojson Attention: For the roi mode, you need to provide a table with the ROIs in a CSV format, where each row corresponds to a slide and contains the slide ID and coordinates of the ROI. # Extract patches from a specific ROI ptb preprocessing patchify slides/ results/ --mode roi --roi-csv results/rois.csv Complete Workflow Example Here's a complete example of processing a dataset: # Step 1: Extract tissue contours with visualization ptb preprocessing contouring slides/ results/ --visualize --config-file tissue_config.yaml # Step 2: Extract patches from the contours ptb preprocessing patching slides/ results/ --contours-directory results/contours/ --config-file patch_config.yaml --patch-exts geojson Results will be saved in: - results/contours/ (tissue contours as pickle files) - results/contoured_images/ (visualizations) - results/patches_256_ovelap_0/ (extracted patches as geojson coordinates) - results/stitched_images_256_ovelap_0/ (patch visualizations) Tips and Best Practices Start with small datasets : Process a few slides first to validate your parameters Use visualizations : Use --visualize flag to check if tissue detection works correctly, and --stitch to visualize the selected patches. Monitor output : Use verbose mode ( -v or -vv ) to see detailed processing information","title":"Preprocessing"},{"location":"cli/preprocessing/#cli-reference-preprocessing","text":"The PrismToolBox CLI provides useful preprocessing capabilities for whole slide images through the ptb preprocessing command.","title":"CLI Reference: Preprocessing"},{"location":"cli/preprocessing/#overview","text":"The preprocessing module includes two main commands: contour : Extract tissue contours from whole slide images patchify : Extract patches from slides using tissue contours","title":"Overview"},{"location":"cli/preprocessing/#installation","text":"Make sure you have PrismToolBox installed: # Basic installation pip install prismtoolbox","title":"Installation"},{"location":"cli/preprocessing/#global-options","text":"All preprocessing commands support these global options: --verbose, -v : Increase verbosity (can be used multiple times: -v , -vv ) --help : Show help message","title":"Global Options"},{"location":"cli/preprocessing/#commands","text":"","title":"Commands"},{"location":"cli/preprocessing/#ptb-preprocessing-contour","text":"Extract tissue contours from whole slide images.","title":"ptb preprocessing contour"},{"location":"cli/preprocessing/#usage","text":"ptb preprocessing contour [ OPTIONS ] SLIDE_DIRECTORY RESULTS_DIRECTORY","title":"Usage"},{"location":"cli/preprocessing/#arguments","text":"SLIDE_DIRECTORY : Path to the directory containing the slide files RESULTS_DIRECTORY : Path to the directory where the results will be saved","title":"Arguments"},{"location":"cli/preprocessing/#options","text":"Option Type Description Default --engine str Engine for reading slides ( openslide , tiffslide ). openslide --annotations-directory str | None Path to annotations directory None --contours-exts list[str] File extensions for contour annotations ( geojson , pickle ) [pickle] --config-file str Path to configuration file None --visualize bool Visualize the extracted contours False","title":"Options"},{"location":"cli/preprocessing/#configuration-file","text":"You can use a YAML configuration file to specify tissue extraction and visualization parameters: # Default configuration for PrismToolBox contouring # Tissue contour extraction parameters contour_settings : seg_level : 4 # (int) Segmentation level for the tissue contour extraction. window_avg : 30 # (int) Size of the window average for tissue extraction. window_eng : 5 # (int) Size of the window to use for computing energy for tissue extraction. thresh : 190 # (int) Threshold for the tissue extraction algorithm. area_min : 50000 # (int) Minimum area for the tissue contour. # Tissue visualization parameters visualization_settings : vis_level : 4 # (int) Visualization level for the tissue contour extraction. number_contours : false # (bool) Plot the id number for each contour. line_thickness : 50 # (bool) Line thickness for the contour visualization.","title":"Configuration File"},{"location":"cli/preprocessing/#examples","text":"# Basic contour extraction ptb preprocessing contour slides/ results/ # With visualization ptb preprocessing contour slides/ results/ --visualize # Using custom configuration ptb preprocessing contour slides/ results/ --config-file custom_config.yaml # With annotations and multiple output formats ptb preprocessing contour slides/ results/ --annotations-directory annotations/ --contours-exts pickle geojson --visualize","title":"Examples"},{"location":"cli/preprocessing/#ptb-preprocessing-patchify","text":"Extract patches from slides using tissue contours.","title":"ptb preprocessing patchify"},{"location":"cli/preprocessing/#usage_1","text":"ptb preprocessing patchify [ OPTIONS ] SLIDE_DIRECTORY RESULTS_DIRECTORY","title":"Usage"},{"location":"cli/preprocessing/#arguments_1","text":"SLIDE_DIRECTORY : Path to the directory containing the slide files RESULTS_DIRECTORY : Path to the directory where the results will be saved","title":"Arguments"},{"location":"cli/preprocessing/#options_1","text":"Option Type Description Default --roi-csv str | None Path to the csv file containing the ROIs None --contours-directory str | None Path to directory containing contour annotations None --engine str Engine for reading slides openslide --mode str Extraction mode ( contours , roi , all ) contours --patch-exts list[str] File extensions for patches ( h5 , geojson ) [h5] --config-file str | None Path to configuration file None","title":"Options"},{"location":"cli/preprocessing/#configuration-file_1","text":"Example configuration for patch extraction: # Default configuration for PrismToolBox patching # Patch extraction parameters patch_settings : patch_level : 0 # (float) Level of the slide to extract patches from. patch_size : 256 # (float) Size of the patches to extract. overlap : 0 # (float) Overlap between the patches. units : [ \"px\" , \"px\" ] # (str, str) Units for the patch size and overlap. Options are 'pixels' or 'micro' for micrometers. contours_mode : \"four_pt\" # (str) The mode to use for the contour checking. Possible values are center, four_pt, and four_pt_hard. rgb_threshs : [ 2 , 240 ] # (int, int) The thresholds for the RGB channels (black threshold, white threshold). percentages : [ 0.6 , 0.9 ] # (float, float) The percentages of pixels below/above the thresholds to consider the patch as black/white. # Patch stitching parameters stitching_settings : vis_level : 4 # (int) Level of the slide to stitch the patches at. draw_grid : false # (bool) Whether to draw a grid on the stitched image.","title":"Configuration File"},{"location":"cli/preprocessing/#examples_1","text":"# Basic patch extraction ptb preprocessing patchify slides/ results/ # Patch extraction within a ROI ptb preprocessing patchify slides/ results/ --mode roi --roi-directory results/rois.csv # Within previously extracted tissue contours and custom configuration ptb preprocessing patchify slides/ results/ --mode contours --contours-directory results/contours/ --config-file patch_config.yaml # Extract patches in multiple formats ptb preprocessing patchify slides/ results/ --contours-directory results/contours/ --patch-exts h5 geojson Attention: For the roi mode, you need to provide a table with the ROIs in a CSV format, where each row corresponds to a slide and contains the slide ID and coordinates of the ROI. # Extract patches from a specific ROI ptb preprocessing patchify slides/ results/ --mode roi --roi-csv results/rois.csv","title":"Examples"},{"location":"cli/preprocessing/#complete-workflow-example","text":"Here's a complete example of processing a dataset: # Step 1: Extract tissue contours with visualization ptb preprocessing contouring slides/ results/ --visualize --config-file tissue_config.yaml # Step 2: Extract patches from the contours ptb preprocessing patching slides/ results/ --contours-directory results/contours/ --config-file patch_config.yaml --patch-exts geojson Results will be saved in: - results/contours/ (tissue contours as pickle files) - results/contoured_images/ (visualizations) - results/patches_256_ovelap_0/ (extracted patches as geojson coordinates) - results/stitched_images_256_ovelap_0/ (patch visualizations)","title":"Complete Workflow Example"},{"location":"cli/preprocessing/#tips-and-best-practices","text":"Start with small datasets : Process a few slides first to validate your parameters Use visualizations : Use --visualize flag to check if tissue detection works correctly, and --stitch to visualize the selected patches. Monitor output : Use verbose mode ( -v or -vv ) to see detailed processing information","title":"Tips and Best Practices"},{"location":"reference/SUMMARY/","text":"prismtoolbox nucleiseg models sop architectures modules postprocessing seg_utils segmenter utils data_utils qupath_utils stain_utils torch_utils vis_utils wsicore core_utils wsi wsiemb emb_utils embedder models clam conch_model coca_model conch transformer utils vision_tower generic pathoduet phikon utils processing","title":"SUMMARY"},{"location":"reference/prismtoolbox/","text":"WSI ( slide_path , engine = 'openslide' ) The WSI (Whole Slide Image) class is responsible for handling operations related to whole slide images. Parameters: Name Type Description Default slide_path str The path to the slide image file. required engine str The engine used to read the slide image. 'openslide' Attributes: Name Type Description slide_path str The path to the slide image file. engine str The engine used to read the slide image. slide_name str The name of the slide image file. Retrieved from the slide path using retrieve_slide_name_ext method. slide_ext str The extension of the slide image file. Retrieved from the slide path using retrieve_slide_name_ext method. slide OpenSlide | TiffSlide The wsi read from the file using engine. dimensions list [ tuple [ int , int ]] The dimensions of the slide image. Set by the set_slide_attributes method. level_dimensions list [ tuple [ int , int ]] The dimensions of the different levels of the slide image. Set by the set_slide_attributes method. level_downsamples list [ tuple [ int , int ]] The downsampling factors of the different levels of the slide image. Set by the set_slide_attributes method. properties dict The properties of the slide image. Set by the set_slide_attributes method. offset tuple [ int , int ] The offset of the slide image. Set by the set_slide_attributes method. ROI ndarray | None The region of interest in the slide image. Please use the set_roi method to set the ROI. ROI_width int | None The width of the region of interest. Set by the set_roi method. ROI_height int | None The height of the region of interest. Set by the set_roi method. tissue_contours list [ ndarray ] | None The contours of the tissue in the slide image. Please use the detect_tissue method to detect the tissue contours. coords ndarray | None The coordinates of patches extracted from slide image. Please use the extract_patches method to extract patches. coords_attrs dict | None The attributes of the coordinates. Set by the extract_patches method. Source code in src/prismtoolbox/wsicore/wsi.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , slide_path : str , engine : str = \"openslide\" ): \"\"\"The WSI (Whole Slide Image) class is responsible for handling operations related to whole slide images. Args: slide_path: The path to the slide image file. engine: The engine used to read the slide image. Attributes: slide_path (str): The path to the slide image file. engine (str): The engine used to read the slide image. slide_name (str): The name of the slide image file. Retrieved from the slide path using [retrieve_slide_name_ext][prismtoolbox.wsicore.WSI.retrieve_slide_name_ext] method. slide_ext (str): The extension of the slide image file. Retrieved from the slide path using [retrieve_slide_name_ext][prismtoolbox.wsicore.WSI.retrieve_slide_name_ext] method. slide (OpenSlide | TiffSlide): The wsi read from the file using engine. dimensions (list[tuple[int, int]]): The dimensions of the slide image. Set by the set_slide_attributes method. level_dimensions (list[tuple[int, int]]): The dimensions of the different levels of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. level_downsamples (list[tuple[int, int]]): The downsampling factors of the different levels of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. properties (dict): The properties of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. offset (tuple[int, int]): The offset of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. ROI (ndarray | None): The region of interest in the slide image. Please use the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method to set the ROI. ROI_width (int | None): The width of the region of interest. Set by the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method. ROI_height (int | None): The height of the region of interest. Set by the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method. tissue_contours (list[ndarray] | None): The contours of the tissue in the slide image. Please use the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method to detect the tissue contours. coords (np.ndarray | None): The coordinates of patches extracted from slide image. Please use the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method to extract patches. coords_attrs (dict | None): The attributes of the coordinates. Set by the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method. \"\"\" self . slide_path = slide_path self . engine = engine self . slide_name , self . slide_ext = self . retrieve_slide_name_ext ( self . slide_path ) self . slide = self . read ( slide_path , engine ) self . offset = ( 0 , 0 ) self . set_slide_attributes () self . ROI = None self . ROI_width = None self . ROI_height = None self . tissue_contours = None self . coords = None self . coords_attrs = None apply_pathologist_annotations ( path , class_name = 'annotation' , column_to_select = 'objectType' ) Apply pathologist annotations to the tissue contours by intersecting the annotations with the tissue contours. Requires the tissue contours to be set for the slide beforehand with the detect_tissue method. Parameters: Name Type Description Default path str The path to the .geojson file containing the annotations extracted from QuPath. required class_name str The class name to use for selecting the annotations to apply. 'annotation' column_to_select str The column to select in the GeoDataFrame. 'objectType' Source code in src/prismtoolbox/wsicore/wsi.py 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 def apply_pathologist_annotations ( self , path : str , class_name : str = \"annotation\" , column_to_select : str = \"objectType\" , ) -> None : \"\"\"Apply pathologist annotations to the tissue contours by intersecting the annotations with the tissue contours. Requires the tissue contours to be set for the slide beforehand with the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method. Args: path: The path to the .geojson file containing the annotations extracted from QuPath. class_name: The class name to use for selecting the annotations to apply. column_to_select: The column to select in the GeoDataFrame. \"\"\" assert ( self . tissue_contours is not None ), \"No tissue contours found for the slide, please run the detect_tissue method first\" offset = ( - self . offset [ 0 ], - self . offset [ 1 ]) pathologist_annotations = read_qupath_annotations ( path , offset = offset , class_name = class_name , column_to_select = column_to_select ) polygons = contoursToPolygons ( self . tissue_contours , make_valid = True ) intersection = intersectionPolygons ( polygons , pathologist_annotations ) self . tissue_contours = PolygonsToContours ( intersection ) convert_micrometer_to_pixel ( value , level , axis = 'x' ) Convert a value from micrometer to pixel. Parameters: Name Type Description Default value float The value to convert (in micrometer). required level int The level at which the conversion should be performed. required axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description int The input value in pixel. Source code in src/prismtoolbox/wsicore/wsi.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def convert_micrometer_to_pixel ( self , value : float , level : int , axis : str = \"x\" , ) -> int : \"\"\"Convert a value from micrometer to pixel. Args: value: The value to convert (in micrometer). level: The level at which the conversion should be performed. axis: The axis to use for getting the conversion factor (x or y). Returns: The input value in pixel. \"\"\" return int ( value / float ( self . properties [ f \" { self . engine } .mpp- { axis } \" ])) // int ( self . level_downsamples [ level ] ) convert_pixel_to_micrometer ( value , level , axis = 'x' ) Convert a value from pixel to micrometer. Parameters: Name Type Description Default value float The value to convert (in pixel). required level int The level at which the conversion should be performed. required axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description float The input value in micrometer. Source code in src/prismtoolbox/wsicore/wsi.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def convert_pixel_to_micrometer ( self , value : float , level : int , axis : str = \"x\" , ) -> float : \"\"\"Convert a value from pixel to micrometer. Args: value: The value to convert (in pixel). level: The level at which the conversion should be performed. axis: The axis to use for getting the conversion factor (x or y). Returns: The input value in micrometer. \"\"\" return ( value * float ( self . properties [ f \" { self . engine } .mpp- { axis } \" ]) * self . level_downsamples [ level ] ) convert_units ( value , level , from_unit , to_unit = 'px' , axis = 'x' ) Convert a value from one unit to another. Parameters: Name Type Description Default value float The value to convert. required level int The level at which the conversion should be performed. required from_unit str The unit to convert from (px or micro). required to_unit str The unit to convert to (px or micro). 'px' axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description int | float The input value converted in the desired unit. Source code in src/prismtoolbox/wsicore/wsi.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 def convert_units ( self , value : float , level : int , from_unit : str , to_unit : str = \"px\" , axis : str = \"x\" , ) -> int | float : \"\"\"Convert a value from one unit to another. Args: value: The value to convert. level: The level at which the conversion should be performed. from_unit: The unit to convert from (px or micro). to_unit: The unit to convert to (px or micro). axis: The axis to use for getting the conversion factor (x or y). Returns: The input value converted in the desired unit. \"\"\" if from_unit == \"micro\" and to_unit == \"px\" : value = self . convert_micrometer_to_pixel ( value , level , axis ) elif from_unit == \"px\" and to_unit == \"micro\" : value = self . convert_pixel_to_micrometer ( value , level , axis ) elif from_unit == to_unit : pass else : raise ValueError ( f \"Conversion from { from_unit } to { to_unit } not supported.\" ) return value create_thumbnail ( level , crop_roi = False ) Create a thumbnail of the slide at a given level. Parameters: Name Type Description Default level int The level at which the thumbnail should be created. required crop_roi bool A boolean to crop the thumbnail to the region of interest defined for the slide (requires a ROI to be set for the slide beforehand with the set_roi method) False Returns: Type Description Image A thumbnail of the slide as a PIL image. Source code in src/prismtoolbox/wsicore/wsi.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 def create_thumbnail ( self , level : int , crop_roi : bool = False , ) -> Image . Image : \"\"\"Create a thumbnail of the slide at a given level. Args: level: The level at which the thumbnail should be created. crop_roi: A boolean to crop the thumbnail to the region of interest defined for the slide (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method) Returns: A thumbnail of the slide as a PIL image. \"\"\" thumb = self . read_region (( 0 , 0 ), level , self . level_dimensions [ level ]) . convert ( \"RGB\" ) if crop_roi : if self . ROI is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) log . info ( f \"Creating thumbnail with ROI { self . ROI } .\" ) coords_roi = ( self . ROI / self . level_downsamples [ level ]) . astype ( int ) thumb = thumb . crop ( tuple ( coords_roi )) return thumb detect_tissue ( seg_level , window_avg , window_eng , thresh , inside_roi = False , inv_thresh = False , area_min = 200000.0 , start = ( 0 , 0 )) Segment the tissue on the slide based on a threshold on the Law's texture energy spot map and floodfill algorithm to fill the holes in the mask. The tissue contours are stored in the tissue_contours attribute. Parameters: Name Type Description Default seg_level int The level at which the segmentation should be performed. required window_avg int The size of the window for local averaging. required window_eng int The size of the window for Law's texture energy computation. required thresh int The threshold for binarization on the Law's texture energy spot map. required inside_roi bool Set to True to identify the tissue only within a ROI (requires a ROI to be set for the slide beforehand with the set_roi method). False inv_thresh bool Set to True to invert the thresholding. False area_min float The minimum area for a contour to be kept. 200000.0 start tuple [ int , int ] The starting point for the floodfill algorithm (should be left at (0, 0) in most cases). (0, 0) Source code in src/prismtoolbox/wsicore/wsi.py 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def detect_tissue ( self , seg_level : int , window_avg : int , window_eng : int , thresh : int , inside_roi : bool = False , inv_thresh : bool = False , area_min : float = 2e5 , start : tuple [ int , int ] = ( 0 , 0 ), ) -> None : \"\"\"Segment the tissue on the slide based on a threshold on the Law's texture energy spot map and floodfill algorithm to fill the holes in the mask. The tissue contours are stored in the tissue_contours attribute. Args: seg_level: The level at which the segmentation should be performed. window_avg: The size of the window for local averaging. window_eng: The size of the window for Law's texture energy computation. thresh: The threshold for binarization on the Law's texture energy spot map. inside_roi: Set to True to identify the tissue only within a ROI (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). inv_thresh: Set to True to invert the thresholding. area_min: The minimum area for a contour to be kept. start: The starting point for the floodfill algorithm (should be left at (0, 0) in most cases). \"\"\" final_contours = [] img = np . array ( self . create_thumbnail ( seg_level , inside_roi )) img_avg = local_average ( np . asarray ( img ), window_avg ) law_feats = compute_law_feats ( img_avg , window_eng )[ 0 ] filterred_img = apply_bilateral_filter ( np . clip ( law_feats , 0 , 255 ) . astype ( \"uint8\" )) threshed_img = apply_binary_thresh ( filterred_img , thresh , inv_thresh ) flooded_img = floodfill_img ( np . pad ( threshed_img , 1 ), start ) contours = contour_mask ( flooded_img ) for contour in contours : c = contour . copy () area = cv2 . contourArea ( c ) if area > area_min : final_contours . append ( contour ) if len ( final_contours ) == 0 : self . tissue_contours = [] log . warning ( f \"No tissue contours found for the slide { self . slide_name } .\" ) else : scale = self . level_downsamples [ seg_level ] offset = np . array ( self . ROI [: 2 ]) if self . ROI is not None else np . array ([ 0 , 0 ]) final_contours = self . scale_contours ( final_contours , scale ) final_contours = [ cont + offset for cont in final_contours ] # Sanity check to ensure that the contours are all within a ROI is provided if self . ROI is not None : assert all ( [ np . all ( cont >= self . ROI [: 2 ]) and np . all ( cont <= self . ROI [ 2 :]) for cont in final_contours ] ) self . tissue_contours = final_contours log . info ( f \"Identified { len ( final_contours ) } contours for the slide { self . slide_name } .\" ) return extract_patches ( patch_size , patch_level , mode , step_size = None , overlap = None , units = ( 'px' , 'px' ), use_padding = True , contours_mode = None , rgb_threshs = ( 2 , 220 ), percentages = ( 0.6 , 0.9 )) Extract valid patches from the slide with different extraction modes. A patch is considered valid if it is not black or white and is within the region of interest or the tissue contours if relevant. The extracted patches are stored as coordinates in the coords attribute, and the attributes of the coordinates are stored in the coords_attrs attribute. Parameters: Name Type Description Default patch_size float The size of the patches to extract (assumed to be square). required patch_level int The level at which the patches should be extracted. required mode str The mode to use for the extraction: \"contours\" mode extracts patches within the tissue contours (requires the tissue contours to be set for the slide beforehand with the detect_tissue method). \"roi\" mode extracts patches within the region of interest (requires the ROI to be set for the slide beforehand with the set_roi method). \"all\" mode extracts patches from the entire slide required step_size float | None The step size for the sliding window (if set to None, the step size will be computed based on the overlap). None overlap float | None The overlap between patches as an absolute value (must be provided if step_size is set to None). None units tuple [ str , str ] The units for the patch size and step size/overlap values (pixels: px, micrometers: micro). ('px', 'px') use_padding bool Set to True to use padding for the extraction. True contours_mode str | None The mode to use for the contour checking function (must be provided if mode is set to contours). See IsInContour for more details. None rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Source code in src/prismtoolbox/wsicore/wsi.py 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 def extract_patches ( self , patch_size : float , patch_level : int , mode : str , step_size : float | None = None , overlap : float | None = None , units : tuple [ str , str ] = ( \"px\" , \"px\" ), use_padding : bool = True , contours_mode : str | None = None , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> None : \"\"\"Extract valid patches from the slide with different extraction modes. A patch is considered valid if it is not black or white and is within the region of interest or the tissue contours if relevant. The extracted patches are stored as coordinates in the coords attribute, and the attributes of the coordinates are stored in the coords_attrs attribute. Args: patch_size: The size of the patches to extract (assumed to be square). patch_level: The level at which the patches should be extracted. mode: The mode to use for the extraction: - \"contours\" mode extracts patches within the tissue contours (requires the tissue contours to be set for the slide beforehand with the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method). - \"roi\" mode extracts patches within the region of interest (requires the ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). - \"all\" mode extracts patches from the entire slide step_size: The step size for the sliding window (if set to None, the step size will be computed based on the overlap). overlap: The overlap between patches as an absolute value (must be provided if step_size is set to None). units: The units for the patch size and step size/overlap values (pixels: px, micrometers: micro). use_padding: Set to True to use padding for the extraction. contours_mode: The mode to use for the contour checking function (must be provided if mode is set to contours). See [IsInContour][prismtoolbox.wsicore.core_utils.IsInContour] for more details. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. \"\"\" assert all ( [ unit in [ \"micro\" , \"px\" ] for unit in units ] ), \"Units must be either 'micro' or 'px'\" patch_size = int ( self . convert_units ( patch_size , patch_level , units [ 0 ])) if step_size is None : if overlap is None : raise ValueError ( \"Either step_size or overlap must be provided if step_size is not set\" ) step_size = int ( patch_size - self . convert_units ( overlap , patch_level , units [ 1 ])) else : step_size = int ( self . convert_units ( step_size , patch_level , units [ 1 ])) log . info ( f \"Extracting patches of size { patch_size } at level { patch_level } with step size { step_size } .\" ) if mode == \"contours\" : log . info ( \"Extracting patches with 'contours' mode.\" ) assert self . tissue_contours is not None , \"Empty tissue contours vector for the slide, please run the detect_tissue method first.\" assert len ( self . tissue_contours ) > 0 , \"No tissue contours found for the slide.\" assert contours_mode is not None , \"Contours mode must be provided if mode is set to 'contours'.\" valid_coords = [] for cont in self . tissue_contours : roi_dim = cv2 . boundingRect ( cont ) # type: ignore log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords . extend ( self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , cont , contours_mode , rgb_threshs = rgb_threshs , percentages = percentages , ) ) valid_coords = np . array ( valid_coords ) elif mode == \"roi\" : log . info ( \"Extracting patches with 'roi' mode.\" ) if self . ROI is None or self . ROI_width is None or self . ROI_height is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) roi_dim = self . ROI [ 0 ], self . ROI [ 1 ], self . ROI_width , self . ROI_height log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords = self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , rgb_threshs = rgb_threshs , percentages = percentages , ) elif mode == \"all\" : roi_dim = 0 , 0 , self . level_dimensions [ 0 ][ 0 ], self . level_dimensions [ 0 ][ 1 ] log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords = self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , rgb_threshs = rgb_threshs , percentages = percentages , ) else : raise ValueError ( f \"Mode { mode } not supported\" ) attr = { \"patch_size\" : patch_size , \"patch_level\" : patch_level , \"downsample\" : self . level_downsamples [ patch_level ], \"downsampled_level_dim\" : tuple ( np . array ( self . level_dimensions [ patch_level ])), \"level_dim\" : self . level_dimensions [ patch_level ], \"name\" : self . slide_name , } if len ( valid_coords ) == 0 : log . warning ( f \"No valid coordinates found for the slide { self . slide_name } .\" ) else : log . info ( f \"Identified a total of { len ( valid_coords ) } valid coordinates for the slide { self . slide_name } .\" ) self . coords = valid_coords self . coords_attrs = attr extract_patches_roi ( patch_level , patch_size , step_size = None , roi_dim = None , use_padding = True , contour = None , contours_mode = None , coord_candidates = None , rgb_threshs = ( 2 , 220 ), percentages = ( 0.6 , 0.9 ), return_indices = False ) Extract valid patches from a region of interest, i.e if the patch is not black or white and is within the region of interest/contours if relevant). Parameters: Name Type Description Default patch_level int The level at which the patches should be extracted. required patch_size int The size of the patches to extract (assumed to be square). required step_size int | None The step size to use for the sliding window. None roi_dim tuple [ int , int , int , int ] | None The top-left corner coordinates and dimensions of the region of interest. Must be provided if coord_candidates is set to None. None use_padding bool Set to True to use padding for the extraction. True contour ndarray | None The tissue contour to use for the extraction. If set to None, will not check if patches are within a contour. None contours_mode str | None The mode for the contour checking function. See IsInContour for more details. Must be provided if mode is set to contours. Otherwise, will not check if patches are within the contours. None coord_candidates ndarray | None Precomputed candidate coordinates for the patches. If set to None, will compute the candidate coordinates based on the ROI dimensions and the step size. None rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) return_indices bool Set to True to return the indices of the valid coordinates. False Returns: Type Description ndarray | tuple [ ndarray , list [ int ]] An array of valid coordinates for the patches (i.e. coordinates of the top-left corner of the patches). Source code in src/prismtoolbox/wsicore/wsi.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 def extract_patches_roi ( self , patch_level : int , patch_size : int , step_size : int | None = None , roi_dim : tuple [ int , int , int , int ] | None = None , use_padding : bool = True , contour : np . ndarray | None = None , contours_mode : str | None = None , coord_candidates : np . ndarray | None = None , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), return_indices : bool = False , ) -> np . ndarray | tuple [ np . ndarray , list [ int ]]: \"\"\"Extract valid patches from a region of interest, i.e if the patch is not black or white and is within the region of interest/contours if relevant). Args: patch_level: The level at which the patches should be extracted. patch_size: The size of the patches to extract (assumed to be square). step_size: The step size to use for the sliding window. roi_dim: The top-left corner coordinates and dimensions of the region of interest. Must be provided if coord_candidates is set to None. use_padding: Set to True to use padding for the extraction. contour: The tissue contour to use for the extraction. If set to None, will not check if patches are within a contour. contours_mode: The mode for the contour checking function. See [IsInContour][prismtoolbox.wsicore.core_utils.IsInContour] for more details. Must be provided if mode is set to contours. Otherwise, will not check if patches are within the contours. coord_candidates: Precomputed candidate coordinates for the patches. If set to None, will compute the candidate coordinates based on the ROI dimensions and the step size. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. return_indices: Set to True to return the indices of the valid coordinates. Returns: An array of valid coordinates for the patches (i.e. coordinates of the top-left corner of the patches). \"\"\" patch_downsample = int ( self . level_downsamples [ patch_level ]) ref_patch_size = patch_size * patch_downsample if coord_candidates is None : if roi_dim is None or step_size is None : raise ValueError ( \"roi_dim and step_size must be provided if coord_candidates is not set.\" ) start_x , start_y , w , h = roi_dim img_w , img_h = self . level_dimensions [ 0 ] if use_padding : stop_y = start_y + h stop_x = start_x + w else : stop_y = min ( start_y + h , img_h - ref_patch_size + 1 ) stop_x = min ( start_x + w , img_w - ref_patch_size + 1 ) step_size = step_size * patch_downsample x_range = np . arange ( start_x , stop_x , step = step_size ) y_range = np . arange ( start_y , stop_y , step = step_size ) x_coords , y_coords = np . meshgrid ( x_range , y_range , indexing = \"ij\" ) coord_candidates = np . array ( [ x_coords . flatten (), y_coords . flatten ()] ) . transpose () if contour is not None : if contours_mode is None : raise ValueError ( \"A contour mode must be provided if patch extraction mode is set to contours.\" ) cont_check_fn = IsInContour ( contour , patch_size = ref_patch_size , center_shift = 0.5 , mode = contours_mode ) log . info ( f \"Extracting patches with contour checking function mode { contours_mode } .\" ) else : cont_check_fn = None num_workers = mp . cpu_count () pool = mp . Pool ( num_workers , initializer = WSI . worker_init , initargs = ( self . slide_path , self . engine , ), ) iterable = [ ( coord , cont_check_fn , patch_level , patch_size , rgb_threshs , percentages ) for coord in coord_candidates ] valid_coords = pool . starmap ( WSI . process_coord_candidate , iterable ) pool . close () valid_indices = [ i for i , coord in enumerate ( valid_coords ) if coord is not None ] valid_coords = np . array ([ coord_candidates [ i ] for i in valid_indices ]) log . info ( f \"Identified { len ( valid_coords ) } valid coordinates in the ROI { roi_dim } .\" ) if return_indices : return valid_coords , valid_indices else : return valid_coords is_black_white ( patch , rgb_threshs = ( 2 , 220 ), percentages = ( 0.6 , 0.9 )) staticmethod Check if a patch is black or white. Parameters: Name Type Description Default patch Image The input patch. required rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Returns: Type Description bool True if the patch is black or white, False otherwise. Source code in src/prismtoolbox/wsicore/wsi.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 @staticmethod def is_black_white ( patch : Image . Image , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> bool : \"\"\"Check if a patch is black or white. Args: patch: The input patch. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. Returns: True if the patch is black or white, False otherwise. \"\"\" return isBlackPatch ( patch , rgb_thresh = rgb_threshs [ 0 ], percentage = percentages [ 0 ] ) or isWhitePatch ( patch , rgb_thresh = rgb_threshs [ 1 ], percentage = percentages [ 1 ]) load_patches ( file_path ) Load the patches from a hdf5 file. Parameters: Name Type Description Default file_path str The path to the hdf5 file containing the patches. required Source code in src/prismtoolbox/wsicore/wsi.py 462 463 464 465 466 467 468 469 def load_patches ( self , file_path : str ) -> None : \"\"\"Load the patches from a hdf5 file. Args: file_path: The path to the hdf5 file containing the patches. \"\"\" log . info ( f \"Loading patches for slide { self . slide_name } from { file_path } .\" ) self . coords , self . coords_attrs = read_h5_file ( file_path , \"coords\" ) load_tissue_contours ( file_path ) Load the tissue contours from a pickle file. Parameters: Name Type Description Default file_path str The path to the pickle file containing the tissue contours. required Source code in src/prismtoolbox/wsicore/wsi.py 370 371 372 373 374 375 376 377 def load_tissue_contours ( self , file_path : str ) -> None : \"\"\"Load the tissue contours from a pickle file. Args: file_path: The path to the pickle file containing the tissue contours. \"\"\" log . info ( f \"Loading tissue contours for slide { self . slide_name } from { file_path } .\" ) self . tissue_contours = load_obj_with_pickle ( file_path ) process_coord_candidate ( coord , cont_check_fn , patch_level , patch_size , rgb_threshs = ( 2 , 220 ), percentages = ( 0.6 , 0.9 )) staticmethod Determine if a candidate coordinate is valid based on a contour checking function and/or black/white thresholding. Parameters: Name Type Description Default coord tuple [ int , int ] The candidate coordinate. required cont_check_fn IsInContour | None The contour checking function. required patch_level int The level at which the patch should be extracted. required patch_size int The size of the patch (assumed to be square). required rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Returns: Type Description tuple [ int , int ] | None The coordinate if it is valid, None otherwise. Source code in src/prismtoolbox/wsicore/wsi.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 @staticmethod def process_coord_candidate ( coord : tuple [ int , int ], cont_check_fn : IsInContour | None , patch_level : int , patch_size : int , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> tuple [ int , int ] | None : \"\"\"Determine if a candidate coordinate is valid based on a contour checking function and/or black/white thresholding. Args: coord: The candidate coordinate. cont_check_fn: The contour checking function. patch_level: The level at which the patch should be extracted. patch_size: The size of the patch (assumed to be square). rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. Returns: The coordinate if it is valid, None otherwise. \"\"\" if cont_check_fn is None or cont_check_fn ( coord ): patch = wsi . read_region ( coord , patch_level , ( patch_size , patch_size ) ) . convert ( \"RGB\" ) if not WSI . is_black_white ( patch , rgb_threshs , percentages ): return coord else : return None else : return None read ( slide_path , engine ) staticmethod Read a slide with a given engine. Parameters: Name Type Description Default slide_path str The path to the slide. required engine str The backend library to use for reading the slide (currently only openslide and tiffslide are supported). required Returns: Type Description A slide object. Source code in src/prismtoolbox/wsicore/wsi.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 @staticmethod def read ( slide_path : str , engine : str ): \"\"\"Read a slide with a given engine. Args: slide_path: The path to the slide. engine: The backend library to use for reading the slide (currently only openslide and tiffslide are supported). Returns: A slide object. \"\"\" if engine == \"openslide\" : import openslide slide = openslide . OpenSlide ( slide_path ) elif engine == \"tiffslide\" : import tiffslide slide = tiffslide . TiffSlide ( slide_path ) else : raise NotImplementedError ( f \"engine { engine } not supported\" ) return slide read_region ( location , level , size ) Read a region from the slide for a given level and size. Parameters: Name Type Description Default location tuple [ int , int ] The coordinates of the top left corner of the region (in pixels). required level int The level at which the region should be read. required size tuple [ int , int ] The size of the region (in pixels). required Returns: Type Description Image The desired region of the slide as a PIL image. Source code in src/prismtoolbox/wsicore/wsi.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def read_region ( self , location : tuple [ int , int ], level : int , size : tuple [ int , int ], ) -> Image . Image : \"\"\"Read a region from the slide for a given level and size. Args: location: The coordinates of the top left corner of the region (in pixels). level: The level at which the region should be read. size: The size of the region (in pixels). Returns: The desired region of the slide as a PIL image. \"\"\" return self . slide . read_region ( location , level , size ) . convert ( \"RGB\" ) retrieve_slide_name_ext ( slide_path ) staticmethod Retrieves slide name and slide extension from slide path. Parameters: Name Type Description Default slide_path str The path to the slide. required Returns: Type Description tuple [ str , str ] A tuple (slide name, slide ext). Source code in src/prismtoolbox/wsicore/wsi.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 @staticmethod def retrieve_slide_name_ext ( slide_path : str ) -> tuple [ str , str ]: \"\"\"Retrieves slide name and slide extension from slide path. Args: slide_path: The path to the slide. Returns: A tuple (slide name, slide ext). \"\"\" slide_name = re . search ( r \"([^/]+?)(?=\\.[\\w\\.]+$)\" , slide_path ) slide_ext = re . search ( r \"(?<=\\.)[\\w\\.]+$\" , slide_path ) if slide_ext is None or slide_name is None : raise ValueError ( f \"Could not retrieve slide name and extension from { slide_path } . \" \"Please check the file path.\" ) else : slide_name , slide_ext = slide_name . group ( 0 ), slide_ext . group ( 0 ) return slide_name , slide_ext save_patches ( save_dir , file_format = 'h5' , selected_idx = None , merge = False , label = None , color = ( 255 , 0 , 0 ), append_to_existing_file = False ) Save the patches in a hdf5 or geojson file. Parameters: Name Type Description Default save_dir str The path to the directory where the patches will be saved. required file_format str The format for the saving (h5 for python processing, geojson for QuPath processing). 'h5' selected_idx ndarray | None An array of indices of the patches to save (if set to None, all the patches will be saved). None merge bool Set to True to merge the patches into a single polygon (for geojson format only). False label str | None An optional label to assign to the patches (for geojson format only). None color tuple [ int , int , int ] An optional color to assign to the patches (for geojson format only). (255, 0, 0) append_to_existing_file bool Set to True to append the patches to an existing geojson file. False Source code in src/prismtoolbox/wsicore/wsi.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def save_patches ( self , save_dir : str , file_format : str = \"h5\" , selected_idx : np . ndarray | None = None , merge : bool = False , label : str | None = None , color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), append_to_existing_file : bool = False , ) -> None : \"\"\"Save the patches in a hdf5 or geojson file. Args: save_dir: The path to the directory where the patches will be saved. file_format: The format for the saving (h5 for python processing, geojson for QuPath processing). selected_idx: An array of indices of the patches to save (if set to None, all the patches will be saved). merge: Set to True to merge the patches into a single polygon (for geojson format only). label: An optional label to assign to the patches (for geojson format only). color: An optional color to assign to the patches (for geojson format only). append_to_existing_file: Set to True to append the patches to an existing geojson file. \"\"\" if self . coords is None : raise ValueError ( \"No patches found. Please check if patches were correctly extracted.\" ) if self . coords_attrs is None : raise ValueError ( \"No attributes set for the patches. \" \"Please check if patches were correctly extracted.\" ) if selected_idx is not None : coords = np . array ( self . coords [ selected_idx ]) else : coords = np . array ( self . coords ) if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if file_format == \"h5\" : asset_dict = { \"coords\" : coords } attr_dict = { \"coords\" : self . coords_attrs } file_path = os . path . join ( save_dir , f \" { self . slide_name } .h5\" ) log . info ( f \"Saving patches for slide { self . slide_name } at { file_path } as h5 file.\" ) save_patches_with_hdf5 ( file_path , asset_dict , attr_dict ) elif file_format == \"geojson\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .geojson\" ) patch_downsample = int ( self . level_downsamples [ self . coords_attrs [ \"patch_level\" ]] ) polygons = patchesToPolygons ( coords , self . coords_attrs [ \"patch_size\" ], patch_downsample , merge ) log . info ( f \"Saving { len ( coords ) } patches for slide { self . slide_name } at { file_path } as geojson file.\" ) export_polygons_to_qupath ( polygons , file_path , \"annotation\" , offset = self . offset , label = label , color = color , append_to_existing_file = append_to_existing_file , ) elif file_format == \"jpg\" or file_format == \"png\" : save_dir = os . path . join ( save_dir , self . slide_name ) if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) log . info ( f \"Saving { len ( coords ) } patches for slide { self . slide_name } at { save_dir } with { file_format } .\" ) for coord in coords : patch = self . read_region ( coord , self . coords_attrs [ \"patch_level\" ], ( self . coords_attrs [ \"patch_size\" ], self . coords_attrs [ \"patch_size\" ]), ) . convert ( \"RGB\" ) patch . save ( os . path . join ( save_dir , f \" { coord [ 0 ] } _ { coord [ 1 ] } . { file_format } \" )) else : raise ValueError ( f \"Format { file_format } not supported.\" ) save_tissue_contours ( save_dir , selected_idx = None , file_format = 'pickle' , merge = False , label = None , color = ( 255 , 0 , 0 ), append_to_existing_file = False , make_valid = False ) Save the tissue contours in a pickle or geojson file. Parameters: Name Type Description Default save_dir str The path to the directory where the contours will be saved. required selected_idx ndarray | None An array of indices of the contours to save (if set to None, all the contours will be saved). None file_format str The file format for saving the contours (pickle for python processing, geojson for QuPath processing). 'pickle' merge bool Set to True to merge the contours into a single polygon (for geojson format only). False label str | None An optional label to assign to the tissue contours (for geojson format only). None color tuple [ int , int , int ] An optional color to assign to the tissue contours (for geojson format only). (255, 0, 0) append_to_existing_file bool Set to True to append the contours to an existing geojson file. False make_valid bool Set to True to make the polygons valid (for geojson format only). False Source code in src/prismtoolbox/wsicore/wsi.py 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def save_tissue_contours ( self , save_dir : str , selected_idx : np . ndarray | None = None , file_format : str = \"pickle\" , merge : bool = False , label : str | None = None , color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), append_to_existing_file : bool = False , make_valid : bool = False , ) -> None : \"\"\"Save the tissue contours in a pickle or geojson file. Args: save_dir: The path to the directory where the contours will be saved. selected_idx: An array of indices of the contours to save (if set to None, all the contours will be saved). file_format: The file format for saving the contours (pickle for python processing, geojson for QuPath processing). merge: Set to True to merge the contours into a single polygon (for geojson format only). label: An optional label to assign to the tissue contours (for geojson format only). color: An optional color to assign to the tissue contours (for geojson format only). append_to_existing_file: Set to True to append the contours to an existing geojson file. make_valid: Set to True to make the polygons valid (for geojson format only). \"\"\" assert self . tissue_contours is not None , ( \"No tissue contours found for the slide, \" \"please run the detect_tissue method first\" ) if selected_idx is not None : tissue_contours = [ self . tissue_contours [ idx ] for idx in selected_idx ] else : tissue_contours = self . tissue_contours if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if file_format == \"pickle\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .pkl\" ) log . info ( f \"Saving tissue contours for slide { self . slide_name } at { file_path } as pickle file.\" ) save_obj_with_pickle ( tissue_contours , file_path ) elif file_format == \"geojson\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .geojson\" ) log . info ( f \"Saving { selected_idx } tissue contours for slide { self . slide_name } at { file_path } as geojson file.\" ) polygons = contoursToPolygons ( tissue_contours , merge , make_valid ) export_polygons_to_qupath ( polygons , file_path , \"annotation\" , offset = self . offset , label = label , color = color , append_to_existing_file = append_to_existing_file , ) else : raise ValueError ( f \"format { file_format } not supported\" ) scale_contours ( contours , scale ) staticmethod Scale the contours by a given factor. Parameters: Name Type Description Default contours list [ ndarray ] The contours to scale. required scale float The scale factor to apply. required Returns: Type Description list [ ndarray ] The input list with scaled contours. Source code in src/prismtoolbox/wsicore/wsi.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 @staticmethod def scale_contours ( contours : list [ np . ndarray ], scale : float , ) -> list [ np . ndarray ]: \"\"\"Scale the contours by a given factor. Args: contours: The contours to scale. scale: The scale factor to apply. Returns: The input list with scaled contours. \"\"\" scaled_contours = [ np . array ( cont * scale , dtype = \"int\" ) for cont in contours ] return scaled_contours set_roi ( roi = None , rois_df_path = None ) Set the region of interest for the slide. Can be set manually or by selecting a region on a thumbnail. The ROI is stored as a tuple in the ROI attribute. Parameters: Name Type Description Default roi tuple [ int , int , int , int ] | None Set the region of interest manually as a tuple (x1, y1, x2, y2). None rois_df_path str | None The path to dataframe containing the ROIs with a slide_id column identifying the slide, and the ROI coordinates as columns (x1, y1, x2, y2). If roi is not provided, the ROI will be set from this dataframe. None Returns: The region of interest set for the slide as a tuple (x1, y1, x2, y2). Source code in src/prismtoolbox/wsicore/wsi.py 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def set_roi ( self , roi : tuple [ int , int , int , int ] | None = None , rois_df_path : str | None = None , ) -> np . ndarray : \"\"\"Set the region of interest for the slide. Can be set manually or by selecting a region on a thumbnail. The ROI is stored as a tuple in the ROI attribute. Args: roi: Set the region of interest manually as a tuple (x1, y1, x2, y2). rois_df_path: The path to dataframe containing the ROIs with a slide_id column identifying the slide, and the ROI coordinates as columns (x1, y1, x2, y2). If roi is not provided, the ROI will be set from this dataframe. Returns: The region of interest set for the slide as a tuple (x1, y1, x2, y2). \"\"\" if roi is not None : ROI = np . array ( roi ) . astype ( int ) elif rois_df_path is not None : rois_df = pd . read_csv ( rois_df_path ) if \"slide_id\" not in rois_df . columns or len ( rois_df . columns ) < 5 : raise ValueError ( \"The provided dataframe does not contain a 'slide_id' column, or does not have enough columns for ROI coordinates.\" ) ROI = rois_df [ rois_df . slide_id == self . slide_name ] . values [ 0 ] . astype ( int ) else : log . info ( \"No ROI provided, prompting user to select one.\" ) level = input ( f \"No ROI was provided, please select a level at which the ROI should be created (max level: { len ( self . level_dimensions ) - 1 } ): \" ) if not level : log . info ( \"No level provided, setting the ROI at the highest level.\" ) level = len ( self . level_downsamples ) - 1 else : level = int ( level ) img = np . array ( self . create_thumbnail ( level )) ROI = select_roi_on_thumbnail ( img , int ( self . level_downsamples [ level ])) ROI = ( ROI * self . level_downsamples [ level ]) . astype ( int ) self . ROI = ROI self . ROI_width = ROI [ 2 ] - ROI [ 0 ] self . ROI_height = ROI [ 3 ] - ROI [ 1 ] log . info ( f \"ROI for slide { self . slide_name } has been set to { self . ROI } .\" ) return ROI set_slide_attributes () Set the slide attributes. Source code in src/prismtoolbox/wsicore/wsi.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def set_slide_attributes ( self ): \"\"\"Set the slide attributes.\"\"\" if self . engine == \"openslide\" : self . dimensions = self . slide . dimensions self . level_dimensions = self . slide . level_dimensions self . level_downsamples = self . slide . level_downsamples self . properties = self . slide . properties elif self . engine == \"tiffslide\" : self . dimensions = self . slide . dimensions self . level_dimensions = self . slide . level_dimensions self . level_downsamples = self . slide . level_downsamples self . properties = self . slide . properties else : raise NotImplementedError ( f \"Engine { self . engine } not supported.\" ) if ( f \" { self . engine } .bounds-x\" in self . properties . keys () and self . properties [ f \" { self . engine } .bounds-x\" ] is not None ): self . offset = ( - int ( self . properties [ f \" { self . engine } .bounds-x\" ]), - int ( self . properties [ f \" { self . engine } .bounds-y\" ]), ) stitch ( vis_level , selected_idx = None , colors = None , alpha = 0.6 , black_white = False , draw_grid = False , crop_roi = False , background_color = ( 0 , 0 , 0 )) Stitch the patches extracted on an image. The patches can be masked and colored depending on the mask and colors provided. Requires the coordinates of the patches to be set for the slide beforehand with the extract_patches method. Parameters: Name Type Description Default vis_level int The level at which the patches should be visualized. required selected_idx ndarray | None An array of indices of the patches to visualize (if set to None, all the patches will be visualized). None colors ndarray | None An array of RGB colors to apply to the patches (if set to None, the patches will be visualized as they are). None alpha float Set the transparency of the colors to apply to the patches. 0.6 black_white bool Set to True to visualize a binary mask of the patches extracted. False draw_grid bool Set to True to draw a grid on the stitched patches. False crop_roi bool Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the set_roi method). False background_color tuple [ int , int , int ] The color of the background. (0, 0, 0) Returns: Type Description Image A PIL image of the stitched patches. Source code in src/prismtoolbox/wsicore/wsi.py 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 def stitch ( self , vis_level : int , selected_idx : np . ndarray | None = None , colors : np . ndarray | None = None , alpha : float = 0.6 , black_white : bool = False , draw_grid : bool = False , crop_roi : bool = False , background_color : tuple [ int , int , int ] = ( 0 , 0 , 0 ), ) -> Image . Image : \"\"\"Stitch the patches extracted on an image. The patches can be masked and colored depending on the mask and colors provided. Requires the coordinates of the patches to be set for the slide beforehand with the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method. Args: vis_level: The level at which the patches should be visualized. selected_idx: An array of indices of the patches to visualize (if set to None, all the patches will be visualized). colors: An array of RGB colors to apply to the patches (if set to None, the patches will be visualized as they are). alpha: Set the transparency of the colors to apply to the patches. black_white: Set to True to visualize a binary mask of the patches extracted. draw_grid: Set to True to draw a grid on the stitched patches. crop_roi: Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). background_color: The color of the background. Returns: A PIL image of the stitched patches. \"\"\" if self . coords_attrs is None : raise RuntimeError ( f \"No attributes set for the patches of the slide { self . slide_name } , please check if patches were correctly extracted.\" ) assert self . coords is not None , ( \"No coordinates provided for the patches to visualize, please run the \" \"extract_patches method first or load the coordinates from a file.\" ) if crop_roi : if self . ROI is None or self . ROI_width is None or self . ROI_height is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) w , h = int ( np . ceil ( self . ROI_width / self . level_downsamples [ vis_level ])), int ( np . ceil ( self . ROI_height / self . level_downsamples [ vis_level ]) ) offset = np . array ( self . ROI [: 2 ]) else : w , h = self . level_dimensions [ vis_level ] offset = np . array ([ 0 , 0 ]) patch_size = self . coords_attrs [ \"patch_size\" ] patch_level = self . coords_attrs [ \"patch_level\" ] patch_size = int ( patch_size * self . level_downsamples [ patch_level ]) canvas = init_image ( w , h , mask = black_white , color_bakground = background_color ) downsample_vis = self . level_downsamples [ vis_level ] idxs = np . arange ( len ( self . coords )) if selected_idx is not None : idxs = idxs [ selected_idx ] patch_size = np . ceil ( patch_size / downsample_vis ) . astype ( int ) log . info ( f \"Stitching { len ( idxs ) } patches at level { vis_level } with patch size { patch_size } , \" f \"with colors { colors is not None } .\" ) for idx in idxs : coord = self . coords [ idx ] coord_downsampled = np . ceil ( np . abs ( coord - offset ) / downsample_vis ) . astype ( int ) patch_size_coord = ( min ( max ( w - coord_downsampled [ 0 ], 0 ), patch_size ), min ( max ( h - coord_downsampled [ 1 ], 0 ), patch_size ), ) if any ( val == 0 for val in patch_size_coord ): continue if black_white : patch = np . ones ( patch_size_coord , dtype = \"uint8\" ) colors = None else : patch = np . array ( self . slide . read_region ( tuple ( coord ), vis_level , patch_size_coord ) . convert ( \"RGB\" ) ) if colors is not None : assert len ( colors ) == len ( idxs ), ( \"The number of colors provided must match \" \"the number of selected coordinates.\" ) color = colors [ idx ] color_patch = ( np . ones (( patch_size_coord [ 1 ], patch_size_coord [ 0 ], 3 )) * color ) . astype ( \"uint8\" ) canvas [ coord_downsampled [ 1 ] : coord_downsampled [ 1 ] + patch_size_coord [ 1 ], coord_downsampled [ 0 ] : coord_downsampled [ 0 ] + patch_size_coord [ 0 ], :, ] = cv2 . addWeighted ( color_patch , alpha , patch , 1 - alpha , 0 , patch ) else : canvas [ coord_downsampled [ 1 ] : coord_downsampled [ 1 ] + patch_size_coord [ 1 ], coord_downsampled [ 0 ] : coord_downsampled [ 0 ] + patch_size_coord [ 0 ], :, ] = patch if draw_grid : cv2 . rectangle ( canvas , tuple ( np . maximum ([ 0 , 0 ], coord_downsampled - 1 )), tuple ( coord_downsampled + patch_size_coord ), ( 0 , 0 , 0 , 255 ), thickness = 2 , ) img = Image . fromarray ( canvas ) return img visualize ( vis_level , crop_roi = False , contours_color = ( 255 , 0 , 0 ), line_thickness = 500 , max_size = None , number_contours = False , black_white = False , view_slide_only = False ) Visualize the slide with or without the contours of the tissue. Parameters: Name Type Description Default vis_level int The level at which the visualization should be performed. required crop_roi bool Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the set_roi method). False contours_color tuple [ int , int , int ] The color to use for the contours. (255, 0, 0) line_thickness int The thickness to use for the contours 500 max_size int | None The maximum size for the visualization for the width or height of the image. None number_contours bool Set to True to number the contours. False black_white bool Set to True to visualize a binary mask of the contoured tissue. False view_slide_only bool Set to True to visualize the slide only (without the contours). False Returns: Type Description Image A PIL image of the visualization. Source code in src/prismtoolbox/wsicore/wsi.py 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 def visualize ( self , vis_level : int , crop_roi : bool = False , contours_color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), line_thickness : int = 500 , max_size : int | None = None , number_contours : bool = False , black_white : bool = False , view_slide_only : bool = False , ) -> Image . Image : \"\"\"Visualize the slide with or without the contours of the tissue. Args: vis_level: The level at which the visualization should be performed. crop_roi: Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). contours_color: The color to use for the contours. line_thickness: The thickness to use for the contours max_size: The maximum size for the visualization for the width or height of the image. number_contours: Set to True to number the contours. black_white: Set to True to visualize a binary mask of the contoured tissue. view_slide_only: Set to True to visualize the slide only (without the contours). Returns: A PIL image of the visualization. \"\"\" assert line_thickness > 0 , \"line_thickness must be greater than 0\" scale = 1 / self . level_downsamples [ vis_level ] if black_white : img = np . zeros_like ( self . create_thumbnail ( vis_level , crop_roi ), dtype = \"uint8\" ) line_thickness = - 1 contours_color = ( 1 , 1 , 1 ) else : img = np . array ( self . create_thumbnail ( vis_level , crop_roi )) line_thickness = int ( line_thickness * scale ) if not view_slide_only : if self . tissue_contours is None : raise RuntimeError ( f \"No tissue contours found for the slide { self . slide_name } , please run the detect_tissue method first.\" ) if crop_roi : if self . ROI is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) offset = self . ROI [: 2 ] else : offset = np . array ([ 0 , 0 ]) contours = [ cont - offset for cont in self . tissue_contours ] contours = self . scale_contours ( contours , scale ) if len ( contours ) > 0 : if not number_contours : cv2 . drawContours ( img , contours , - 1 , contours_color , line_thickness , lineType = cv2 . LINE_8 , ) else : # add numbering to each contour for idx , cont in enumerate ( contours ): M = cv2 . moments ( cont ) cX = int ( M [ \"m10\" ] / ( M [ \"m00\" ] + 1e-9 )) cY = int ( M [ \"m01\" ] / ( M [ \"m00\" ] + 1e-9 )) # draw the contour and put text next to center cv2 . drawContours ( img , [ cont ], - 1 , contours_color , line_thickness , lineType = cv2 . LINE_8 , ) cv2 . putText ( img , \" {} \" . format ( idx ), ( cX , cY ), cv2 . FONT_HERSHEY_SIMPLEX , 2 , ( 255 , 0 , 0 ), 10 , ) img = Image . fromarray ( img ) if black_white : img = img . convert ( \"L\" ) w , h = img . size if max_size is not None and ( w > max_size or h > max_size ): resizeFactor = max_size / w if w > h else max_size / h img = img . resize (( int ( w * resizeFactor ), int ( h * resizeFactor ))) return img worker_init ( slide_path , engine ) staticmethod Initialize the worker process with a wsi object. Parameters: Name Type Description Default slide_path str The path to the slide. required engine str The backend library to use for reading the slide (currently only openslide and tiffslide are supported) required Source code in src/prismtoolbox/wsicore/wsi.py 170 171 172 173 174 175 176 177 178 179 180 @staticmethod def worker_init ( slide_path : str , engine : str ) -> None : \"\"\"Initialize the worker process with a wsi object. Args: slide_path: The path to the slide. engine: The backend library to use for reading the slide (currently only openslide and tiffslide are supported) \"\"\" global wsi wsi = WSI . read ( slide_path , engine )","title":"Index"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI","text":"The WSI (Whole Slide Image) class is responsible for handling operations related to whole slide images. Parameters: Name Type Description Default slide_path str The path to the slide image file. required engine str The engine used to read the slide image. 'openslide' Attributes: Name Type Description slide_path str The path to the slide image file. engine str The engine used to read the slide image. slide_name str The name of the slide image file. Retrieved from the slide path using retrieve_slide_name_ext method. slide_ext str The extension of the slide image file. Retrieved from the slide path using retrieve_slide_name_ext method. slide OpenSlide | TiffSlide The wsi read from the file using engine. dimensions list [ tuple [ int , int ]] The dimensions of the slide image. Set by the set_slide_attributes method. level_dimensions list [ tuple [ int , int ]] The dimensions of the different levels of the slide image. Set by the set_slide_attributes method. level_downsamples list [ tuple [ int , int ]] The downsampling factors of the different levels of the slide image. Set by the set_slide_attributes method. properties dict The properties of the slide image. Set by the set_slide_attributes method. offset tuple [ int , int ] The offset of the slide image. Set by the set_slide_attributes method. ROI ndarray | None The region of interest in the slide image. Please use the set_roi method to set the ROI. ROI_width int | None The width of the region of interest. Set by the set_roi method. ROI_height int | None The height of the region of interest. Set by the set_roi method. tissue_contours list [ ndarray ] | None The contours of the tissue in the slide image. Please use the detect_tissue method to detect the tissue contours. coords ndarray | None The coordinates of patches extracted from slide image. Please use the extract_patches method to extract patches. coords_attrs dict | None The attributes of the coordinates. Set by the extract_patches method. Source code in src/prismtoolbox/wsicore/wsi.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , slide_path : str , engine : str = \"openslide\" ): \"\"\"The WSI (Whole Slide Image) class is responsible for handling operations related to whole slide images. Args: slide_path: The path to the slide image file. engine: The engine used to read the slide image. Attributes: slide_path (str): The path to the slide image file. engine (str): The engine used to read the slide image. slide_name (str): The name of the slide image file. Retrieved from the slide path using [retrieve_slide_name_ext][prismtoolbox.wsicore.WSI.retrieve_slide_name_ext] method. slide_ext (str): The extension of the slide image file. Retrieved from the slide path using [retrieve_slide_name_ext][prismtoolbox.wsicore.WSI.retrieve_slide_name_ext] method. slide (OpenSlide | TiffSlide): The wsi read from the file using engine. dimensions (list[tuple[int, int]]): The dimensions of the slide image. Set by the set_slide_attributes method. level_dimensions (list[tuple[int, int]]): The dimensions of the different levels of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. level_downsamples (list[tuple[int, int]]): The downsampling factors of the different levels of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. properties (dict): The properties of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. offset (tuple[int, int]): The offset of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. ROI (ndarray | None): The region of interest in the slide image. Please use the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method to set the ROI. ROI_width (int | None): The width of the region of interest. Set by the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method. ROI_height (int | None): The height of the region of interest. Set by the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method. tissue_contours (list[ndarray] | None): The contours of the tissue in the slide image. Please use the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method to detect the tissue contours. coords (np.ndarray | None): The coordinates of patches extracted from slide image. Please use the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method to extract patches. coords_attrs (dict | None): The attributes of the coordinates. Set by the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method. \"\"\" self . slide_path = slide_path self . engine = engine self . slide_name , self . slide_ext = self . retrieve_slide_name_ext ( self . slide_path ) self . slide = self . read ( slide_path , engine ) self . offset = ( 0 , 0 ) self . set_slide_attributes () self . ROI = None self . ROI_width = None self . ROI_height = None self . tissue_contours = None self . coords = None self . coords_attrs = None","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;WSI"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.apply_pathologist_annotations","text":"Apply pathologist annotations to the tissue contours by intersecting the annotations with the tissue contours. Requires the tissue contours to be set for the slide beforehand with the detect_tissue method. Parameters: Name Type Description Default path str The path to the .geojson file containing the annotations extracted from QuPath. required class_name str The class name to use for selecting the annotations to apply. 'annotation' column_to_select str The column to select in the GeoDataFrame. 'objectType' Source code in src/prismtoolbox/wsicore/wsi.py 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 def apply_pathologist_annotations ( self , path : str , class_name : str = \"annotation\" , column_to_select : str = \"objectType\" , ) -> None : \"\"\"Apply pathologist annotations to the tissue contours by intersecting the annotations with the tissue contours. Requires the tissue contours to be set for the slide beforehand with the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method. Args: path: The path to the .geojson file containing the annotations extracted from QuPath. class_name: The class name to use for selecting the annotations to apply. column_to_select: The column to select in the GeoDataFrame. \"\"\" assert ( self . tissue_contours is not None ), \"No tissue contours found for the slide, please run the detect_tissue method first\" offset = ( - self . offset [ 0 ], - self . offset [ 1 ]) pathologist_annotations = read_qupath_annotations ( path , offset = offset , class_name = class_name , column_to_select = column_to_select ) polygons = contoursToPolygons ( self . tissue_contours , make_valid = True ) intersection = intersectionPolygons ( polygons , pathologist_annotations ) self . tissue_contours = PolygonsToContours ( intersection )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;apply_pathologist_annotations"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.convert_micrometer_to_pixel","text":"Convert a value from micrometer to pixel. Parameters: Name Type Description Default value float The value to convert (in micrometer). required level int The level at which the conversion should be performed. required axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description int The input value in pixel. Source code in src/prismtoolbox/wsicore/wsi.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def convert_micrometer_to_pixel ( self , value : float , level : int , axis : str = \"x\" , ) -> int : \"\"\"Convert a value from micrometer to pixel. Args: value: The value to convert (in micrometer). level: The level at which the conversion should be performed. axis: The axis to use for getting the conversion factor (x or y). Returns: The input value in pixel. \"\"\" return int ( value / float ( self . properties [ f \" { self . engine } .mpp- { axis } \" ])) // int ( self . level_downsamples [ level ] )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;convert_micrometer_to_pixel"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.convert_pixel_to_micrometer","text":"Convert a value from pixel to micrometer. Parameters: Name Type Description Default value float The value to convert (in pixel). required level int The level at which the conversion should be performed. required axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description float The input value in micrometer. Source code in src/prismtoolbox/wsicore/wsi.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def convert_pixel_to_micrometer ( self , value : float , level : int , axis : str = \"x\" , ) -> float : \"\"\"Convert a value from pixel to micrometer. Args: value: The value to convert (in pixel). level: The level at which the conversion should be performed. axis: The axis to use for getting the conversion factor (x or y). Returns: The input value in micrometer. \"\"\" return ( value * float ( self . properties [ f \" { self . engine } .mpp- { axis } \" ]) * self . level_downsamples [ level ] )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;convert_pixel_to_micrometer"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.convert_units","text":"Convert a value from one unit to another. Parameters: Name Type Description Default value float The value to convert. required level int The level at which the conversion should be performed. required from_unit str The unit to convert from (px or micro). required to_unit str The unit to convert to (px or micro). 'px' axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description int | float The input value converted in the desired unit. Source code in src/prismtoolbox/wsicore/wsi.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 def convert_units ( self , value : float , level : int , from_unit : str , to_unit : str = \"px\" , axis : str = \"x\" , ) -> int | float : \"\"\"Convert a value from one unit to another. Args: value: The value to convert. level: The level at which the conversion should be performed. from_unit: The unit to convert from (px or micro). to_unit: The unit to convert to (px or micro). axis: The axis to use for getting the conversion factor (x or y). Returns: The input value converted in the desired unit. \"\"\" if from_unit == \"micro\" and to_unit == \"px\" : value = self . convert_micrometer_to_pixel ( value , level , axis ) elif from_unit == \"px\" and to_unit == \"micro\" : value = self . convert_pixel_to_micrometer ( value , level , axis ) elif from_unit == to_unit : pass else : raise ValueError ( f \"Conversion from { from_unit } to { to_unit } not supported.\" ) return value","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;convert_units"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.create_thumbnail","text":"Create a thumbnail of the slide at a given level. Parameters: Name Type Description Default level int The level at which the thumbnail should be created. required crop_roi bool A boolean to crop the thumbnail to the region of interest defined for the slide (requires a ROI to be set for the slide beforehand with the set_roi method) False Returns: Type Description Image A thumbnail of the slide as a PIL image. Source code in src/prismtoolbox/wsicore/wsi.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 def create_thumbnail ( self , level : int , crop_roi : bool = False , ) -> Image . Image : \"\"\"Create a thumbnail of the slide at a given level. Args: level: The level at which the thumbnail should be created. crop_roi: A boolean to crop the thumbnail to the region of interest defined for the slide (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method) Returns: A thumbnail of the slide as a PIL image. \"\"\" thumb = self . read_region (( 0 , 0 ), level , self . level_dimensions [ level ]) . convert ( \"RGB\" ) if crop_roi : if self . ROI is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) log . info ( f \"Creating thumbnail with ROI { self . ROI } .\" ) coords_roi = ( self . ROI / self . level_downsamples [ level ]) . astype ( int ) thumb = thumb . crop ( tuple ( coords_roi )) return thumb","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;create_thumbnail"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.detect_tissue","text":"Segment the tissue on the slide based on a threshold on the Law's texture energy spot map and floodfill algorithm to fill the holes in the mask. The tissue contours are stored in the tissue_contours attribute. Parameters: Name Type Description Default seg_level int The level at which the segmentation should be performed. required window_avg int The size of the window for local averaging. required window_eng int The size of the window for Law's texture energy computation. required thresh int The threshold for binarization on the Law's texture energy spot map. required inside_roi bool Set to True to identify the tissue only within a ROI (requires a ROI to be set for the slide beforehand with the set_roi method). False inv_thresh bool Set to True to invert the thresholding. False area_min float The minimum area for a contour to be kept. 200000.0 start tuple [ int , int ] The starting point for the floodfill algorithm (should be left at (0, 0) in most cases). (0, 0) Source code in src/prismtoolbox/wsicore/wsi.py 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def detect_tissue ( self , seg_level : int , window_avg : int , window_eng : int , thresh : int , inside_roi : bool = False , inv_thresh : bool = False , area_min : float = 2e5 , start : tuple [ int , int ] = ( 0 , 0 ), ) -> None : \"\"\"Segment the tissue on the slide based on a threshold on the Law's texture energy spot map and floodfill algorithm to fill the holes in the mask. The tissue contours are stored in the tissue_contours attribute. Args: seg_level: The level at which the segmentation should be performed. window_avg: The size of the window for local averaging. window_eng: The size of the window for Law's texture energy computation. thresh: The threshold for binarization on the Law's texture energy spot map. inside_roi: Set to True to identify the tissue only within a ROI (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). inv_thresh: Set to True to invert the thresholding. area_min: The minimum area for a contour to be kept. start: The starting point for the floodfill algorithm (should be left at (0, 0) in most cases). \"\"\" final_contours = [] img = np . array ( self . create_thumbnail ( seg_level , inside_roi )) img_avg = local_average ( np . asarray ( img ), window_avg ) law_feats = compute_law_feats ( img_avg , window_eng )[ 0 ] filterred_img = apply_bilateral_filter ( np . clip ( law_feats , 0 , 255 ) . astype ( \"uint8\" )) threshed_img = apply_binary_thresh ( filterred_img , thresh , inv_thresh ) flooded_img = floodfill_img ( np . pad ( threshed_img , 1 ), start ) contours = contour_mask ( flooded_img ) for contour in contours : c = contour . copy () area = cv2 . contourArea ( c ) if area > area_min : final_contours . append ( contour ) if len ( final_contours ) == 0 : self . tissue_contours = [] log . warning ( f \"No tissue contours found for the slide { self . slide_name } .\" ) else : scale = self . level_downsamples [ seg_level ] offset = np . array ( self . ROI [: 2 ]) if self . ROI is not None else np . array ([ 0 , 0 ]) final_contours = self . scale_contours ( final_contours , scale ) final_contours = [ cont + offset for cont in final_contours ] # Sanity check to ensure that the contours are all within a ROI is provided if self . ROI is not None : assert all ( [ np . all ( cont >= self . ROI [: 2 ]) and np . all ( cont <= self . ROI [ 2 :]) for cont in final_contours ] ) self . tissue_contours = final_contours log . info ( f \"Identified { len ( final_contours ) } contours for the slide { self . slide_name } .\" ) return","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;detect_tissue"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.extract_patches","text":"Extract valid patches from the slide with different extraction modes. A patch is considered valid if it is not black or white and is within the region of interest or the tissue contours if relevant. The extracted patches are stored as coordinates in the coords attribute, and the attributes of the coordinates are stored in the coords_attrs attribute. Parameters: Name Type Description Default patch_size float The size of the patches to extract (assumed to be square). required patch_level int The level at which the patches should be extracted. required mode str The mode to use for the extraction: \"contours\" mode extracts patches within the tissue contours (requires the tissue contours to be set for the slide beforehand with the detect_tissue method). \"roi\" mode extracts patches within the region of interest (requires the ROI to be set for the slide beforehand with the set_roi method). \"all\" mode extracts patches from the entire slide required step_size float | None The step size for the sliding window (if set to None, the step size will be computed based on the overlap). None overlap float | None The overlap between patches as an absolute value (must be provided if step_size is set to None). None units tuple [ str , str ] The units for the patch size and step size/overlap values (pixels: px, micrometers: micro). ('px', 'px') use_padding bool Set to True to use padding for the extraction. True contours_mode str | None The mode to use for the contour checking function (must be provided if mode is set to contours). See IsInContour for more details. None rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Source code in src/prismtoolbox/wsicore/wsi.py 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 def extract_patches ( self , patch_size : float , patch_level : int , mode : str , step_size : float | None = None , overlap : float | None = None , units : tuple [ str , str ] = ( \"px\" , \"px\" ), use_padding : bool = True , contours_mode : str | None = None , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> None : \"\"\"Extract valid patches from the slide with different extraction modes. A patch is considered valid if it is not black or white and is within the region of interest or the tissue contours if relevant. The extracted patches are stored as coordinates in the coords attribute, and the attributes of the coordinates are stored in the coords_attrs attribute. Args: patch_size: The size of the patches to extract (assumed to be square). patch_level: The level at which the patches should be extracted. mode: The mode to use for the extraction: - \"contours\" mode extracts patches within the tissue contours (requires the tissue contours to be set for the slide beforehand with the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method). - \"roi\" mode extracts patches within the region of interest (requires the ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). - \"all\" mode extracts patches from the entire slide step_size: The step size for the sliding window (if set to None, the step size will be computed based on the overlap). overlap: The overlap between patches as an absolute value (must be provided if step_size is set to None). units: The units for the patch size and step size/overlap values (pixels: px, micrometers: micro). use_padding: Set to True to use padding for the extraction. contours_mode: The mode to use for the contour checking function (must be provided if mode is set to contours). See [IsInContour][prismtoolbox.wsicore.core_utils.IsInContour] for more details. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. \"\"\" assert all ( [ unit in [ \"micro\" , \"px\" ] for unit in units ] ), \"Units must be either 'micro' or 'px'\" patch_size = int ( self . convert_units ( patch_size , patch_level , units [ 0 ])) if step_size is None : if overlap is None : raise ValueError ( \"Either step_size or overlap must be provided if step_size is not set\" ) step_size = int ( patch_size - self . convert_units ( overlap , patch_level , units [ 1 ])) else : step_size = int ( self . convert_units ( step_size , patch_level , units [ 1 ])) log . info ( f \"Extracting patches of size { patch_size } at level { patch_level } with step size { step_size } .\" ) if mode == \"contours\" : log . info ( \"Extracting patches with 'contours' mode.\" ) assert self . tissue_contours is not None , \"Empty tissue contours vector for the slide, please run the detect_tissue method first.\" assert len ( self . tissue_contours ) > 0 , \"No tissue contours found for the slide.\" assert contours_mode is not None , \"Contours mode must be provided if mode is set to 'contours'.\" valid_coords = [] for cont in self . tissue_contours : roi_dim = cv2 . boundingRect ( cont ) # type: ignore log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords . extend ( self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , cont , contours_mode , rgb_threshs = rgb_threshs , percentages = percentages , ) ) valid_coords = np . array ( valid_coords ) elif mode == \"roi\" : log . info ( \"Extracting patches with 'roi' mode.\" ) if self . ROI is None or self . ROI_width is None or self . ROI_height is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) roi_dim = self . ROI [ 0 ], self . ROI [ 1 ], self . ROI_width , self . ROI_height log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords = self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , rgb_threshs = rgb_threshs , percentages = percentages , ) elif mode == \"all\" : roi_dim = 0 , 0 , self . level_dimensions [ 0 ][ 0 ], self . level_dimensions [ 0 ][ 1 ] log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords = self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , rgb_threshs = rgb_threshs , percentages = percentages , ) else : raise ValueError ( f \"Mode { mode } not supported\" ) attr = { \"patch_size\" : patch_size , \"patch_level\" : patch_level , \"downsample\" : self . level_downsamples [ patch_level ], \"downsampled_level_dim\" : tuple ( np . array ( self . level_dimensions [ patch_level ])), \"level_dim\" : self . level_dimensions [ patch_level ], \"name\" : self . slide_name , } if len ( valid_coords ) == 0 : log . warning ( f \"No valid coordinates found for the slide { self . slide_name } .\" ) else : log . info ( f \"Identified a total of { len ( valid_coords ) } valid coordinates for the slide { self . slide_name } .\" ) self . coords = valid_coords self . coords_attrs = attr","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;extract_patches"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.extract_patches_roi","text":"Extract valid patches from a region of interest, i.e if the patch is not black or white and is within the region of interest/contours if relevant). Parameters: Name Type Description Default patch_level int The level at which the patches should be extracted. required patch_size int The size of the patches to extract (assumed to be square). required step_size int | None The step size to use for the sliding window. None roi_dim tuple [ int , int , int , int ] | None The top-left corner coordinates and dimensions of the region of interest. Must be provided if coord_candidates is set to None. None use_padding bool Set to True to use padding for the extraction. True contour ndarray | None The tissue contour to use for the extraction. If set to None, will not check if patches are within a contour. None contours_mode str | None The mode for the contour checking function. See IsInContour for more details. Must be provided if mode is set to contours. Otherwise, will not check if patches are within the contours. None coord_candidates ndarray | None Precomputed candidate coordinates for the patches. If set to None, will compute the candidate coordinates based on the ROI dimensions and the step size. None rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) return_indices bool Set to True to return the indices of the valid coordinates. False Returns: Type Description ndarray | tuple [ ndarray , list [ int ]] An array of valid coordinates for the patches (i.e. coordinates of the top-left corner of the patches). Source code in src/prismtoolbox/wsicore/wsi.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 def extract_patches_roi ( self , patch_level : int , patch_size : int , step_size : int | None = None , roi_dim : tuple [ int , int , int , int ] | None = None , use_padding : bool = True , contour : np . ndarray | None = None , contours_mode : str | None = None , coord_candidates : np . ndarray | None = None , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), return_indices : bool = False , ) -> np . ndarray | tuple [ np . ndarray , list [ int ]]: \"\"\"Extract valid patches from a region of interest, i.e if the patch is not black or white and is within the region of interest/contours if relevant). Args: patch_level: The level at which the patches should be extracted. patch_size: The size of the patches to extract (assumed to be square). step_size: The step size to use for the sliding window. roi_dim: The top-left corner coordinates and dimensions of the region of interest. Must be provided if coord_candidates is set to None. use_padding: Set to True to use padding for the extraction. contour: The tissue contour to use for the extraction. If set to None, will not check if patches are within a contour. contours_mode: The mode for the contour checking function. See [IsInContour][prismtoolbox.wsicore.core_utils.IsInContour] for more details. Must be provided if mode is set to contours. Otherwise, will not check if patches are within the contours. coord_candidates: Precomputed candidate coordinates for the patches. If set to None, will compute the candidate coordinates based on the ROI dimensions and the step size. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. return_indices: Set to True to return the indices of the valid coordinates. Returns: An array of valid coordinates for the patches (i.e. coordinates of the top-left corner of the patches). \"\"\" patch_downsample = int ( self . level_downsamples [ patch_level ]) ref_patch_size = patch_size * patch_downsample if coord_candidates is None : if roi_dim is None or step_size is None : raise ValueError ( \"roi_dim and step_size must be provided if coord_candidates is not set.\" ) start_x , start_y , w , h = roi_dim img_w , img_h = self . level_dimensions [ 0 ] if use_padding : stop_y = start_y + h stop_x = start_x + w else : stop_y = min ( start_y + h , img_h - ref_patch_size + 1 ) stop_x = min ( start_x + w , img_w - ref_patch_size + 1 ) step_size = step_size * patch_downsample x_range = np . arange ( start_x , stop_x , step = step_size ) y_range = np . arange ( start_y , stop_y , step = step_size ) x_coords , y_coords = np . meshgrid ( x_range , y_range , indexing = \"ij\" ) coord_candidates = np . array ( [ x_coords . flatten (), y_coords . flatten ()] ) . transpose () if contour is not None : if contours_mode is None : raise ValueError ( \"A contour mode must be provided if patch extraction mode is set to contours.\" ) cont_check_fn = IsInContour ( contour , patch_size = ref_patch_size , center_shift = 0.5 , mode = contours_mode ) log . info ( f \"Extracting patches with contour checking function mode { contours_mode } .\" ) else : cont_check_fn = None num_workers = mp . cpu_count () pool = mp . Pool ( num_workers , initializer = WSI . worker_init , initargs = ( self . slide_path , self . engine , ), ) iterable = [ ( coord , cont_check_fn , patch_level , patch_size , rgb_threshs , percentages ) for coord in coord_candidates ] valid_coords = pool . starmap ( WSI . process_coord_candidate , iterable ) pool . close () valid_indices = [ i for i , coord in enumerate ( valid_coords ) if coord is not None ] valid_coords = np . array ([ coord_candidates [ i ] for i in valid_indices ]) log . info ( f \"Identified { len ( valid_coords ) } valid coordinates in the ROI { roi_dim } .\" ) if return_indices : return valid_coords , valid_indices else : return valid_coords","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;extract_patches_roi"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.is_black_white","text":"Check if a patch is black or white. Parameters: Name Type Description Default patch Image The input patch. required rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Returns: Type Description bool True if the patch is black or white, False otherwise. Source code in src/prismtoolbox/wsicore/wsi.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 @staticmethod def is_black_white ( patch : Image . Image , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> bool : \"\"\"Check if a patch is black or white. Args: patch: The input patch. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. Returns: True if the patch is black or white, False otherwise. \"\"\" return isBlackPatch ( patch , rgb_thresh = rgb_threshs [ 0 ], percentage = percentages [ 0 ] ) or isWhitePatch ( patch , rgb_thresh = rgb_threshs [ 1 ], percentage = percentages [ 1 ])","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;is_black_white"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.load_patches","text":"Load the patches from a hdf5 file. Parameters: Name Type Description Default file_path str The path to the hdf5 file containing the patches. required Source code in src/prismtoolbox/wsicore/wsi.py 462 463 464 465 466 467 468 469 def load_patches ( self , file_path : str ) -> None : \"\"\"Load the patches from a hdf5 file. Args: file_path: The path to the hdf5 file containing the patches. \"\"\" log . info ( f \"Loading patches for slide { self . slide_name } from { file_path } .\" ) self . coords , self . coords_attrs = read_h5_file ( file_path , \"coords\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;load_patches"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.load_tissue_contours","text":"Load the tissue contours from a pickle file. Parameters: Name Type Description Default file_path str The path to the pickle file containing the tissue contours. required Source code in src/prismtoolbox/wsicore/wsi.py 370 371 372 373 374 375 376 377 def load_tissue_contours ( self , file_path : str ) -> None : \"\"\"Load the tissue contours from a pickle file. Args: file_path: The path to the pickle file containing the tissue contours. \"\"\" log . info ( f \"Loading tissue contours for slide { self . slide_name } from { file_path } .\" ) self . tissue_contours = load_obj_with_pickle ( file_path )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;load_tissue_contours"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.process_coord_candidate","text":"Determine if a candidate coordinate is valid based on a contour checking function and/or black/white thresholding. Parameters: Name Type Description Default coord tuple [ int , int ] The candidate coordinate. required cont_check_fn IsInContour | None The contour checking function. required patch_level int The level at which the patch should be extracted. required patch_size int The size of the patch (assumed to be square). required rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Returns: Type Description tuple [ int , int ] | None The coordinate if it is valid, None otherwise. Source code in src/prismtoolbox/wsicore/wsi.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 @staticmethod def process_coord_candidate ( coord : tuple [ int , int ], cont_check_fn : IsInContour | None , patch_level : int , patch_size : int , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> tuple [ int , int ] | None : \"\"\"Determine if a candidate coordinate is valid based on a contour checking function and/or black/white thresholding. Args: coord: The candidate coordinate. cont_check_fn: The contour checking function. patch_level: The level at which the patch should be extracted. patch_size: The size of the patch (assumed to be square). rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. Returns: The coordinate if it is valid, None otherwise. \"\"\" if cont_check_fn is None or cont_check_fn ( coord ): patch = wsi . read_region ( coord , patch_level , ( patch_size , patch_size ) ) . convert ( \"RGB\" ) if not WSI . is_black_white ( patch , rgb_threshs , percentages ): return coord else : return None else : return None","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;process_coord_candidate"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.read","text":"Read a slide with a given engine. Parameters: Name Type Description Default slide_path str The path to the slide. required engine str The backend library to use for reading the slide (currently only openslide and tiffslide are supported). required Returns: Type Description A slide object. Source code in src/prismtoolbox/wsicore/wsi.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 @staticmethod def read ( slide_path : str , engine : str ): \"\"\"Read a slide with a given engine. Args: slide_path: The path to the slide. engine: The backend library to use for reading the slide (currently only openslide and tiffslide are supported). Returns: A slide object. \"\"\" if engine == \"openslide\" : import openslide slide = openslide . OpenSlide ( slide_path ) elif engine == \"tiffslide\" : import tiffslide slide = tiffslide . TiffSlide ( slide_path ) else : raise NotImplementedError ( f \"engine { engine } not supported\" ) return slide","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;read"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.read_region","text":"Read a region from the slide for a given level and size. Parameters: Name Type Description Default location tuple [ int , int ] The coordinates of the top left corner of the region (in pixels). required level int The level at which the region should be read. required size tuple [ int , int ] The size of the region (in pixels). required Returns: Type Description Image The desired region of the slide as a PIL image. Source code in src/prismtoolbox/wsicore/wsi.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def read_region ( self , location : tuple [ int , int ], level : int , size : tuple [ int , int ], ) -> Image . Image : \"\"\"Read a region from the slide for a given level and size. Args: location: The coordinates of the top left corner of the region (in pixels). level: The level at which the region should be read. size: The size of the region (in pixels). Returns: The desired region of the slide as a PIL image. \"\"\" return self . slide . read_region ( location , level , size ) . convert ( \"RGB\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;read_region"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.retrieve_slide_name_ext","text":"Retrieves slide name and slide extension from slide path. Parameters: Name Type Description Default slide_path str The path to the slide. required Returns: Type Description tuple [ str , str ] A tuple (slide name, slide ext). Source code in src/prismtoolbox/wsicore/wsi.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 @staticmethod def retrieve_slide_name_ext ( slide_path : str ) -> tuple [ str , str ]: \"\"\"Retrieves slide name and slide extension from slide path. Args: slide_path: The path to the slide. Returns: A tuple (slide name, slide ext). \"\"\" slide_name = re . search ( r \"([^/]+?)(?=\\.[\\w\\.]+$)\" , slide_path ) slide_ext = re . search ( r \"(?<=\\.)[\\w\\.]+$\" , slide_path ) if slide_ext is None or slide_name is None : raise ValueError ( f \"Could not retrieve slide name and extension from { slide_path } . \" \"Please check the file path.\" ) else : slide_name , slide_ext = slide_name . group ( 0 ), slide_ext . group ( 0 ) return slide_name , slide_ext","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;retrieve_slide_name_ext"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.save_patches","text":"Save the patches in a hdf5 or geojson file. Parameters: Name Type Description Default save_dir str The path to the directory where the patches will be saved. required file_format str The format for the saving (h5 for python processing, geojson for QuPath processing). 'h5' selected_idx ndarray | None An array of indices of the patches to save (if set to None, all the patches will be saved). None merge bool Set to True to merge the patches into a single polygon (for geojson format only). False label str | None An optional label to assign to the patches (for geojson format only). None color tuple [ int , int , int ] An optional color to assign to the patches (for geojson format only). (255, 0, 0) append_to_existing_file bool Set to True to append the patches to an existing geojson file. False Source code in src/prismtoolbox/wsicore/wsi.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def save_patches ( self , save_dir : str , file_format : str = \"h5\" , selected_idx : np . ndarray | None = None , merge : bool = False , label : str | None = None , color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), append_to_existing_file : bool = False , ) -> None : \"\"\"Save the patches in a hdf5 or geojson file. Args: save_dir: The path to the directory where the patches will be saved. file_format: The format for the saving (h5 for python processing, geojson for QuPath processing). selected_idx: An array of indices of the patches to save (if set to None, all the patches will be saved). merge: Set to True to merge the patches into a single polygon (for geojson format only). label: An optional label to assign to the patches (for geojson format only). color: An optional color to assign to the patches (for geojson format only). append_to_existing_file: Set to True to append the patches to an existing geojson file. \"\"\" if self . coords is None : raise ValueError ( \"No patches found. Please check if patches were correctly extracted.\" ) if self . coords_attrs is None : raise ValueError ( \"No attributes set for the patches. \" \"Please check if patches were correctly extracted.\" ) if selected_idx is not None : coords = np . array ( self . coords [ selected_idx ]) else : coords = np . array ( self . coords ) if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if file_format == \"h5\" : asset_dict = { \"coords\" : coords } attr_dict = { \"coords\" : self . coords_attrs } file_path = os . path . join ( save_dir , f \" { self . slide_name } .h5\" ) log . info ( f \"Saving patches for slide { self . slide_name } at { file_path } as h5 file.\" ) save_patches_with_hdf5 ( file_path , asset_dict , attr_dict ) elif file_format == \"geojson\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .geojson\" ) patch_downsample = int ( self . level_downsamples [ self . coords_attrs [ \"patch_level\" ]] ) polygons = patchesToPolygons ( coords , self . coords_attrs [ \"patch_size\" ], patch_downsample , merge ) log . info ( f \"Saving { len ( coords ) } patches for slide { self . slide_name } at { file_path } as geojson file.\" ) export_polygons_to_qupath ( polygons , file_path , \"annotation\" , offset = self . offset , label = label , color = color , append_to_existing_file = append_to_existing_file , ) elif file_format == \"jpg\" or file_format == \"png\" : save_dir = os . path . join ( save_dir , self . slide_name ) if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) log . info ( f \"Saving { len ( coords ) } patches for slide { self . slide_name } at { save_dir } with { file_format } .\" ) for coord in coords : patch = self . read_region ( coord , self . coords_attrs [ \"patch_level\" ], ( self . coords_attrs [ \"patch_size\" ], self . coords_attrs [ \"patch_size\" ]), ) . convert ( \"RGB\" ) patch . save ( os . path . join ( save_dir , f \" { coord [ 0 ] } _ { coord [ 1 ] } . { file_format } \" )) else : raise ValueError ( f \"Format { file_format } not supported.\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;save_patches"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.save_tissue_contours","text":"Save the tissue contours in a pickle or geojson file. Parameters: Name Type Description Default save_dir str The path to the directory where the contours will be saved. required selected_idx ndarray | None An array of indices of the contours to save (if set to None, all the contours will be saved). None file_format str The file format for saving the contours (pickle for python processing, geojson for QuPath processing). 'pickle' merge bool Set to True to merge the contours into a single polygon (for geojson format only). False label str | None An optional label to assign to the tissue contours (for geojson format only). None color tuple [ int , int , int ] An optional color to assign to the tissue contours (for geojson format only). (255, 0, 0) append_to_existing_file bool Set to True to append the contours to an existing geojson file. False make_valid bool Set to True to make the polygons valid (for geojson format only). False Source code in src/prismtoolbox/wsicore/wsi.py 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def save_tissue_contours ( self , save_dir : str , selected_idx : np . ndarray | None = None , file_format : str = \"pickle\" , merge : bool = False , label : str | None = None , color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), append_to_existing_file : bool = False , make_valid : bool = False , ) -> None : \"\"\"Save the tissue contours in a pickle or geojson file. Args: save_dir: The path to the directory where the contours will be saved. selected_idx: An array of indices of the contours to save (if set to None, all the contours will be saved). file_format: The file format for saving the contours (pickle for python processing, geojson for QuPath processing). merge: Set to True to merge the contours into a single polygon (for geojson format only). label: An optional label to assign to the tissue contours (for geojson format only). color: An optional color to assign to the tissue contours (for geojson format only). append_to_existing_file: Set to True to append the contours to an existing geojson file. make_valid: Set to True to make the polygons valid (for geojson format only). \"\"\" assert self . tissue_contours is not None , ( \"No tissue contours found for the slide, \" \"please run the detect_tissue method first\" ) if selected_idx is not None : tissue_contours = [ self . tissue_contours [ idx ] for idx in selected_idx ] else : tissue_contours = self . tissue_contours if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if file_format == \"pickle\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .pkl\" ) log . info ( f \"Saving tissue contours for slide { self . slide_name } at { file_path } as pickle file.\" ) save_obj_with_pickle ( tissue_contours , file_path ) elif file_format == \"geojson\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .geojson\" ) log . info ( f \"Saving { selected_idx } tissue contours for slide { self . slide_name } at { file_path } as geojson file.\" ) polygons = contoursToPolygons ( tissue_contours , merge , make_valid ) export_polygons_to_qupath ( polygons , file_path , \"annotation\" , offset = self . offset , label = label , color = color , append_to_existing_file = append_to_existing_file , ) else : raise ValueError ( f \"format { file_format } not supported\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;save_tissue_contours"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.scale_contours","text":"Scale the contours by a given factor. Parameters: Name Type Description Default contours list [ ndarray ] The contours to scale. required scale float The scale factor to apply. required Returns: Type Description list [ ndarray ] The input list with scaled contours. Source code in src/prismtoolbox/wsicore/wsi.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 @staticmethod def scale_contours ( contours : list [ np . ndarray ], scale : float , ) -> list [ np . ndarray ]: \"\"\"Scale the contours by a given factor. Args: contours: The contours to scale. scale: The scale factor to apply. Returns: The input list with scaled contours. \"\"\" scaled_contours = [ np . array ( cont * scale , dtype = \"int\" ) for cont in contours ] return scaled_contours","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;scale_contours"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.set_roi","text":"Set the region of interest for the slide. Can be set manually or by selecting a region on a thumbnail. The ROI is stored as a tuple in the ROI attribute. Parameters: Name Type Description Default roi tuple [ int , int , int , int ] | None Set the region of interest manually as a tuple (x1, y1, x2, y2). None rois_df_path str | None The path to dataframe containing the ROIs with a slide_id column identifying the slide, and the ROI coordinates as columns (x1, y1, x2, y2). If roi is not provided, the ROI will be set from this dataframe. None Returns: The region of interest set for the slide as a tuple (x1, y1, x2, y2). Source code in src/prismtoolbox/wsicore/wsi.py 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def set_roi ( self , roi : tuple [ int , int , int , int ] | None = None , rois_df_path : str | None = None , ) -> np . ndarray : \"\"\"Set the region of interest for the slide. Can be set manually or by selecting a region on a thumbnail. The ROI is stored as a tuple in the ROI attribute. Args: roi: Set the region of interest manually as a tuple (x1, y1, x2, y2). rois_df_path: The path to dataframe containing the ROIs with a slide_id column identifying the slide, and the ROI coordinates as columns (x1, y1, x2, y2). If roi is not provided, the ROI will be set from this dataframe. Returns: The region of interest set for the slide as a tuple (x1, y1, x2, y2). \"\"\" if roi is not None : ROI = np . array ( roi ) . astype ( int ) elif rois_df_path is not None : rois_df = pd . read_csv ( rois_df_path ) if \"slide_id\" not in rois_df . columns or len ( rois_df . columns ) < 5 : raise ValueError ( \"The provided dataframe does not contain a 'slide_id' column, or does not have enough columns for ROI coordinates.\" ) ROI = rois_df [ rois_df . slide_id == self . slide_name ] . values [ 0 ] . astype ( int ) else : log . info ( \"No ROI provided, prompting user to select one.\" ) level = input ( f \"No ROI was provided, please select a level at which the ROI should be created (max level: { len ( self . level_dimensions ) - 1 } ): \" ) if not level : log . info ( \"No level provided, setting the ROI at the highest level.\" ) level = len ( self . level_downsamples ) - 1 else : level = int ( level ) img = np . array ( self . create_thumbnail ( level )) ROI = select_roi_on_thumbnail ( img , int ( self . level_downsamples [ level ])) ROI = ( ROI * self . level_downsamples [ level ]) . astype ( int ) self . ROI = ROI self . ROI_width = ROI [ 2 ] - ROI [ 0 ] self . ROI_height = ROI [ 3 ] - ROI [ 1 ] log . info ( f \"ROI for slide { self . slide_name } has been set to { self . ROI } .\" ) return ROI","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;set_roi"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.set_slide_attributes","text":"Set the slide attributes. Source code in src/prismtoolbox/wsicore/wsi.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def set_slide_attributes ( self ): \"\"\"Set the slide attributes.\"\"\" if self . engine == \"openslide\" : self . dimensions = self . slide . dimensions self . level_dimensions = self . slide . level_dimensions self . level_downsamples = self . slide . level_downsamples self . properties = self . slide . properties elif self . engine == \"tiffslide\" : self . dimensions = self . slide . dimensions self . level_dimensions = self . slide . level_dimensions self . level_downsamples = self . slide . level_downsamples self . properties = self . slide . properties else : raise NotImplementedError ( f \"Engine { self . engine } not supported.\" ) if ( f \" { self . engine } .bounds-x\" in self . properties . keys () and self . properties [ f \" { self . engine } .bounds-x\" ] is not None ): self . offset = ( - int ( self . properties [ f \" { self . engine } .bounds-x\" ]), - int ( self . properties [ f \" { self . engine } .bounds-y\" ]), )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;set_slide_attributes"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.stitch","text":"Stitch the patches extracted on an image. The patches can be masked and colored depending on the mask and colors provided. Requires the coordinates of the patches to be set for the slide beforehand with the extract_patches method. Parameters: Name Type Description Default vis_level int The level at which the patches should be visualized. required selected_idx ndarray | None An array of indices of the patches to visualize (if set to None, all the patches will be visualized). None colors ndarray | None An array of RGB colors to apply to the patches (if set to None, the patches will be visualized as they are). None alpha float Set the transparency of the colors to apply to the patches. 0.6 black_white bool Set to True to visualize a binary mask of the patches extracted. False draw_grid bool Set to True to draw a grid on the stitched patches. False crop_roi bool Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the set_roi method). False background_color tuple [ int , int , int ] The color of the background. (0, 0, 0) Returns: Type Description Image A PIL image of the stitched patches. Source code in src/prismtoolbox/wsicore/wsi.py 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 def stitch ( self , vis_level : int , selected_idx : np . ndarray | None = None , colors : np . ndarray | None = None , alpha : float = 0.6 , black_white : bool = False , draw_grid : bool = False , crop_roi : bool = False , background_color : tuple [ int , int , int ] = ( 0 , 0 , 0 ), ) -> Image . Image : \"\"\"Stitch the patches extracted on an image. The patches can be masked and colored depending on the mask and colors provided. Requires the coordinates of the patches to be set for the slide beforehand with the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method. Args: vis_level: The level at which the patches should be visualized. selected_idx: An array of indices of the patches to visualize (if set to None, all the patches will be visualized). colors: An array of RGB colors to apply to the patches (if set to None, the patches will be visualized as they are). alpha: Set the transparency of the colors to apply to the patches. black_white: Set to True to visualize a binary mask of the patches extracted. draw_grid: Set to True to draw a grid on the stitched patches. crop_roi: Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). background_color: The color of the background. Returns: A PIL image of the stitched patches. \"\"\" if self . coords_attrs is None : raise RuntimeError ( f \"No attributes set for the patches of the slide { self . slide_name } , please check if patches were correctly extracted.\" ) assert self . coords is not None , ( \"No coordinates provided for the patches to visualize, please run the \" \"extract_patches method first or load the coordinates from a file.\" ) if crop_roi : if self . ROI is None or self . ROI_width is None or self . ROI_height is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) w , h = int ( np . ceil ( self . ROI_width / self . level_downsamples [ vis_level ])), int ( np . ceil ( self . ROI_height / self . level_downsamples [ vis_level ]) ) offset = np . array ( self . ROI [: 2 ]) else : w , h = self . level_dimensions [ vis_level ] offset = np . array ([ 0 , 0 ]) patch_size = self . coords_attrs [ \"patch_size\" ] patch_level = self . coords_attrs [ \"patch_level\" ] patch_size = int ( patch_size * self . level_downsamples [ patch_level ]) canvas = init_image ( w , h , mask = black_white , color_bakground = background_color ) downsample_vis = self . level_downsamples [ vis_level ] idxs = np . arange ( len ( self . coords )) if selected_idx is not None : idxs = idxs [ selected_idx ] patch_size = np . ceil ( patch_size / downsample_vis ) . astype ( int ) log . info ( f \"Stitching { len ( idxs ) } patches at level { vis_level } with patch size { patch_size } , \" f \"with colors { colors is not None } .\" ) for idx in idxs : coord = self . coords [ idx ] coord_downsampled = np . ceil ( np . abs ( coord - offset ) / downsample_vis ) . astype ( int ) patch_size_coord = ( min ( max ( w - coord_downsampled [ 0 ], 0 ), patch_size ), min ( max ( h - coord_downsampled [ 1 ], 0 ), patch_size ), ) if any ( val == 0 for val in patch_size_coord ): continue if black_white : patch = np . ones ( patch_size_coord , dtype = \"uint8\" ) colors = None else : patch = np . array ( self . slide . read_region ( tuple ( coord ), vis_level , patch_size_coord ) . convert ( \"RGB\" ) ) if colors is not None : assert len ( colors ) == len ( idxs ), ( \"The number of colors provided must match \" \"the number of selected coordinates.\" ) color = colors [ idx ] color_patch = ( np . ones (( patch_size_coord [ 1 ], patch_size_coord [ 0 ], 3 )) * color ) . astype ( \"uint8\" ) canvas [ coord_downsampled [ 1 ] : coord_downsampled [ 1 ] + patch_size_coord [ 1 ], coord_downsampled [ 0 ] : coord_downsampled [ 0 ] + patch_size_coord [ 0 ], :, ] = cv2 . addWeighted ( color_patch , alpha , patch , 1 - alpha , 0 , patch ) else : canvas [ coord_downsampled [ 1 ] : coord_downsampled [ 1 ] + patch_size_coord [ 1 ], coord_downsampled [ 0 ] : coord_downsampled [ 0 ] + patch_size_coord [ 0 ], :, ] = patch if draw_grid : cv2 . rectangle ( canvas , tuple ( np . maximum ([ 0 , 0 ], coord_downsampled - 1 )), tuple ( coord_downsampled + patch_size_coord ), ( 0 , 0 , 0 , 255 ), thickness = 2 , ) img = Image . fromarray ( canvas ) return img","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;stitch"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.visualize","text":"Visualize the slide with or without the contours of the tissue. Parameters: Name Type Description Default vis_level int The level at which the visualization should be performed. required crop_roi bool Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the set_roi method). False contours_color tuple [ int , int , int ] The color to use for the contours. (255, 0, 0) line_thickness int The thickness to use for the contours 500 max_size int | None The maximum size for the visualization for the width or height of the image. None number_contours bool Set to True to number the contours. False black_white bool Set to True to visualize a binary mask of the contoured tissue. False view_slide_only bool Set to True to visualize the slide only (without the contours). False Returns: Type Description Image A PIL image of the visualization. Source code in src/prismtoolbox/wsicore/wsi.py 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 def visualize ( self , vis_level : int , crop_roi : bool = False , contours_color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), line_thickness : int = 500 , max_size : int | None = None , number_contours : bool = False , black_white : bool = False , view_slide_only : bool = False , ) -> Image . Image : \"\"\"Visualize the slide with or without the contours of the tissue. Args: vis_level: The level at which the visualization should be performed. crop_roi: Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). contours_color: The color to use for the contours. line_thickness: The thickness to use for the contours max_size: The maximum size for the visualization for the width or height of the image. number_contours: Set to True to number the contours. black_white: Set to True to visualize a binary mask of the contoured tissue. view_slide_only: Set to True to visualize the slide only (without the contours). Returns: A PIL image of the visualization. \"\"\" assert line_thickness > 0 , \"line_thickness must be greater than 0\" scale = 1 / self . level_downsamples [ vis_level ] if black_white : img = np . zeros_like ( self . create_thumbnail ( vis_level , crop_roi ), dtype = \"uint8\" ) line_thickness = - 1 contours_color = ( 1 , 1 , 1 ) else : img = np . array ( self . create_thumbnail ( vis_level , crop_roi )) line_thickness = int ( line_thickness * scale ) if not view_slide_only : if self . tissue_contours is None : raise RuntimeError ( f \"No tissue contours found for the slide { self . slide_name } , please run the detect_tissue method first.\" ) if crop_roi : if self . ROI is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) offset = self . ROI [: 2 ] else : offset = np . array ([ 0 , 0 ]) contours = [ cont - offset for cont in self . tissue_contours ] contours = self . scale_contours ( contours , scale ) if len ( contours ) > 0 : if not number_contours : cv2 . drawContours ( img , contours , - 1 , contours_color , line_thickness , lineType = cv2 . LINE_8 , ) else : # add numbering to each contour for idx , cont in enumerate ( contours ): M = cv2 . moments ( cont ) cX = int ( M [ \"m10\" ] / ( M [ \"m00\" ] + 1e-9 )) cY = int ( M [ \"m01\" ] / ( M [ \"m00\" ] + 1e-9 )) # draw the contour and put text next to center cv2 . drawContours ( img , [ cont ], - 1 , contours_color , line_thickness , lineType = cv2 . LINE_8 , ) cv2 . putText ( img , \" {} \" . format ( idx ), ( cX , cY ), cv2 . FONT_HERSHEY_SIMPLEX , 2 , ( 255 , 0 , 0 ), 10 , ) img = Image . fromarray ( img ) if black_white : img = img . convert ( \"L\" ) w , h = img . size if max_size is not None and ( w > max_size or h > max_size ): resizeFactor = max_size / w if w > h else max_size / h img = img . resize (( int ( w * resizeFactor ), int ( h * resizeFactor ))) return img","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;visualize"},{"location":"reference/prismtoolbox/#prismtoolbox.WSI.worker_init","text":"Initialize the worker process with a wsi object. Parameters: Name Type Description Default slide_path str The path to the slide. required engine str The backend library to use for reading the slide (currently only openslide and tiffslide are supported) required Source code in src/prismtoolbox/wsicore/wsi.py 170 171 172 173 174 175 176 177 178 179 180 @staticmethod def worker_init ( slide_path : str , engine : str ) -> None : \"\"\"Initialize the worker process with a wsi object. Args: slide_path: The path to the slide. engine: The backend library to use for reading the slide (currently only openslide and tiffslide are supported) \"\"\" global wsi wsi = WSI . read ( slide_path , engine )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;worker_init"},{"location":"reference/prismtoolbox/nucleiseg/","text":"","title":"Index"},{"location":"reference/prismtoolbox/nucleiseg/seg_utils/","text":"solve_conflicts ( cells , threshold = 0.5 , return_indices = False ) Resolve segmentation conflicts (i.e. overlap) after running segmentation on patches Parameters: Name Type Description Default cells list [ Polygon ] List of cell polygons required threshold float When two cells are overlapping, we look at the area of intersection over the area of the smallest cell. If this value is higher than the threshold , the cells are merged 0.5 return_indices bool If True , returns also the cells indices. Merged cells have an index of -1. False Returns: Type Description MultiPolygon | tuple [ MultiPolygon , ndarray ] Array of resolved cells polygons. If return_indices , it also returns an array of cell indices. Source code in src/prismtoolbox/nucleiseg/seg_utils.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def solve_conflicts ( cells : list [ Polygon ], threshold : float = 0.5 , return_indices : bool = False , ) -> MultiPolygon | tuple [ MultiPolygon , np . ndarray ]: \"\"\"Resolve segmentation conflicts (i.e. overlap) after running segmentation on patches Args: cells: List of cell polygons threshold: When two cells are overlapping, we look at the area of intersection over the area of the smallest cell. If this value is higher than the `threshold`, the cells are merged return_indices: If `True`, returns also the cells indices. Merged cells have an index of -1. Returns: Array of resolved cells polygons. If `return_indices`, it also returns an array of cell indices. \"\"\" cells = list ( cells ) n_cells = len ( cells ) resolved_indices = np . arange ( n_cells ) assert n_cells > 0 , \"No cells was segmented, cannot continue\" tree = shapely . STRtree ( cells ) conflicts = tree . query ( cells , predicate = \"intersects\" ) conflicts = conflicts [:, conflicts [ 0 ] != conflicts [ 1 ]] . T conflicts = np . array ([ c for c in conflicts if c [ 0 ] < c [ 1 ]]) for i1 , i2 in tqdm ( conflicts , desc = \"Resolving conflicts\" ): resolved_i1 = resolved_indices [ i1 ] resolved_i2 = resolved_indices [ i2 ] cell1 , cell2 = cells [ resolved_i1 ], cells [ resolved_i2 ] intersection = cell1 . intersection ( cell2 ) . area if intersection >= threshold * min ( cell1 . area , cell2 . area ): cell = cell1 | cell2 resolved_indices [ np . isin ( resolved_indices , [ resolved_i1 , resolved_i2 ])] = len ( cells ) cells . append ( cell ) unique_indices = np . unique ( resolved_indices ) unique_cells = MultiPolygon ( list ( np . array ( cells )[ unique_indices ])) if return_indices : return unique_cells , np . where ( unique_indices < n_cells , unique_indices , - 1 ) return unique_cells","title":"Seg utils"},{"location":"reference/prismtoolbox/nucleiseg/seg_utils/#prismtoolbox.nucleiseg.seg_utils.solve_conflicts","text":"Resolve segmentation conflicts (i.e. overlap) after running segmentation on patches Parameters: Name Type Description Default cells list [ Polygon ] List of cell polygons required threshold float When two cells are overlapping, we look at the area of intersection over the area of the smallest cell. If this value is higher than the threshold , the cells are merged 0.5 return_indices bool If True , returns also the cells indices. Merged cells have an index of -1. False Returns: Type Description MultiPolygon | tuple [ MultiPolygon , ndarray ] Array of resolved cells polygons. If return_indices , it also returns an array of cell indices. Source code in src/prismtoolbox/nucleiseg/seg_utils.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def solve_conflicts ( cells : list [ Polygon ], threshold : float = 0.5 , return_indices : bool = False , ) -> MultiPolygon | tuple [ MultiPolygon , np . ndarray ]: \"\"\"Resolve segmentation conflicts (i.e. overlap) after running segmentation on patches Args: cells: List of cell polygons threshold: When two cells are overlapping, we look at the area of intersection over the area of the smallest cell. If this value is higher than the `threshold`, the cells are merged return_indices: If `True`, returns also the cells indices. Merged cells have an index of -1. Returns: Array of resolved cells polygons. If `return_indices`, it also returns an array of cell indices. \"\"\" cells = list ( cells ) n_cells = len ( cells ) resolved_indices = np . arange ( n_cells ) assert n_cells > 0 , \"No cells was segmented, cannot continue\" tree = shapely . STRtree ( cells ) conflicts = tree . query ( cells , predicate = \"intersects\" ) conflicts = conflicts [:, conflicts [ 0 ] != conflicts [ 1 ]] . T conflicts = np . array ([ c for c in conflicts if c [ 0 ] < c [ 1 ]]) for i1 , i2 in tqdm ( conflicts , desc = \"Resolving conflicts\" ): resolved_i1 = resolved_indices [ i1 ] resolved_i2 = resolved_indices [ i2 ] cell1 , cell2 = cells [ resolved_i1 ], cells [ resolved_i2 ] intersection = cell1 . intersection ( cell2 ) . area if intersection >= threshold * min ( cell1 . area , cell2 . area ): cell = cell1 | cell2 resolved_indices [ np . isin ( resolved_indices , [ resolved_i1 , resolved_i2 ])] = len ( cells ) cells . append ( cell ) unique_indices = np . unique ( resolved_indices ) unique_cells = MultiPolygon ( list ( np . array ( cells )[ unique_indices ])) if return_indices : return unique_cells , np . where ( unique_indices < n_cells , unique_indices , - 1 ) return unique_cells","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;solve_conflicts"},{"location":"reference/prismtoolbox/nucleiseg/segmenter/","text":"","title":"prismtoolbox.nucleiseg"},{"location":"reference/prismtoolbox/nucleiseg/models/","text":"","title":"Index"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/","text":"","title":"Index"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/","text":"ResnetBlock ( dim , padding_type , norm_layer , use_dropout , use_bias ) Bases: Module Define a Resnet block Initialize the Resnet block A resnet block is a conv block with skip connections We construct a conv block with build_conv_block function, and implement skip connections in function. Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 277 278 279 280 281 282 283 284 285 286 287 def __init__ ( self , dim , padding_type , norm_layer , use_dropout , use_bias ): \"\"\"Initialize the Resnet block A resnet block is a conv block with skip connections We construct a conv block with build_conv_block function, and implement skip connections in <forward> function. Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf \"\"\" super ( ResnetBlock , self ) . __init__ () self . conv_block = self . build_conv_block ( dim , padding_type , norm_layer , use_dropout , use_bias ) build_conv_block ( dim , padding_type , norm_layer , use_dropout , use_bias ) Construct a convolutional block. Parameters: dim (int) -- the number of channels in the conv layer. padding_type (str) -- the name of padding layer: reflect | replicate | zero norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers. use_bias (bool) -- if the conv layer uses bias or not Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU)) Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def build_conv_block ( self , dim , padding_type , norm_layer , use_dropout , use_bias ): \"\"\"Construct a convolutional block. Parameters: dim (int) -- the number of channels in the conv layer. padding_type (str) -- the name of padding layer: reflect | replicate | zero norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers. use_bias (bool) -- if the conv layer uses bias or not Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU)) \"\"\" conv_block = [] p = 0 if padding_type == \"reflect\" : conv_block += [ nn . ReflectionPad2d ( 1 )] elif padding_type == \"replicate\" : conv_block += [ nn . ReplicationPad2d ( 1 )] elif padding_type == \"zero\" : p = 1 else : raise NotImplementedError ( \"padding [ %s ] is not implemented\" % padding_type ) conv_block += [ nn . Conv2d ( dim , dim , kernel_size = 3 , padding = p , bias = use_bias ), norm_layer ( dim ), nn . ReLU ( True ), ] if use_dropout : conv_block += [ nn . Dropout ( 0.5 )] p = 0 if padding_type == \"reflect\" : conv_block += [ nn . ReflectionPad2d ( 1 )] elif padding_type == \"replicate\" : conv_block += [ nn . ReplicationPad2d ( 1 )] elif padding_type == \"zero\" : p = 1 else : raise NotImplementedError ( \"padding [ %s ] is not implemented\" % padding_type ) conv_block += [ nn . Conv2d ( dim , dim , kernel_size = 3 , padding = p , bias = use_bias ), norm_layer ( dim ), ] return nn . Sequential ( * conv_block ) forward ( x ) Forward function (with skip connections) Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 334 335 336 337 def forward ( self , x ): \"\"\"Forward function (with skip connections)\"\"\" out = x + self . conv_block ( x ) # add skip connections return out ResnetGenerator ( input_nc , output_nc , ngf = 64 , norm_layer = nn . BatchNorm2d , use_dropout = False , n_blocks = 6 , padding_type = 'reflect' ) Bases: Module Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations. We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style) Construct a Resnet-based generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images ngf (int) -- the number of filters in the last conv layer norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers n_blocks (int) -- the number of ResNet blocks padding_type (str) -- the name of padding layer in conv layers: reflect | replicate | zero Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def __init__ ( self , input_nc , output_nc , ngf = 64 , norm_layer = nn . BatchNorm2d , use_dropout = False , n_blocks = 6 , padding_type = \"reflect\" , ): \"\"\"Construct a Resnet-based generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images ngf (int) -- the number of filters in the last conv layer norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers n_blocks (int) -- the number of ResNet blocks padding_type (str) -- the name of padding layer in conv layers: reflect | replicate | zero \"\"\" assert n_blocks >= 0 super ( ResnetGenerator , self ) . __init__ () if type ( norm_layer ) == functools . partial : use_bias = norm_layer . func == nn . InstanceNorm2d else : use_bias = norm_layer == nn . InstanceNorm2d model = [ nn . ReflectionPad2d ( 3 ), nn . Conv2d ( input_nc , ngf , kernel_size = 7 , padding = 0 , bias = use_bias ), norm_layer ( ngf ), nn . ReLU ( True ), ] n_downsampling = 2 for i in range ( n_downsampling ): # add downsampling layers mult = 2 ** i model += [ nn . Conv2d ( ngf * mult , ngf * mult * 2 , kernel_size = 3 , stride = 2 , padding = 1 , bias = use_bias , ), norm_layer ( ngf * mult * 2 ), nn . ReLU ( True ), ] mult = 2 ** n_downsampling for i in range ( n_blocks ): # add ResNet blocks model += [ ResnetBlock ( ngf * mult , padding_type = padding_type , norm_layer = norm_layer , use_dropout = use_dropout , use_bias = use_bias , ) ] for i in range ( n_downsampling ): # add upsampling layers mult = 2 ** ( n_downsampling - i ) model += [ nn . ConvTranspose2d ( ngf * mult , int ( ngf * mult / 2 ), kernel_size = 3 , stride = 2 , padding = 1 , output_padding = 1 , bias = use_bias , ), norm_layer ( int ( ngf * mult / 2 )), nn . ReLU ( True ), ] model += [ nn . ReflectionPad2d ( 3 )] model += [ nn . Conv2d ( ngf , output_nc , kernel_size = 7 , padding = 0 )] # Model += [nn.Tanh()] self . model = nn . Sequential ( * model ) forward ( input ) Standard forward Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 269 270 271 def forward ( self , input ): \"\"\"Standard forward\"\"\" return self . model ( input ) UnetGenerator ( input_nc , output_nc , num_downs , ngf = 64 , norm_layer = nn . BatchNorm2d , use_dropout = False , dropout_value = 0.5 , bias_last_conv = True ) Bases: Module Create a Unet-based generator Construct a Unet generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7, image of size 128x128 will become of size 1x1 # at the bottleneck ngf (int) -- the number of filters in the last conv layer norm_layer -- normalization layer We construct the U-Net from the innermost layer to the outermost layer. It is a recursive process. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 def __init__ ( self , input_nc , output_nc , num_downs , ngf = 64 , norm_layer = nn . BatchNorm2d , use_dropout = False , dropout_value = 0.5 , bias_last_conv = True , ): \"\"\"Construct a Unet generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7, image of size 128x128 will become of size 1x1 # at the bottleneck ngf (int) -- the number of filters in the last conv layer norm_layer -- normalization layer We construct the U-Net from the innermost layer to the outermost layer. It is a recursive process. \"\"\" super ( UnetGenerator , self ) . __init__ () # construct unet structure unet_block = UnetSkipConnectionBlock ( ngf * 8 , ngf * 8 , input_nc = None , submodule = None , norm_layer = norm_layer , innermost = True , ) # add the innermost layer for i in range ( num_downs - 5 ): # add intermediate layers with ngf * 8 filters unet_block = UnetSkipConnectionBlock ( ngf * 8 , ngf * 8 , input_nc = None , submodule = unet_block , norm_layer = norm_layer , use_dropout = use_dropout , dropout_value = dropout_value , ) # gradually reduce the number of filters from ngf * 8 to ngf unet_block = UnetSkipConnectionBlock ( ngf * 4 , ngf * 8 , input_nc = None , submodule = unet_block , norm_layer = norm_layer ) unet_block = UnetSkipConnectionBlock ( ngf * 2 , ngf * 4 , input_nc = None , submodule = unet_block , norm_layer = norm_layer ) unet_block = UnetSkipConnectionBlock ( ngf , ngf * 2 , input_nc = None , submodule = unet_block , norm_layer = norm_layer ) self . model = UnetSkipConnectionBlock ( output_nc , ngf , input_nc = input_nc , submodule = unet_block , outermost = True , norm_layer = norm_layer , bias_last_conv = bias_last_conv , ) # add the outermost layer forward ( input ) Standard forward Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 405 406 407 def forward ( self , input ): \"\"\"Standard forward\"\"\" return self . model ( input ) UnetSkipConnectionBlock ( outer_nc , inner_nc , input_nc = None , submodule = None , outermost = False , innermost = False , norm_layer = nn . BatchNorm2d , use_dropout = False , dropout_value = 0.5 , bias_last_conv = True ) Bases: Module Defines the Unet submodule with skip connection. X -------------------identity---------------------- |-- downsampling -- |submodule| -- upsampling --| Construct a Unet submodule with skip connections. Parameters: outer_nc (int) -- the number of filters in the outer conv layer inner_nc (int) -- the number of filters in the inner conv layer input_nc (int) -- the number of channels in input images/features submodule (UnetSkipConnectionBlock) -- previously defined submodules outermost (bool) -- if this module is the outermost module innermost (bool) -- if this module is the innermost module norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def __init__ ( self , outer_nc , inner_nc , input_nc = None , submodule = None , outermost = False , innermost = False , norm_layer = nn . BatchNorm2d , use_dropout = False , dropout_value = 0.5 , bias_last_conv = True , ): \"\"\"Construct a Unet submodule with skip connections. Parameters: outer_nc (int) -- the number of filters in the outer conv layer inner_nc (int) -- the number of filters in the inner conv layer input_nc (int) -- the number of channels in input images/features submodule (UnetSkipConnectionBlock) -- previously defined submodules outermost (bool) -- if this module is the outermost module innermost (bool) -- if this module is the innermost module norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers. \"\"\" super ( UnetSkipConnectionBlock , self ) . __init__ () self . outermost = outermost if type ( norm_layer ) == functools . partial : use_bias = norm_layer . func == nn . InstanceNorm2d else : use_bias = norm_layer == nn . InstanceNorm2d if input_nc is None : input_nc = outer_nc downconv = nn . Conv2d ( input_nc , inner_nc , kernel_size = 4 , stride = 2 , padding = 1 , bias = use_bias ) downrelu = nn . LeakyReLU ( 0.2 , True ) downnorm = norm_layer ( inner_nc ) uprelu = nn . ReLU ( True ) upnorm = norm_layer ( outer_nc ) if outermost : upconv = nn . ConvTranspose2d ( inner_nc * 2 , outer_nc , kernel_size = 4 , stride = 2 , padding = 1 , bias = bias_last_conv , ) down = [ downconv ] up = [ uprelu , upconv ] model = down + [ submodule ] + up elif innermost : upconv = nn . ConvTranspose2d ( inner_nc , outer_nc , kernel_size = 4 , stride = 2 , padding = 1 , bias = use_bias ) down = [ downrelu , downconv ] up = [ uprelu , upconv , upnorm ] model = down + up else : upconv = nn . ConvTranspose2d ( inner_nc * 2 , outer_nc , kernel_size = 4 , stride = 2 , padding = 1 , bias = use_bias ) down = [ downrelu , downconv , downnorm ] up = [ uprelu , upconv , upnorm ] if use_dropout : model = down + [ submodule ] + up + [ nn . Dropout ( dropout_value )] else : model = down + [ submodule ] + up self . model = nn . Sequential ( * model ) define_G ( input_nc , output_nc , ngf , netG , norm = 'batch' , use_dropout = False , dropout_value = 0.5 , init_type = 'normal' , init_gain = 0.02 , gpu_ids = [], bias_last_conv = True ) Create a generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images ngf (int) -- the number of filters in the last conv layer netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128 norm (str) -- the name of normalization layers used in the network: batch | instance | none use_dropout (bool) -- if use dropout layers. init_type (str) -- the name of our initialization method. init_gain (float) -- scaling factor for normal, xavier and orthogonal. gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2 Returns a generator Our current implementation provides two types of generators: U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images) The original U-Net paper: https://arxiv.org/abs/1505.04597 Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks) Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations. We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style). The generator has been initialized by . It uses RELU for non-linearity. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def define_G ( input_nc , output_nc , ngf , netG , norm = \"batch\" , use_dropout = False , dropout_value = 0.5 , init_type = \"normal\" , init_gain = 0.02 , gpu_ids = [], bias_last_conv = True , ): \"\"\"Create a generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images ngf (int) -- the number of filters in the last conv layer netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128 norm (str) -- the name of normalization layers used in the network: batch | instance | none use_dropout (bool) -- if use dropout layers. init_type (str) -- the name of our initialization method. init_gain (float) -- scaling factor for normal, xavier and orthogonal. gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2 Returns a generator Our current implementation provides two types of generators: U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images) The original U-Net paper: https://arxiv.org/abs/1505.04597 Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks) Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations. We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style). The generator has been initialized by <init_net>. It uses RELU for non-linearity. \"\"\" net = None norm_layer = get_norm_layer ( norm_type = norm ) if netG == \"resnet_9blocks\" : net = ResnetGenerator ( input_nc , output_nc , ngf , norm_layer = norm_layer , use_dropout = use_dropout , n_blocks = 9 , ) elif netG == \"resnet_6blocks\" : net = ResnetGenerator ( input_nc , output_nc , ngf , norm_layer = norm_layer , use_dropout = use_dropout , n_blocks = 6 , ) elif netG == \"unet_128\" : net = UnetGenerator ( input_nc , output_nc , 7 , ngf , norm_layer = norm_layer , use_dropout = use_dropout , dropout_value = dropout_value , bias_last_conv = bias_last_conv , ) elif netG == \"unet_256\" : net = UnetGenerator ( input_nc , output_nc , 8 , ngf , norm_layer = norm_layer , use_dropout = use_dropout , dropout_value = dropout_value , bias_last_conv = bias_last_conv , ) else : raise NotImplementedError ( \"Generator model name [ %s ] is not recognized\" % netG ) return init_net ( net , init_type , init_gain , gpu_ids ) get_norm_layer ( norm_type = 'instance' ) Return a normalization layer Parameters: norm_type (str) -- the name of the normalization layer: batch | instance | none For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev). For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def get_norm_layer ( norm_type = \"instance\" ): \"\"\"Return a normalization layer Parameters: norm_type (str) -- the name of the normalization layer: batch | instance | none For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev). For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics. \"\"\" if norm_type == \"batch\" : norm_layer = functools . partial ( nn . BatchNorm2d , affine = True , track_running_stats = True ) elif norm_type == \"instance\" : norm_layer = functools . partial ( nn . InstanceNorm2d , affine = False , track_running_stats = False ) elif norm_type == \"none\" : def norm_layer ( x ): return Identity () else : raise NotImplementedError ( \"normalization layer [ %s ] is not found\" % norm_type ) return norm_layer init_net ( net , init_type = 'normal' , init_gain = 0.02 , gpu_ids = []) Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights Parameters: net (network) -- the network to be initialized init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal gain (float) -- scaling factor for normal, xavier and orthogonal. gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2 Return an initialized network. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def init_net ( net , init_type = \"normal\" , init_gain = 0.02 , gpu_ids = []): \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights Parameters: net (network) -- the network to be initialized init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal gain (float) -- scaling factor for normal, xavier and orthogonal. gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2 Return an initialized network. \"\"\" if len ( gpu_ids ) > 0 : assert torch . cuda . is_available () net . to ( gpu_ids [ 0 ]) net = torch . nn . DataParallel ( net , gpu_ids ) # multi-GPUs init_weights ( net , init_type , init_gain = init_gain ) return net init_weights ( net , init_type = 'normal' , init_gain = 0.02 ) Initialize network weights. Parameters: net (network) -- network to be initialized init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal init_gain (float) -- scaling factor for normal, xavier and orthogonal. We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might work better for some applications. Feel free to try yourself. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def init_weights ( net , init_type = \"normal\" , init_gain = 0.02 ): \"\"\"Initialize network weights. Parameters: net (network) -- network to be initialized init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal init_gain (float) -- scaling factor for normal, xavier and orthogonal. We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might work better for some applications. Feel free to try yourself. \"\"\" def init_func ( m ): # define the initialization function classname = m . __class__ . __name__ if hasattr ( m , \"weight\" ) and ( classname . find ( \"Conv\" ) != - 1 or classname . find ( \"Linear\" ) != - 1 ): if init_type == \"normal\" : init . normal_ ( m . weight . data , 0.0 , init_gain ) elif init_type == \"xavier\" : init . xavier_normal_ ( m . weight . data , gain = init_gain ) elif init_type == \"kaiming\" : init . kaiming_normal_ ( m . weight . data , a = 0 , mode = \"fan_in\" ) elif init_type == \"orthogonal\" : init . orthogonal_ ( m . weight . data , gain = init_gain ) else : raise NotImplementedError ( \"initialization method [ %s ] is not implemented\" % init_type ) if hasattr ( m , \"bias\" ) and m . bias is not None : init . constant_ ( m . bias . data , 0.0 ) elif ( classname . find ( \"BatchNorm2d\" ) != - 1 ): # BatchNorm Layer's weight is not a matrix; only normal distribution applies. init . normal_ ( m . weight . data , 1.0 , init_gain ) init . constant_ ( m . bias . data , 0.0 ) print ( \"initialize network with %s \" % init_type ) net . apply ( init_func ) # apply the initialization function <init_func>","title":"Architectures"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.ResnetBlock","text":"Bases: Module Define a Resnet block Initialize the Resnet block A resnet block is a conv block with skip connections We construct a conv block with build_conv_block function, and implement skip connections in function. Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 277 278 279 280 281 282 283 284 285 286 287 def __init__ ( self , dim , padding_type , norm_layer , use_dropout , use_bias ): \"\"\"Initialize the Resnet block A resnet block is a conv block with skip connections We construct a conv block with build_conv_block function, and implement skip connections in <forward> function. Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf \"\"\" super ( ResnetBlock , self ) . __init__ () self . conv_block = self . build_conv_block ( dim , padding_type , norm_layer , use_dropout , use_bias )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;ResnetBlock"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.ResnetBlock.build_conv_block","text":"Construct a convolutional block. Parameters: dim (int) -- the number of channels in the conv layer. padding_type (str) -- the name of padding layer: reflect | replicate | zero norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers. use_bias (bool) -- if the conv layer uses bias or not Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU)) Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def build_conv_block ( self , dim , padding_type , norm_layer , use_dropout , use_bias ): \"\"\"Construct a convolutional block. Parameters: dim (int) -- the number of channels in the conv layer. padding_type (str) -- the name of padding layer: reflect | replicate | zero norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers. use_bias (bool) -- if the conv layer uses bias or not Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU)) \"\"\" conv_block = [] p = 0 if padding_type == \"reflect\" : conv_block += [ nn . ReflectionPad2d ( 1 )] elif padding_type == \"replicate\" : conv_block += [ nn . ReplicationPad2d ( 1 )] elif padding_type == \"zero\" : p = 1 else : raise NotImplementedError ( \"padding [ %s ] is not implemented\" % padding_type ) conv_block += [ nn . Conv2d ( dim , dim , kernel_size = 3 , padding = p , bias = use_bias ), norm_layer ( dim ), nn . ReLU ( True ), ] if use_dropout : conv_block += [ nn . Dropout ( 0.5 )] p = 0 if padding_type == \"reflect\" : conv_block += [ nn . ReflectionPad2d ( 1 )] elif padding_type == \"replicate\" : conv_block += [ nn . ReplicationPad2d ( 1 )] elif padding_type == \"zero\" : p = 1 else : raise NotImplementedError ( \"padding [ %s ] is not implemented\" % padding_type ) conv_block += [ nn . Conv2d ( dim , dim , kernel_size = 3 , padding = p , bias = use_bias ), norm_layer ( dim ), ] return nn . Sequential ( * conv_block )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;build_conv_block"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.ResnetBlock.forward","text":"Forward function (with skip connections) Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 334 335 336 337 def forward ( self , x ): \"\"\"Forward function (with skip connections)\"\"\" out = x + self . conv_block ( x ) # add skip connections return out","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;forward"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.ResnetGenerator","text":"Bases: Module Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations. We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style) Construct a Resnet-based generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images ngf (int) -- the number of filters in the last conv layer norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers n_blocks (int) -- the number of ResNet blocks padding_type (str) -- the name of padding layer in conv layers: reflect | replicate | zero Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def __init__ ( self , input_nc , output_nc , ngf = 64 , norm_layer = nn . BatchNorm2d , use_dropout = False , n_blocks = 6 , padding_type = \"reflect\" , ): \"\"\"Construct a Resnet-based generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images ngf (int) -- the number of filters in the last conv layer norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers n_blocks (int) -- the number of ResNet blocks padding_type (str) -- the name of padding layer in conv layers: reflect | replicate | zero \"\"\" assert n_blocks >= 0 super ( ResnetGenerator , self ) . __init__ () if type ( norm_layer ) == functools . partial : use_bias = norm_layer . func == nn . InstanceNorm2d else : use_bias = norm_layer == nn . InstanceNorm2d model = [ nn . ReflectionPad2d ( 3 ), nn . Conv2d ( input_nc , ngf , kernel_size = 7 , padding = 0 , bias = use_bias ), norm_layer ( ngf ), nn . ReLU ( True ), ] n_downsampling = 2 for i in range ( n_downsampling ): # add downsampling layers mult = 2 ** i model += [ nn . Conv2d ( ngf * mult , ngf * mult * 2 , kernel_size = 3 , stride = 2 , padding = 1 , bias = use_bias , ), norm_layer ( ngf * mult * 2 ), nn . ReLU ( True ), ] mult = 2 ** n_downsampling for i in range ( n_blocks ): # add ResNet blocks model += [ ResnetBlock ( ngf * mult , padding_type = padding_type , norm_layer = norm_layer , use_dropout = use_dropout , use_bias = use_bias , ) ] for i in range ( n_downsampling ): # add upsampling layers mult = 2 ** ( n_downsampling - i ) model += [ nn . ConvTranspose2d ( ngf * mult , int ( ngf * mult / 2 ), kernel_size = 3 , stride = 2 , padding = 1 , output_padding = 1 , bias = use_bias , ), norm_layer ( int ( ngf * mult / 2 )), nn . ReLU ( True ), ] model += [ nn . ReflectionPad2d ( 3 )] model += [ nn . Conv2d ( ngf , output_nc , kernel_size = 7 , padding = 0 )] # Model += [nn.Tanh()] self . model = nn . Sequential ( * model )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;ResnetGenerator"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.ResnetGenerator.forward","text":"Standard forward Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 269 270 271 def forward ( self , input ): \"\"\"Standard forward\"\"\" return self . model ( input )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;forward"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.UnetGenerator","text":"Bases: Module Create a Unet-based generator Construct a Unet generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7, image of size 128x128 will become of size 1x1 # at the bottleneck ngf (int) -- the number of filters in the last conv layer norm_layer -- normalization layer We construct the U-Net from the innermost layer to the outermost layer. It is a recursive process. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 def __init__ ( self , input_nc , output_nc , num_downs , ngf = 64 , norm_layer = nn . BatchNorm2d , use_dropout = False , dropout_value = 0.5 , bias_last_conv = True , ): \"\"\"Construct a Unet generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7, image of size 128x128 will become of size 1x1 # at the bottleneck ngf (int) -- the number of filters in the last conv layer norm_layer -- normalization layer We construct the U-Net from the innermost layer to the outermost layer. It is a recursive process. \"\"\" super ( UnetGenerator , self ) . __init__ () # construct unet structure unet_block = UnetSkipConnectionBlock ( ngf * 8 , ngf * 8 , input_nc = None , submodule = None , norm_layer = norm_layer , innermost = True , ) # add the innermost layer for i in range ( num_downs - 5 ): # add intermediate layers with ngf * 8 filters unet_block = UnetSkipConnectionBlock ( ngf * 8 , ngf * 8 , input_nc = None , submodule = unet_block , norm_layer = norm_layer , use_dropout = use_dropout , dropout_value = dropout_value , ) # gradually reduce the number of filters from ngf * 8 to ngf unet_block = UnetSkipConnectionBlock ( ngf * 4 , ngf * 8 , input_nc = None , submodule = unet_block , norm_layer = norm_layer ) unet_block = UnetSkipConnectionBlock ( ngf * 2 , ngf * 4 , input_nc = None , submodule = unet_block , norm_layer = norm_layer ) unet_block = UnetSkipConnectionBlock ( ngf , ngf * 2 , input_nc = None , submodule = unet_block , norm_layer = norm_layer ) self . model = UnetSkipConnectionBlock ( output_nc , ngf , input_nc = input_nc , submodule = unet_block , outermost = True , norm_layer = norm_layer , bias_last_conv = bias_last_conv , ) # add the outermost layer","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;UnetGenerator"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.UnetGenerator.forward","text":"Standard forward Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 405 406 407 def forward ( self , input ): \"\"\"Standard forward\"\"\" return self . model ( input )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;forward"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.UnetSkipConnectionBlock","text":"Bases: Module Defines the Unet submodule with skip connection. X -------------------identity---------------------- |-- downsampling -- |submodule| -- upsampling --| Construct a Unet submodule with skip connections. Parameters: outer_nc (int) -- the number of filters in the outer conv layer inner_nc (int) -- the number of filters in the inner conv layer input_nc (int) -- the number of channels in input images/features submodule (UnetSkipConnectionBlock) -- previously defined submodules outermost (bool) -- if this module is the outermost module innermost (bool) -- if this module is the innermost module norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def __init__ ( self , outer_nc , inner_nc , input_nc = None , submodule = None , outermost = False , innermost = False , norm_layer = nn . BatchNorm2d , use_dropout = False , dropout_value = 0.5 , bias_last_conv = True , ): \"\"\"Construct a Unet submodule with skip connections. Parameters: outer_nc (int) -- the number of filters in the outer conv layer inner_nc (int) -- the number of filters in the inner conv layer input_nc (int) -- the number of channels in input images/features submodule (UnetSkipConnectionBlock) -- previously defined submodules outermost (bool) -- if this module is the outermost module innermost (bool) -- if this module is the innermost module norm_layer -- normalization layer use_dropout (bool) -- if use dropout layers. \"\"\" super ( UnetSkipConnectionBlock , self ) . __init__ () self . outermost = outermost if type ( norm_layer ) == functools . partial : use_bias = norm_layer . func == nn . InstanceNorm2d else : use_bias = norm_layer == nn . InstanceNorm2d if input_nc is None : input_nc = outer_nc downconv = nn . Conv2d ( input_nc , inner_nc , kernel_size = 4 , stride = 2 , padding = 1 , bias = use_bias ) downrelu = nn . LeakyReLU ( 0.2 , True ) downnorm = norm_layer ( inner_nc ) uprelu = nn . ReLU ( True ) upnorm = norm_layer ( outer_nc ) if outermost : upconv = nn . ConvTranspose2d ( inner_nc * 2 , outer_nc , kernel_size = 4 , stride = 2 , padding = 1 , bias = bias_last_conv , ) down = [ downconv ] up = [ uprelu , upconv ] model = down + [ submodule ] + up elif innermost : upconv = nn . ConvTranspose2d ( inner_nc , outer_nc , kernel_size = 4 , stride = 2 , padding = 1 , bias = use_bias ) down = [ downrelu , downconv ] up = [ uprelu , upconv , upnorm ] model = down + up else : upconv = nn . ConvTranspose2d ( inner_nc * 2 , outer_nc , kernel_size = 4 , stride = 2 , padding = 1 , bias = use_bias ) down = [ downrelu , downconv , downnorm ] up = [ uprelu , upconv , upnorm ] if use_dropout : model = down + [ submodule ] + up + [ nn . Dropout ( dropout_value )] else : model = down + [ submodule ] + up self . model = nn . Sequential ( * model )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;UnetSkipConnectionBlock"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.define_G","text":"Create a generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images ngf (int) -- the number of filters in the last conv layer netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128 norm (str) -- the name of normalization layers used in the network: batch | instance | none use_dropout (bool) -- if use dropout layers. init_type (str) -- the name of our initialization method. init_gain (float) -- scaling factor for normal, xavier and orthogonal. gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2 Returns a generator Our current implementation provides two types of generators: U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images) The original U-Net paper: https://arxiv.org/abs/1505.04597 Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks) Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations. We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style). The generator has been initialized by . It uses RELU for non-linearity. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def define_G ( input_nc , output_nc , ngf , netG , norm = \"batch\" , use_dropout = False , dropout_value = 0.5 , init_type = \"normal\" , init_gain = 0.02 , gpu_ids = [], bias_last_conv = True , ): \"\"\"Create a generator Parameters: input_nc (int) -- the number of channels in input images output_nc (int) -- the number of channels in output images ngf (int) -- the number of filters in the last conv layer netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128 norm (str) -- the name of normalization layers used in the network: batch | instance | none use_dropout (bool) -- if use dropout layers. init_type (str) -- the name of our initialization method. init_gain (float) -- scaling factor for normal, xavier and orthogonal. gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2 Returns a generator Our current implementation provides two types of generators: U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images) The original U-Net paper: https://arxiv.org/abs/1505.04597 Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks) Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations. We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style). The generator has been initialized by <init_net>. It uses RELU for non-linearity. \"\"\" net = None norm_layer = get_norm_layer ( norm_type = norm ) if netG == \"resnet_9blocks\" : net = ResnetGenerator ( input_nc , output_nc , ngf , norm_layer = norm_layer , use_dropout = use_dropout , n_blocks = 9 , ) elif netG == \"resnet_6blocks\" : net = ResnetGenerator ( input_nc , output_nc , ngf , norm_layer = norm_layer , use_dropout = use_dropout , n_blocks = 6 , ) elif netG == \"unet_128\" : net = UnetGenerator ( input_nc , output_nc , 7 , ngf , norm_layer = norm_layer , use_dropout = use_dropout , dropout_value = dropout_value , bias_last_conv = bias_last_conv , ) elif netG == \"unet_256\" : net = UnetGenerator ( input_nc , output_nc , 8 , ngf , norm_layer = norm_layer , use_dropout = use_dropout , dropout_value = dropout_value , bias_last_conv = bias_last_conv , ) else : raise NotImplementedError ( \"Generator model name [ %s ] is not recognized\" % netG ) return init_net ( net , init_type , init_gain , gpu_ids )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;define_G"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.get_norm_layer","text":"Return a normalization layer Parameters: norm_type (str) -- the name of the normalization layer: batch | instance | none For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev). For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def get_norm_layer ( norm_type = \"instance\" ): \"\"\"Return a normalization layer Parameters: norm_type (str) -- the name of the normalization layer: batch | instance | none For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev). For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics. \"\"\" if norm_type == \"batch\" : norm_layer = functools . partial ( nn . BatchNorm2d , affine = True , track_running_stats = True ) elif norm_type == \"instance\" : norm_layer = functools . partial ( nn . InstanceNorm2d , affine = False , track_running_stats = False ) elif norm_type == \"none\" : def norm_layer ( x ): return Identity () else : raise NotImplementedError ( \"normalization layer [ %s ] is not found\" % norm_type ) return norm_layer","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;get_norm_layer"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.init_net","text":"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights Parameters: net (network) -- the network to be initialized init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal gain (float) -- scaling factor for normal, xavier and orthogonal. gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2 Return an initialized network. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def init_net ( net , init_type = \"normal\" , init_gain = 0.02 , gpu_ids = []): \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights Parameters: net (network) -- the network to be initialized init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal gain (float) -- scaling factor for normal, xavier and orthogonal. gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2 Return an initialized network. \"\"\" if len ( gpu_ids ) > 0 : assert torch . cuda . is_available () net . to ( gpu_ids [ 0 ]) net = torch . nn . DataParallel ( net , gpu_ids ) # multi-GPUs init_weights ( net , init_type , init_gain = init_gain ) return net","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;init_net"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/architectures/#prismtoolbox.nucleiseg.models.sop.architectures.init_weights","text":"Initialize network weights. Parameters: net (network) -- network to be initialized init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal init_gain (float) -- scaling factor for normal, xavier and orthogonal. We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might work better for some applications. Feel free to try yourself. Source code in src/prismtoolbox/nucleiseg/models/sop/architectures.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def init_weights ( net , init_type = \"normal\" , init_gain = 0.02 ): \"\"\"Initialize network weights. Parameters: net (network) -- network to be initialized init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal init_gain (float) -- scaling factor for normal, xavier and orthogonal. We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might work better for some applications. Feel free to try yourself. \"\"\" def init_func ( m ): # define the initialization function classname = m . __class__ . __name__ if hasattr ( m , \"weight\" ) and ( classname . find ( \"Conv\" ) != - 1 or classname . find ( \"Linear\" ) != - 1 ): if init_type == \"normal\" : init . normal_ ( m . weight . data , 0.0 , init_gain ) elif init_type == \"xavier\" : init . xavier_normal_ ( m . weight . data , gain = init_gain ) elif init_type == \"kaiming\" : init . kaiming_normal_ ( m . weight . data , a = 0 , mode = \"fan_in\" ) elif init_type == \"orthogonal\" : init . orthogonal_ ( m . weight . data , gain = init_gain ) else : raise NotImplementedError ( \"initialization method [ %s ] is not implemented\" % init_type ) if hasattr ( m , \"bias\" ) and m . bias is not None : init . constant_ ( m . bias . data , 0.0 ) elif ( classname . find ( \"BatchNorm2d\" ) != - 1 ): # BatchNorm Layer's weight is not a matrix; only normal distribution applies. init . normal_ ( m . weight . data , 1.0 , init_gain ) init . constant_ ( m . bias . data , 0.0 ) print ( \"initialize network with %s \" % init_type ) net . apply ( init_func ) # apply the initialization function <init_func>","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;init_weights"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/modules/","text":"","title":"Modules"},{"location":"reference/prismtoolbox/nucleiseg/models/sop/postprocessing/","text":"","title":"Postprocessing"},{"location":"reference/prismtoolbox/utils/","text":"","title":"Index"},{"location":"reference/prismtoolbox/utils/data_utils/","text":"load_obj_with_json ( file_path ) Load an object from a file using json. Parameters: Name Type Description Default file_path str The path to the json file. required Returns: Type Description Any A json object from the file. Source code in src/prismtoolbox/utils/data_utils.py 46 47 48 49 50 51 52 53 54 55 56 def load_obj_with_json ( file_path : str ) -> Any : \"\"\"Load an object from a file using json. Args: file_path: The path to the json file. Returns: A json object from the file. \"\"\" with open ( file_path , \"r\" ) as f : return json . load ( f ) load_obj_with_pickle ( file_path ) Load an object from a file using pickle. Parameters: Name Type Description Default file_path str The path to the pickle file. required Returns: Type Description Any A pickeable object from the file. Source code in src/prismtoolbox/utils/data_utils.py 33 34 35 36 37 38 39 40 41 42 43 def load_obj_with_pickle ( file_path : str ) -> Any : \"\"\"Load an object from a file using pickle. Args: file_path: The path to the pickle file. Returns: A pickeable object from the file. \"\"\" with open ( file_path , \"rb\" ) as f : return pickle . load ( f ) read_h5_file ( file_path , key ) Read an object from a h5 file. Parameters: Name Type Description Default file_path str The path to the h5 file. required key str The key to select the dataset in the h5 file. required Returns: Type Description tuple [ ndarray , dict ] A dataset from the h5 file. Source code in src/prismtoolbox/utils/data_utils.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def read_h5_file ( file_path : str , key : str ) -> tuple [ np . ndarray , dict ]: \"\"\"Read an object from a h5 file. Args: file_path: The path to the h5 file. key: The key to select the dataset in the h5 file. Returns: A dataset from the h5 file. \"\"\" with h5py . File ( file_path , \"r\" ) as f : if key not in f : raise KeyError ( f \"Key ' { key } ' not found in the h5 file.\" ) dataset = f [ key ] if isinstance ( dataset , h5py . Dataset ): obj = dataset [()] attrs = { k : v for k , v in dataset . attrs . items ()} else : raise TypeError ( f \"Key ' { key } ' does not refer to a dataset in the h5 file.\" ) return obj , attrs read_json_with_geopandas ( file_path , offset = ( 0 , 0 )) Read a json file with geopandas. Parameters: Name Type Description Default file_path str The path to a json file. required offset tuple [ int , int ] The offset to apply to the coordinates of the geometries. (0, 0) Returns: Type Description GeoDataFrame A GeoDataFrame object from the json file. Source code in src/prismtoolbox/utils/data_utils.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def read_json_with_geopandas ( file_path : str , offset : tuple [ int , int ] = ( 0 , 0 ) ) -> gpd . GeoDataFrame : \"\"\"Read a json file with geopandas. Args: file_path: The path to a json file. offset: The offset to apply to the coordinates of the geometries. Returns: A GeoDataFrame object from the json file. \"\"\" data = load_obj_with_json ( file_path ) df = gpd . GeoDataFrame . from_features ( data ) df . translate ( xoff = offset [ 0 ], yoff = offset [ 1 ]) if not df . is_valid . any (): df . loc [ ~ df . is_valid , :] = df . loc [ ~ df . is_valid , :] . buffer ( 0 ) if \"classification\" in df . columns : df [ \"classification\" ] = df [ \"classification\" ] . apply ( lambda x : x [ \"name\" ] if type ( x ) == dict else x ) return df save_obj_with_json ( obj , file_path ) Save an object to a file using json. Parameters: Name Type Description Default obj object A json object. required file_path str The path to the file. required Source code in src/prismtoolbox/utils/data_utils.py 22 23 24 25 26 27 28 29 30 def save_obj_with_json ( obj : object , file_path : str ) -> None : \"\"\"Save an object to a file using json. Args: obj: A json object. file_path: The path to the file. \"\"\" with open ( file_path , \"w\" ) as f : json . dump ( obj , f ) save_obj_with_pickle ( obj , file_path ) Save an object to a file using pickle. Parameters: Name Type Description Default obj object A pickeable object. required file_path str The path to the file. required Source code in src/prismtoolbox/utils/data_utils.py 11 12 13 14 15 16 17 18 19 def save_obj_with_pickle ( obj : object , file_path : str ) -> None : \"\"\"Save an object to a file using pickle. Args: obj: A pickeable object. file_path: The path to the file. \"\"\" with open ( file_path , \"wb\" ) as f : pickle . dump ( obj , f )","title":"Data utils"},{"location":"reference/prismtoolbox/utils/data_utils/#prismtoolbox.utils.data_utils.load_obj_with_json","text":"Load an object from a file using json. Parameters: Name Type Description Default file_path str The path to the json file. required Returns: Type Description Any A json object from the file. Source code in src/prismtoolbox/utils/data_utils.py 46 47 48 49 50 51 52 53 54 55 56 def load_obj_with_json ( file_path : str ) -> Any : \"\"\"Load an object from a file using json. Args: file_path: The path to the json file. Returns: A json object from the file. \"\"\" with open ( file_path , \"r\" ) as f : return json . load ( f )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;load_obj_with_json"},{"location":"reference/prismtoolbox/utils/data_utils/#prismtoolbox.utils.data_utils.load_obj_with_pickle","text":"Load an object from a file using pickle. Parameters: Name Type Description Default file_path str The path to the pickle file. required Returns: Type Description Any A pickeable object from the file. Source code in src/prismtoolbox/utils/data_utils.py 33 34 35 36 37 38 39 40 41 42 43 def load_obj_with_pickle ( file_path : str ) -> Any : \"\"\"Load an object from a file using pickle. Args: file_path: The path to the pickle file. Returns: A pickeable object from the file. \"\"\" with open ( file_path , \"rb\" ) as f : return pickle . load ( f )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;load_obj_with_pickle"},{"location":"reference/prismtoolbox/utils/data_utils/#prismtoolbox.utils.data_utils.read_h5_file","text":"Read an object from a h5 file. Parameters: Name Type Description Default file_path str The path to the h5 file. required key str The key to select the dataset in the h5 file. required Returns: Type Description tuple [ ndarray , dict ] A dataset from the h5 file. Source code in src/prismtoolbox/utils/data_utils.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def read_h5_file ( file_path : str , key : str ) -> tuple [ np . ndarray , dict ]: \"\"\"Read an object from a h5 file. Args: file_path: The path to the h5 file. key: The key to select the dataset in the h5 file. Returns: A dataset from the h5 file. \"\"\" with h5py . File ( file_path , \"r\" ) as f : if key not in f : raise KeyError ( f \"Key ' { key } ' not found in the h5 file.\" ) dataset = f [ key ] if isinstance ( dataset , h5py . Dataset ): obj = dataset [()] attrs = { k : v for k , v in dataset . attrs . items ()} else : raise TypeError ( f \"Key ' { key } ' does not refer to a dataset in the h5 file.\" ) return obj , attrs","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;read_h5_file"},{"location":"reference/prismtoolbox/utils/data_utils/#prismtoolbox.utils.data_utils.read_json_with_geopandas","text":"Read a json file with geopandas. Parameters: Name Type Description Default file_path str The path to a json file. required offset tuple [ int , int ] The offset to apply to the coordinates of the geometries. (0, 0) Returns: Type Description GeoDataFrame A GeoDataFrame object from the json file. Source code in src/prismtoolbox/utils/data_utils.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def read_json_with_geopandas ( file_path : str , offset : tuple [ int , int ] = ( 0 , 0 ) ) -> gpd . GeoDataFrame : \"\"\"Read a json file with geopandas. Args: file_path: The path to a json file. offset: The offset to apply to the coordinates of the geometries. Returns: A GeoDataFrame object from the json file. \"\"\" data = load_obj_with_json ( file_path ) df = gpd . GeoDataFrame . from_features ( data ) df . translate ( xoff = offset [ 0 ], yoff = offset [ 1 ]) if not df . is_valid . any (): df . loc [ ~ df . is_valid , :] = df . loc [ ~ df . is_valid , :] . buffer ( 0 ) if \"classification\" in df . columns : df [ \"classification\" ] = df [ \"classification\" ] . apply ( lambda x : x [ \"name\" ] if type ( x ) == dict else x ) return df","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;read_json_with_geopandas"},{"location":"reference/prismtoolbox/utils/data_utils/#prismtoolbox.utils.data_utils.save_obj_with_json","text":"Save an object to a file using json. Parameters: Name Type Description Default obj object A json object. required file_path str The path to the file. required Source code in src/prismtoolbox/utils/data_utils.py 22 23 24 25 26 27 28 29 30 def save_obj_with_json ( obj : object , file_path : str ) -> None : \"\"\"Save an object to a file using json. Args: obj: A json object. file_path: The path to the file. \"\"\" with open ( file_path , \"w\" ) as f : json . dump ( obj , f )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;save_obj_with_json"},{"location":"reference/prismtoolbox/utils/data_utils/#prismtoolbox.utils.data_utils.save_obj_with_pickle","text":"Save an object to a file using pickle. Parameters: Name Type Description Default obj object A pickeable object. required file_path str The path to the file. required Source code in src/prismtoolbox/utils/data_utils.py 11 12 13 14 15 16 17 18 19 def save_obj_with_pickle ( obj : object , file_path : str ) -> None : \"\"\"Save an object to a file using pickle. Args: obj: A pickeable object. file_path: The path to the file. \"\"\" with open ( file_path , \"wb\" ) as f : pickle . dump ( obj , f )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;save_obj_with_pickle"},{"location":"reference/prismtoolbox/utils/qupath_utils/","text":"PolygonsToContours ( polygons ) Converts shapely polygons to list of arrays. :param polygons: shapely polygons to convert to arrays :return: list of contours containing the opencv-like contours created from the shapely polygons Source code in src/prismtoolbox/utils/qupath_utils.py 53 54 55 56 57 58 59 60 61 62 def PolygonsToContours ( polygons : MultiPolygon ) -> list [ np . ndarray ]: \"\"\"Converts shapely polygons to list of arrays. :param polygons: shapely polygons to convert to arrays :return: list of contours containing the opencv-like contours created from the shapely polygons \"\"\" return [ np . array ( poly . exterior . coords )[: - 1 , None , ... ] . astype ( int ) for poly in polygons . geoms ] contoursToPolygons ( contours , merge = False , make_valid = False ) Converts list of arrays to shapely polygons. :param contours: list of contours to convert to shapely polygons :param merge: optional boolean to merge the polygons :param make_valid: optional boolean to enforce validity of the polygons :return: MultiPolygon object containing the polygons created from the arrays Source code in src/prismtoolbox/utils/qupath_utils.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def contoursToPolygons ( contours : list [ np . ndarray ], merge : bool = False , make_valid : bool = False , ) -> MultiPolygon : \"\"\"Converts list of arrays to shapely polygons. :param contours: list of contours to convert to shapely polygons :param merge: optional boolean to merge the polygons :param make_valid: optional boolean to enforce validity of the polygons :return: MultiPolygon object containing the polygons created from the arrays \"\"\" polygons = [ Polygon ( contour . squeeze ()) . buffer ( 0 ) for contour in contours ] result = [] for poly in polygons : if poly . is_empty : continue if isinstance ( poly , MultiPolygon ): result . append ( max ( poly . geoms , key = lambda x : x . area )) else : result . append ( poly ) polygons = MultiPolygon ( result ) if make_valid and not polygons . is_valid : buffered = polygons . buffer ( 0 ) if isinstance ( buffered , Polygon ): polygons = MultiPolygon ([ buffered ]) if merge : polygons = unary_union ( polygons ) if isinstance ( polygons , Polygon ): polygons = MultiPolygon ([ polygons ]) if not isinstance ( polygons , MultiPolygon ): raise ValueError ( \"Resulting polygons are not a MultiPolygon.\" ) return polygons convert_rgb_to_java_int_signed ( rgb ) Converts RGB tuple to Java signed integer. :param rgb: RGB tuple :return: Java signed integer Source code in src/prismtoolbox/utils/qupath_utils.py 98 99 100 101 102 103 104 105 106 107 108 def convert_rgb_to_java_int_signed ( rgb : tuple [ int , int , int ]) -> int : \"\"\"Converts RGB tuple to Java signed integer. :param rgb: RGB tuple :return: Java signed integer \"\"\" r , g , b = rgb java_rgb = ( 255 << 24 ) | ( r << 16 ) | ( g << 8 ) | b if java_rgb >= ( 1 << 31 ): java_rgb -= 1 << 32 return java_rgb export_polygons_to_qupath ( polygons , path , object_type , offset = ( 0 , 0 ), label = None , color = None , append_to_existing_file = False , as_feature_collection = False ) Exports polygons to a .json or .geojson file. :param polygons: shapely polygons to export :param path: path to the .geojson file :param object_type: type of the object (should be either \"annotation\" or \"detection\") :param offset: optional offset to add to each coordinate in the arrays :param label: optional label of the polygons :param color: optional color of the polygons :param append_to_existing_file: optional boolean to append the polygons to an existing file :param as_feature_collection: optional boolean to save the polygons as a FeatureCollection Source code in src/prismtoolbox/utils/qupath_utils.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def export_polygons_to_qupath ( polygons : MultiPolygon | Polygon , path : str , object_type : str , offset : tuple [ int , int ] = ( 0 , 0 ), label : str | None = None , color : tuple [ int , int , int ] | None = None , append_to_existing_file : bool = False , as_feature_collection : bool = False , ): \"\"\"Exports polygons to a .json or .geojson file. :param polygons: shapely polygons to export :param path: path to the .geojson file :param object_type: type of the object (should be either \"annotation\" or \"detection\") :param offset: optional offset to add to each coordinate in the arrays :param label: optional label of the polygons :param color: optional color of the polygons :param append_to_existing_file: optional boolean to append the polygons to an existing file :param as_feature_collection: optional boolean to save the polygons as a FeatureCollection \"\"\" if isinstance ( polygons , Polygon ): polygons = MultiPolygon ([ polygons ]) features = [] properties = {} properties [ \"objectType\" ] = object_type if label is not None : if color is None : log . warning ( \"No color provided for the label, using default color (255, 0, 0).\" ) color = ( 255 , 0 , 0 ) properties [ \"classification\" ] = { \"name\" : label , \"colorRGB\" : convert_rgb_to_java_int_signed ( color ), } polygons = translate ( polygons , xoff = offset [ 0 ], yoff = offset [ 1 ]) for poly in polygons . geoms : features . append ( { \"type\" : \"Feature\" , \"id\" : str ( uuid . uuid4 ()), \"geometry\" : mapping ( poly ), \"properties\" : properties , } ) features = ( { \"type\" : \"FeatureCollection\" , \"features\" : features } if as_feature_collection else features ) if os . path . exists ( path ) and append_to_existing_file : previous_features = load_obj_with_json ( path ) if len ( previous_features ) == 0 : log . warning ( \"The .geojson file does not contain any features, creating new file.\" ) else : if as_feature_collection : previous_features [ \"features\" ] . extend ( features [ \"features\" ]) # type: ignore else : previous_features . extend ( features ) features = previous_features save_obj_with_json ( features , path ) intersectionPolygons ( polygons1 , polygons2 ) Computes the intersection of two MultiPolygons. :param polygons1: first MultiPolygon :param polygons2: second MultiPolygon :return: MultiPolygon containing the intersection of the two input MultiPolygons Source code in src/prismtoolbox/utils/qupath_utils.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def intersectionPolygons ( polygons1 : MultiPolygon , polygons2 : MultiPolygon ) -> MultiPolygon : \"\"\"Computes the intersection of two MultiPolygons. :param polygons1: first MultiPolygon :param polygons2: second MultiPolygon :return: MultiPolygon containing the intersection of the two input MultiPolygons \"\"\" intersection = polygons1 & polygons2 if isinstance ( intersection , MultiPolygon ): return intersection elif intersection . geom_type == \"GeometryCollection\" : intersection = MultiPolygon ( [ poly for poly in intersection . geoms if isinstance ( poly , Polygon )] # type: ignore ) elif isinstance ( intersection , Polygon ): intersection = MultiPolygon ([ intersection ]) else : raise ValueError ( \"Intersection of provided MultiPolygons is not a MultiPolygon or a Polygon\" ) return intersection patchesToPolygons ( patches , patch_size , patch_downsample , merge = False ) Converts patches to shapely polygons. :param patches: Top left point coordinates of the patches to convert to shapely polygons :param patch_size: size of the patches :param merge: optional boolean to merge the polygons :return: MultiPolygon object containing the polygons created from the patches Source code in src/prismtoolbox/utils/qupath_utils.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def patchesToPolygons ( patches : np . ndarray , patch_size : int , patch_downsample : int , merge : bool = False , ) -> MultiPolygon : \"\"\"Converts patches to shapely polygons. :param patches: Top left point coordinates of the patches to convert to shapely polygons :param patch_size: size of the patches :param merge: optional boolean to merge the polygons :return: MultiPolygon object containing the polygons created from the patches \"\"\" polygons = [] ref_patch_size = patch_size * patch_downsample for patch in patches : x , y = patch polygons . append ( box ( x , y , x + ref_patch_size , y + ref_patch_size , ccw = False )) polygons = MultiPolygon ( polygons ) if merge : polygons = unary_union ( polygons ) if isinstance ( polygons , Polygon ): polygons = MultiPolygon ([ polygons ]) if not isinstance ( polygons , MultiPolygon ): raise ValueError ( \"Resulting polygons are not a MultiPolygon.\" ) return polygons read_qupath_annotations ( path , offset = ( 0 , 0 ), class_name = 'annotation' , column_to_select = 'objectType' ) Reads pathologist annotations from a .geojson file. :param path: path to the .geojson file :param offset: optional offset to add to each coordinate in the arrays :param class_name: name of the class to select :param column_to_select: optional column to select :return: MultiPolygon object containing the polygons of the selected class. Source code in src/prismtoolbox/utils/qupath_utils.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def read_qupath_annotations ( path : str , offset : tuple [ int , int ] = ( 0 , 0 ), class_name : str = \"annotation\" , column_to_select : str = \"objectType\" , ) -> MultiPolygon : \"\"\"Reads pathologist annotations from a .geojson file. :param path: path to the .geojson file :param offset: optional offset to add to each coordinate in the arrays :param class_name: name of the class to select :param column_to_select: optional column to select :return: MultiPolygon object containing the polygons of the selected class. \"\"\" df = read_json_with_geopandas ( path , offset ) polygons = [] for poly in df . loc [ df [ column_to_select ] == class_name , \"geometry\" ] . values : if poly . geom_type == \"Polygon\" : polygons . append ( poly ) elif poly . geom_type == \"MultiPolygon\" : polygons . extend ( poly . geoms ) else : raise ValueError ( \"Geometry type not supported.\" ) polygons = MultiPolygon ( polygons ) if not polygons . is_valid : buffered = polygons . buffer ( 0 ) if isinstance ( buffered , Polygon ): polygons = MultiPolygon ([ buffered ]) if not isinstance ( polygons , MultiPolygon ): raise ValueError ( \"Resulting polygons are not a MultiPolygon.\" ) return polygons","title":"Qupath utils"},{"location":"reference/prismtoolbox/utils/qupath_utils/#prismtoolbox.utils.qupath_utils.PolygonsToContours","text":"Converts shapely polygons to list of arrays. :param polygons: shapely polygons to convert to arrays :return: list of contours containing the opencv-like contours created from the shapely polygons Source code in src/prismtoolbox/utils/qupath_utils.py 53 54 55 56 57 58 59 60 61 62 def PolygonsToContours ( polygons : MultiPolygon ) -> list [ np . ndarray ]: \"\"\"Converts shapely polygons to list of arrays. :param polygons: shapely polygons to convert to arrays :return: list of contours containing the opencv-like contours created from the shapely polygons \"\"\" return [ np . array ( poly . exterior . coords )[: - 1 , None , ... ] . astype ( int ) for poly in polygons . geoms ]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;PolygonsToContours"},{"location":"reference/prismtoolbox/utils/qupath_utils/#prismtoolbox.utils.qupath_utils.contoursToPolygons","text":"Converts list of arrays to shapely polygons. :param contours: list of contours to convert to shapely polygons :param merge: optional boolean to merge the polygons :param make_valid: optional boolean to enforce validity of the polygons :return: MultiPolygon object containing the polygons created from the arrays Source code in src/prismtoolbox/utils/qupath_utils.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def contoursToPolygons ( contours : list [ np . ndarray ], merge : bool = False , make_valid : bool = False , ) -> MultiPolygon : \"\"\"Converts list of arrays to shapely polygons. :param contours: list of contours to convert to shapely polygons :param merge: optional boolean to merge the polygons :param make_valid: optional boolean to enforce validity of the polygons :return: MultiPolygon object containing the polygons created from the arrays \"\"\" polygons = [ Polygon ( contour . squeeze ()) . buffer ( 0 ) for contour in contours ] result = [] for poly in polygons : if poly . is_empty : continue if isinstance ( poly , MultiPolygon ): result . append ( max ( poly . geoms , key = lambda x : x . area )) else : result . append ( poly ) polygons = MultiPolygon ( result ) if make_valid and not polygons . is_valid : buffered = polygons . buffer ( 0 ) if isinstance ( buffered , Polygon ): polygons = MultiPolygon ([ buffered ]) if merge : polygons = unary_union ( polygons ) if isinstance ( polygons , Polygon ): polygons = MultiPolygon ([ polygons ]) if not isinstance ( polygons , MultiPolygon ): raise ValueError ( \"Resulting polygons are not a MultiPolygon.\" ) return polygons","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;contoursToPolygons"},{"location":"reference/prismtoolbox/utils/qupath_utils/#prismtoolbox.utils.qupath_utils.convert_rgb_to_java_int_signed","text":"Converts RGB tuple to Java signed integer. :param rgb: RGB tuple :return: Java signed integer Source code in src/prismtoolbox/utils/qupath_utils.py 98 99 100 101 102 103 104 105 106 107 108 def convert_rgb_to_java_int_signed ( rgb : tuple [ int , int , int ]) -> int : \"\"\"Converts RGB tuple to Java signed integer. :param rgb: RGB tuple :return: Java signed integer \"\"\" r , g , b = rgb java_rgb = ( 255 << 24 ) | ( r << 16 ) | ( g << 8 ) | b if java_rgb >= ( 1 << 31 ): java_rgb -= 1 << 32 return java_rgb","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;convert_rgb_to_java_int_signed"},{"location":"reference/prismtoolbox/utils/qupath_utils/#prismtoolbox.utils.qupath_utils.export_polygons_to_qupath","text":"Exports polygons to a .json or .geojson file. :param polygons: shapely polygons to export :param path: path to the .geojson file :param object_type: type of the object (should be either \"annotation\" or \"detection\") :param offset: optional offset to add to each coordinate in the arrays :param label: optional label of the polygons :param color: optional color of the polygons :param append_to_existing_file: optional boolean to append the polygons to an existing file :param as_feature_collection: optional boolean to save the polygons as a FeatureCollection Source code in src/prismtoolbox/utils/qupath_utils.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def export_polygons_to_qupath ( polygons : MultiPolygon | Polygon , path : str , object_type : str , offset : tuple [ int , int ] = ( 0 , 0 ), label : str | None = None , color : tuple [ int , int , int ] | None = None , append_to_existing_file : bool = False , as_feature_collection : bool = False , ): \"\"\"Exports polygons to a .json or .geojson file. :param polygons: shapely polygons to export :param path: path to the .geojson file :param object_type: type of the object (should be either \"annotation\" or \"detection\") :param offset: optional offset to add to each coordinate in the arrays :param label: optional label of the polygons :param color: optional color of the polygons :param append_to_existing_file: optional boolean to append the polygons to an existing file :param as_feature_collection: optional boolean to save the polygons as a FeatureCollection \"\"\" if isinstance ( polygons , Polygon ): polygons = MultiPolygon ([ polygons ]) features = [] properties = {} properties [ \"objectType\" ] = object_type if label is not None : if color is None : log . warning ( \"No color provided for the label, using default color (255, 0, 0).\" ) color = ( 255 , 0 , 0 ) properties [ \"classification\" ] = { \"name\" : label , \"colorRGB\" : convert_rgb_to_java_int_signed ( color ), } polygons = translate ( polygons , xoff = offset [ 0 ], yoff = offset [ 1 ]) for poly in polygons . geoms : features . append ( { \"type\" : \"Feature\" , \"id\" : str ( uuid . uuid4 ()), \"geometry\" : mapping ( poly ), \"properties\" : properties , } ) features = ( { \"type\" : \"FeatureCollection\" , \"features\" : features } if as_feature_collection else features ) if os . path . exists ( path ) and append_to_existing_file : previous_features = load_obj_with_json ( path ) if len ( previous_features ) == 0 : log . warning ( \"The .geojson file does not contain any features, creating new file.\" ) else : if as_feature_collection : previous_features [ \"features\" ] . extend ( features [ \"features\" ]) # type: ignore else : previous_features . extend ( features ) features = previous_features save_obj_with_json ( features , path )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;export_polygons_to_qupath"},{"location":"reference/prismtoolbox/utils/qupath_utils/#prismtoolbox.utils.qupath_utils.intersectionPolygons","text":"Computes the intersection of two MultiPolygons. :param polygons1: first MultiPolygon :param polygons2: second MultiPolygon :return: MultiPolygon containing the intersection of the two input MultiPolygons Source code in src/prismtoolbox/utils/qupath_utils.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def intersectionPolygons ( polygons1 : MultiPolygon , polygons2 : MultiPolygon ) -> MultiPolygon : \"\"\"Computes the intersection of two MultiPolygons. :param polygons1: first MultiPolygon :param polygons2: second MultiPolygon :return: MultiPolygon containing the intersection of the two input MultiPolygons \"\"\" intersection = polygons1 & polygons2 if isinstance ( intersection , MultiPolygon ): return intersection elif intersection . geom_type == \"GeometryCollection\" : intersection = MultiPolygon ( [ poly for poly in intersection . geoms if isinstance ( poly , Polygon )] # type: ignore ) elif isinstance ( intersection , Polygon ): intersection = MultiPolygon ([ intersection ]) else : raise ValueError ( \"Intersection of provided MultiPolygons is not a MultiPolygon or a Polygon\" ) return intersection","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;intersectionPolygons"},{"location":"reference/prismtoolbox/utils/qupath_utils/#prismtoolbox.utils.qupath_utils.patchesToPolygons","text":"Converts patches to shapely polygons. :param patches: Top left point coordinates of the patches to convert to shapely polygons :param patch_size: size of the patches :param merge: optional boolean to merge the polygons :return: MultiPolygon object containing the polygons created from the patches Source code in src/prismtoolbox/utils/qupath_utils.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def patchesToPolygons ( patches : np . ndarray , patch_size : int , patch_downsample : int , merge : bool = False , ) -> MultiPolygon : \"\"\"Converts patches to shapely polygons. :param patches: Top left point coordinates of the patches to convert to shapely polygons :param patch_size: size of the patches :param merge: optional boolean to merge the polygons :return: MultiPolygon object containing the polygons created from the patches \"\"\" polygons = [] ref_patch_size = patch_size * patch_downsample for patch in patches : x , y = patch polygons . append ( box ( x , y , x + ref_patch_size , y + ref_patch_size , ccw = False )) polygons = MultiPolygon ( polygons ) if merge : polygons = unary_union ( polygons ) if isinstance ( polygons , Polygon ): polygons = MultiPolygon ([ polygons ]) if not isinstance ( polygons , MultiPolygon ): raise ValueError ( \"Resulting polygons are not a MultiPolygon.\" ) return polygons","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;patchesToPolygons"},{"location":"reference/prismtoolbox/utils/qupath_utils/#prismtoolbox.utils.qupath_utils.read_qupath_annotations","text":"Reads pathologist annotations from a .geojson file. :param path: path to the .geojson file :param offset: optional offset to add to each coordinate in the arrays :param class_name: name of the class to select :param column_to_select: optional column to select :return: MultiPolygon object containing the polygons of the selected class. Source code in src/prismtoolbox/utils/qupath_utils.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def read_qupath_annotations ( path : str , offset : tuple [ int , int ] = ( 0 , 0 ), class_name : str = \"annotation\" , column_to_select : str = \"objectType\" , ) -> MultiPolygon : \"\"\"Reads pathologist annotations from a .geojson file. :param path: path to the .geojson file :param offset: optional offset to add to each coordinate in the arrays :param class_name: name of the class to select :param column_to_select: optional column to select :return: MultiPolygon object containing the polygons of the selected class. \"\"\" df = read_json_with_geopandas ( path , offset ) polygons = [] for poly in df . loc [ df [ column_to_select ] == class_name , \"geometry\" ] . values : if poly . geom_type == \"Polygon\" : polygons . append ( poly ) elif poly . geom_type == \"MultiPolygon\" : polygons . extend ( poly . geoms ) else : raise ValueError ( \"Geometry type not supported.\" ) polygons = MultiPolygon ( polygons ) if not polygons . is_valid : buffered = polygons . buffer ( 0 ) if isinstance ( buffered , Polygon ): polygons = MultiPolygon ([ buffered ]) if not isinstance ( polygons , MultiPolygon ): raise ValueError ( \"Resulting polygons are not a MultiPolygon.\" ) return polygons","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;read_qupath_annotations"},{"location":"reference/prismtoolbox/utils/stain_utils/","text":"","title":"Stain utils"},{"location":"reference/prismtoolbox/utils/torch_utils/","text":"create_transforms ( transforms_dict ) Create a torchvision.transforms.Compose object from a dictionary of transforms. Parameters: Name Type Description Default transforms_dict dict [ str , dict [ str , Any ]] Dictionary of transforms. The keys are the names of the transforms and the values are the parameters to pass to the transform as a dictionary. Possible transforms are: \"totensor\": ToTensorv2. A custom transform that converts a PIL image to a tensor as done in torchvision's original ToTensor transform. \"clip_custom\": ClipCustom. Any torchvision v2 transform. The name of the transform should be in lowercase. Please refer to the torchvision documentation for the parameters of each torchvision transform. required Returns: Type Description Compose A torchvision.transforms.Compose object. Source code in src/prismtoolbox/utils/torch_utils.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 def create_transforms ( transforms_dict : dict [ str , dict [ str , Any ]]) -> transformsv2 . Compose : \"\"\"Create a torchvision.transforms.Compose object from a dictionary of transforms. Args: transforms_dict: Dictionary of transforms. The keys are the names of the transforms and the values are the parameters to pass to the transform as a dictionary. Possible transforms are: - \"totensor\": ToTensorv2. A custom transform that converts a PIL image to a tensor as done in torchvision's original ToTensor transform. - \"clip_custom\": ClipCustom. - Any torchvision v2 transform. The name of the transform should be in lowercase. Please refer to the [torchvision documentation](https://pytorch.org/vision/stable/transforms.html) for the parameters of each torchvision transform. Returns: A torchvision.transforms.Compose object. \"\"\" if any ( transform_name not in possible_transforms for transform_name in transforms_dict ): raise ValueError ( f \"invalid transform name. Possible transforms: { possible_transforms . keys () } \" ) transforms = transformsv2 . Compose ( [ possible_transforms [ transform_name ]( ** transform_params ) for transform_name , transform_params in transforms_dict . items () ] ) return transforms","title":"Torch utils"},{"location":"reference/prismtoolbox/utils/torch_utils/#prismtoolbox.utils.torch_utils.create_transforms","text":"Create a torchvision.transforms.Compose object from a dictionary of transforms. Parameters: Name Type Description Default transforms_dict dict [ str , dict [ str , Any ]] Dictionary of transforms. The keys are the names of the transforms and the values are the parameters to pass to the transform as a dictionary. Possible transforms are: \"totensor\": ToTensorv2. A custom transform that converts a PIL image to a tensor as done in torchvision's original ToTensor transform. \"clip_custom\": ClipCustom. Any torchvision v2 transform. The name of the transform should be in lowercase. Please refer to the torchvision documentation for the parameters of each torchvision transform. required Returns: Type Description Compose A torchvision.transforms.Compose object. Source code in src/prismtoolbox/utils/torch_utils.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 def create_transforms ( transforms_dict : dict [ str , dict [ str , Any ]]) -> transformsv2 . Compose : \"\"\"Create a torchvision.transforms.Compose object from a dictionary of transforms. Args: transforms_dict: Dictionary of transforms. The keys are the names of the transforms and the values are the parameters to pass to the transform as a dictionary. Possible transforms are: - \"totensor\": ToTensorv2. A custom transform that converts a PIL image to a tensor as done in torchvision's original ToTensor transform. - \"clip_custom\": ClipCustom. - Any torchvision v2 transform. The name of the transform should be in lowercase. Please refer to the [torchvision documentation](https://pytorch.org/vision/stable/transforms.html) for the parameters of each torchvision transform. Returns: A torchvision.transforms.Compose object. \"\"\" if any ( transform_name not in possible_transforms for transform_name in transforms_dict ): raise ValueError ( f \"invalid transform name. Possible transforms: { possible_transforms . keys () } \" ) transforms = transformsv2 . Compose ( [ possible_transforms [ transform_name ]( ** transform_params ) for transform_name , transform_params in transforms_dict . items () ] ) return transforms","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;create_transforms"},{"location":"reference/prismtoolbox/utils/vis_utils/","text":"bbox_from_contours ( contours , downsample_factor = 1 ) Compute the bounding box from a set of contours :param contours: list of contours :return: bounding box of the contours Source code in src/prismtoolbox/utils/vis_utils.py 29 30 31 32 33 34 35 36 37 38 39 40 def bbox_from_contours ( contours : list [ np . ndarray ], downsample_factor : int = 1 ) -> tuple [ int , int , int , int ]: \"\"\" Compute the bounding box from a set of contours :param contours: list of contours :return: bounding box of the contours \"\"\" flatten_contours = np . concatenate ( contours ) x_min , y_min = flatten_contours . min ( axis = 0 ) . squeeze () . astype ( int ) / downsample_factor x_max , y_max = flatten_contours . max ( axis = 0 ) . squeeze () . astype ( int ) / downsample_factor return x_min , y_min , x_max , y_max bbox_from_coords ( coords , patch_size = 0 , downsample_factor = 1 ) Compute the bounding box from a set of coordinates :param coords: coordinates of the patches :param patch_size: size of the patches (at level extraction) :return: bounding box of the patches Source code in src/prismtoolbox/utils/vis_utils.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def bbox_from_coords ( coords : np . ndarray , patch_size : int = 0 , downsample_factor : int = 1 , ) -> tuple [ int , int , int , int ]: \"\"\" Compute the bounding box from a set of coordinates :param coords: coordinates of the patches :param patch_size: size of the patches (at level extraction) :return: bounding box of the patches \"\"\" x_min , y_min = coords . min ( axis = 0 ) . astype ( int ) / downsample_factor x_max , y_max = coords . max ( axis = 0 ) . astype ( int ) / downsample_factor return x_min , y_min , x_max + patch_size , y_max + patch_size get_colors_from_cmap ( cmap_name , n_colors , scale = 255 ) Get a list of colors from a matplotlib colormap :param cmap_name: name of a matplotlib colormap :param n_colors: number of colors :return: list of colors Source code in src/prismtoolbox/utils/vis_utils.py 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_colors_from_cmap ( cmap_name , n_colors , scale = 255 ): \"\"\" Get a list of colors from a matplotlib colormap :param cmap_name: name of a matplotlib colormap :param n_colors: number of colors :return: list of colors \"\"\" cmap_name = plt . get_cmap ( cmap_name , n_colors ) colors_from_cmap = np . array ([ to_rgb ( cmap_name ( i )) for i in range ( n_colors )]) if scale > 1 : return ( colors_from_cmap * scale ) . astype ( int ) else : return colors_from_cmap init_image ( w , h , color_bakground = ( 255 , 255 , 255 ), mask = False ) Create an image with the dimension (w,h) and the color of the background set to color_bakground :param w: width of image :param h: height of image :param color_bakground: color of the background :param mask: set to True to create a binary mask :return: PIL image with the dimension (w,h), in grayscale when mask if set to True Source code in src/prismtoolbox/utils/vis_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def init_image ( w : int , h : int , color_bakground : tuple [ int , int , int ] = ( 255 , 255 , 255 ), mask : bool = False , ) -> np . ndarray : \"\"\" Create an image with the dimension (w,h) and the color of the background set to color_bakground :param w: width of image :param h: height of image :param color_bakground: color of the background :param mask: set to True to create a binary mask :return: PIL image with the dimension (w,h), in grayscale when mask if set to True \"\"\" if mask : return np . array ( ImageOps . grayscale ( Image . new ( size = ( w , h ), mode = \"RGB\" , color = 0 ))) else : return np . array ( Image . new ( size = ( w , h ), mode = \"RGB\" , color = color_bakground ))","title":"Vis utils"},{"location":"reference/prismtoolbox/utils/vis_utils/#prismtoolbox.utils.vis_utils.bbox_from_contours","text":"Compute the bounding box from a set of contours :param contours: list of contours :return: bounding box of the contours Source code in src/prismtoolbox/utils/vis_utils.py 29 30 31 32 33 34 35 36 37 38 39 40 def bbox_from_contours ( contours : list [ np . ndarray ], downsample_factor : int = 1 ) -> tuple [ int , int , int , int ]: \"\"\" Compute the bounding box from a set of contours :param contours: list of contours :return: bounding box of the contours \"\"\" flatten_contours = np . concatenate ( contours ) x_min , y_min = flatten_contours . min ( axis = 0 ) . squeeze () . astype ( int ) / downsample_factor x_max , y_max = flatten_contours . max ( axis = 0 ) . squeeze () . astype ( int ) / downsample_factor return x_min , y_min , x_max , y_max","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;bbox_from_contours"},{"location":"reference/prismtoolbox/utils/vis_utils/#prismtoolbox.utils.vis_utils.bbox_from_coords","text":"Compute the bounding box from a set of coordinates :param coords: coordinates of the patches :param patch_size: size of the patches (at level extraction) :return: bounding box of the patches Source code in src/prismtoolbox/utils/vis_utils.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def bbox_from_coords ( coords : np . ndarray , patch_size : int = 0 , downsample_factor : int = 1 , ) -> tuple [ int , int , int , int ]: \"\"\" Compute the bounding box from a set of coordinates :param coords: coordinates of the patches :param patch_size: size of the patches (at level extraction) :return: bounding box of the patches \"\"\" x_min , y_min = coords . min ( axis = 0 ) . astype ( int ) / downsample_factor x_max , y_max = coords . max ( axis = 0 ) . astype ( int ) / downsample_factor return x_min , y_min , x_max + patch_size , y_max + patch_size","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;bbox_from_coords"},{"location":"reference/prismtoolbox/utils/vis_utils/#prismtoolbox.utils.vis_utils.get_colors_from_cmap","text":"Get a list of colors from a matplotlib colormap :param cmap_name: name of a matplotlib colormap :param n_colors: number of colors :return: list of colors Source code in src/prismtoolbox/utils/vis_utils.py 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_colors_from_cmap ( cmap_name , n_colors , scale = 255 ): \"\"\" Get a list of colors from a matplotlib colormap :param cmap_name: name of a matplotlib colormap :param n_colors: number of colors :return: list of colors \"\"\" cmap_name = plt . get_cmap ( cmap_name , n_colors ) colors_from_cmap = np . array ([ to_rgb ( cmap_name ( i )) for i in range ( n_colors )]) if scale > 1 : return ( colors_from_cmap * scale ) . astype ( int ) else : return colors_from_cmap","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;get_colors_from_cmap"},{"location":"reference/prismtoolbox/utils/vis_utils/#prismtoolbox.utils.vis_utils.init_image","text":"Create an image with the dimension (w,h) and the color of the background set to color_bakground :param w: width of image :param h: height of image :param color_bakground: color of the background :param mask: set to True to create a binary mask :return: PIL image with the dimension (w,h), in grayscale when mask if set to True Source code in src/prismtoolbox/utils/vis_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def init_image ( w : int , h : int , color_bakground : tuple [ int , int , int ] = ( 255 , 255 , 255 ), mask : bool = False , ) -> np . ndarray : \"\"\" Create an image with the dimension (w,h) and the color of the background set to color_bakground :param w: width of image :param h: height of image :param color_bakground: color of the background :param mask: set to True to create a binary mask :return: PIL image with the dimension (w,h), in grayscale when mask if set to True \"\"\" if mask : return np . array ( ImageOps . grayscale ( Image . new ( size = ( w , h ), mode = \"RGB\" , color = 0 ))) else : return np . array ( Image . new ( size = ( w , h ), mode = \"RGB\" , color = color_bakground ))","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;init_image"},{"location":"reference/prismtoolbox/wsicore/","text":"","title":"Index"},{"location":"reference/prismtoolbox/wsicore/core_utils/","text":"IsInContour ( contour , patch_size , center_shift = 0.5 , mode = 'center' ) The IsInContour class checks if a patch is inside a contour. Parameters: Name Type Description Default contour ndarray The contour to check if the patch is inside. required patch_size int The size of the patches that will be checked. required center_shift float The shift of the center of the patch. 0.5 mode str The mode to use for the contour checking. Possible values are: \"center\" mode checks if the center of the patch is within the contour. \"four_pt\" mode checks if one out of the four corners of the patch are within the contour. \"four_pt_hard\" mode checks if the four corners of the patch are within the contour. 'center' Source code in src/prismtoolbox/wsicore/core_utils.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def __init__ ( self , contour : np . ndarray , patch_size : int , center_shift : float = 0.5 , mode : str = \"center\" , ): \"\"\"The IsInContour class checks if a patch is inside a contour. Args: contour: The contour to check if the patch is inside. patch_size: The size of the patches that will be checked. center_shift: The shift of the center of the patch. mode: The mode to use for the contour checking. Possible values are: - \"center\" mode checks if the center of the patch is within the contour. - \"four_pt\" mode checks if one out of the four corners of the patch are within the contour. - \"four_pt_hard\" mode checks if the four corners of the patch are within the contour. \"\"\" self . cont = contour self . patch_size = patch_size self . shift = int ( patch_size // 2 * center_shift ) self . mode = mode __call__ ( pt ) Check if a point is inside the contour. Parameters: Name Type Description Default pt tuple [ int , int ] The top left coordinate of the patch. required Returns: Type Description int 1 if the patch is inside the contour, 0 otherwise. Source code in src/prismtoolbox/wsicore/core_utils.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def __call__ ( self , pt : tuple [ int , int ]) -> int : \"\"\"Check if a point is inside the contour. Args: pt: The top left coordinate of the patch. Returns: 1 if the patch is inside the contour, 0 otherwise. \"\"\" center = ( int ( pt [ 0 ] + self . patch_size // 2 ), int ( pt [ 1 ] + self . patch_size // 2 )) if self . mode == \"center\" : return 1 if cv2 . pointPolygonTest ( self . cont , center , False ) >= 0 else 0 else : if self . shift > 0 : all_points = [ ( center [ 0 ] - self . shift , center [ 1 ] - self . shift ), ( center [ 0 ] + self . shift , center [ 1 ] + self . shift ), ( center [ 0 ] + self . shift , center [ 1 ] - self . shift ), ( center [ 0 ] - self . shift , center [ 1 ] + self . shift ), ] else : all_points = [ center ] # Easy version of 4pt contour checking function - 1 of 4 points need to be in the contour for test to pass if self . mode == \"four_pt\" : for points in all_points : if cv2 . pointPolygonTest ( self . cont , points , False ) >= 0 : return 1 return 0 # Hard version of 4pt contour checking function - all 4 points need to be in the contour for test to pass elif self . mode == \"four_pt_hard\" : for points in all_points : if cv2 . pointPolygonTest ( self . cont , points , False ) < 0 : return 0 return 1 else : raise ValueError ( f \"mode { self . mode } not recognized\" ) apply_bilateral_filter ( img ) Apply a bilateral filter on a grayscale image. Parameters: Name Type Description Default img ndarray An input grayscale image. required Returns: Type Description ndarray The input image after applying a bilateral filter of parameters 3, 3*2, 3/2. Source code in src/prismtoolbox/wsicore/core_utils.py 150 151 152 153 154 155 156 157 158 159 160 161 def apply_bilateral_filter ( img : np . ndarray ) -> np . ndarray : \"\"\"Apply a bilateral filter on a grayscale image. Args: img: An input grayscale image. Returns: The input image after applying a bilateral filter of parameters 3, 3*2, 3/2. \"\"\" assert len ( img . shape ) == 2 , f \"Input image should be grayscale, got shape { img . shape } \" img_filtered = cv2 . bilateralFilter ( img , 3 , 3 * 2 , 3 / 2 ) return img_filtered apply_binary_thresh ( img , thresh , inv ) Apply a binary threshold on a grayscale image. Parameters: Name Type Description Default img ndarray An input grayscale image. required thresh int The threshold value. required inv bool Set to True to invert the binary threshold. required Returns: Type Description ndarray The input image after applying a binary threshold. Source code in src/prismtoolbox/wsicore/core_utils.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def apply_binary_thresh ( img : np . ndarray , thresh : int , inv : bool ) -> np . ndarray : \"\"\"Apply a binary threshold on a grayscale image. Args: img: An input grayscale image. thresh: The threshold value. inv: Set to True to invert the binary threshold. Returns: The input image after applying a binary threshold. \"\"\" if inv : _ , img_thresh = cv2 . threshold ( img , thresh , 255 , cv2 . THRESH_BINARY_INV ) else : _ , img_thresh = cv2 . threshold ( img , thresh , 255 , cv2 . THRESH_BINARY ) return img_thresh compute_law_feats ( img , window_size , only_s5 = True ) Compute the Law's texture energy for a given grayscale image and window size. Parameters: Name Type Description Default img ndarray An input grayscale image. required window_size int The window size to use for Law's texture energy computation. required only_s5 bool Set to True to only compute the Law's texture energy for the S5S5 filter. True Returns: Type Description list [ ndarray ] A list of Law's texture energy map for each filter extracted from the input image. Source code in src/prismtoolbox/wsicore/core_utils.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def compute_law_feats ( img : np . ndarray , window_size : int , only_s5 : bool = True ) -> list [ np . ndarray ]: \"\"\"Compute the Law's texture energy for a given grayscale image and window size. Args: img: An input grayscale image. window_size: The window size to use for Law's texture energy computation. only_s5: Set to True to only compute the Law's texture energy for the S5S5 filter. Returns: A list of Law's texture energy map for each filter extracted from the input image. \"\"\" L5 = np . array ([ 1 , 4 , 6 , 4 , 1 ]) E5 = np . array ([ - 1 , - 2 , 0 , 2 , 1 ]) S5 = np . array ([ - 1 , 0 , 2 , 0 , - 1 ]) R5 = np . array ([ 1 , - 4 , 6 , - 4 , 1 ]) vectors = [ L5 , E5 , S5 , R5 ] vectors_name = [ \"L5\" , \"E5\" , \"S5\" , \"R5\" ] if only_s5 : vectors = [ vectors [ 2 ]] vectors_name = [ vectors_name [ 2 ]] filters = [ np . expand_dims ( vectors [ i ], - 1 ) . dot ( np . expand_dims ( vectors [ j ], - 1 ) . T ) for i , j in product ( range ( len ( vectors )), range ( len ( vectors ))) ] filters_name = np . array ( [ vectors_name [ i ] + vectors_name [ j ] for i , j in product ( range ( len ( vectors )), range ( len ( vectors ))) ] ) imgs_filtered = [] for filt in filters : imgs_filtered . append ( oaconvolve ( img , filt , mode = \"same\" )) window = np . ones (( window_size , window_size )) imgs_energy = [] for img_filtered in imgs_filtered : imgs_energy . append ( oaconvolve ( np . abs ( img_filtered ), window , mode = \"same\" )) def _get_img_energy ( name ): return imgs_energy [ np . where ( filters_name == name )[ 0 ] . item ()] if only_s5 : imgs_feats = [ _get_img_energy ( \"S5S5\" )] else : imgs_feats = [ np . mean ( np . array ([ _get_img_energy ( \"L5E5\" ), _get_img_energy ( \"E5L5\" )]), axis = 0 ), np . mean ( np . array ([ _get_img_energy ( \"L5R5\" ), _get_img_energy ( \"R5L5\" )]), axis = 0 ), np . mean ( np . array ([ _get_img_energy ( \"E5S5\" ), _get_img_energy ( \"S5E5\" )]), axis = 0 ), _get_img_energy ( \"S5S5\" ), _get_img_energy ( \"R5R5\" ), np . mean ( np . array ([ _get_img_energy ( \"L5S5\" ), _get_img_energy ( \"S5L5\" )]), axis = 0 ), _get_img_energy ( \"E5E5\" ), np . mean ( np . array ([ _get_img_energy ( \"E5R5\" ), _get_img_energy ( \"R5E5\" )]), axis = 0 ), np . mean ( np . array ([ _get_img_energy ( \"S5R5\" ), _get_img_energy ( \"R5S5\" )]), axis = 0 ), ] return imgs_feats contour_mask ( mask ) Find the contours in a binary image with opencv findContours. Parameters: Name Type Description Default mask ndarray A binary image. required Returns: Type Description list [ ndarray ] A list of contours found in the input image with opencv findContours. Source code in src/prismtoolbox/wsicore/core_utils.py 201 202 203 204 205 206 207 208 209 210 211 212 def contour_mask ( mask : np . ndarray ) -> list [ np . ndarray ]: \"\"\"Find the contours in a binary image with opencv findContours. Args: mask: A binary image. Returns: A list of contours found in the input image with opencv findContours. \"\"\" contours = cv2 . findContours ( mask , cv2 . RETR_TREE , cv2 . CHAIN_APPROX_SIMPLE ) contours = contours [ 0 ] if len ( contours ) == 2 else contours [ 1 ] return [ np . array ( contour ) for contour in contours ] floodfill_img ( img , start ) Perform the floodfill algorithm on a binary image. Parameters: Name Type Description Default img ndarray A binary image. required start tuple [ int , int ] coordinates of the starting point for the floodfill algorithm. required Returns: Type Description ndarray The input image after applying the floodfill algorithm. Source code in src/prismtoolbox/wsicore/core_utils.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def floodfill_img ( img : np . ndarray , start : tuple [ int , int ]) -> np . ndarray : \"\"\"Perform the floodfill algorithm on a binary image. Args: img: A binary image. start: coordinates of the starting point for the floodfill algorithm. Returns: The input image after applying the floodfill algorithm. \"\"\" img_to_floodfill = img . copy () h , w = img . shape [: 2 ] mask = np . zeros (( h + 2 , w + 2 ), np . uint8 ) cv2 . floodFill ( img_to_floodfill , mask , start , 255 ) # type: ignore im_floodfill_inv = cv2 . bitwise_not ( img_to_floodfill ) img_out = img | im_floodfill_inv return img_out [ 1 : - 1 , 1 : - 1 ] isBlackPatch ( patch , rgb_thresh = 20 , percentage = 0.05 ) Check if a patch is black. Parameters: Name Type Description Default patch Image An input patch. required rgb_thresh int The threshold value for the RGB channels to be considered black. 20 percentage float The percentage of pixels below the threshold to consider the patch as black. 0.05 Returns: Type Description bool True if the patch is black, False otherwise. Source code in src/prismtoolbox/wsicore/core_utils.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def isBlackPatch ( patch : Image . Image , rgb_thresh : int = 20 , percentage : float = 0.05 ) -> bool : \"\"\"Check if a patch is black. Args: patch: An input patch. rgb_thresh: The threshold value for the RGB channels to be considered black. percentage: The percentage of pixels below the threshold to consider the patch as black. Returns: True if the patch is black, False otherwise. \"\"\" num_pixels = patch . size [ 0 ] * patch . size [ 1 ] return ( True if np . all ( np . array ( patch ) < rgb_thresh , axis = 2 ) . sum () > num_pixels * percentage else False ) isWhitePatch ( patch , rgb_thresh = 220 , percentage = 0.2 ) Check if a patch is white. Parameters: Name Type Description Default patch Image An input patch. required rgb_thresh int The threshold value for the RGB channels to be considered white. 220 percentage float The percentage of pixels above the threshold to consider the patch as white. 0.2 Returns: Type Description bool True if the patch is white, False otherwise. Source code in src/prismtoolbox/wsicore/core_utils.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def isWhitePatch ( patch : Image . Image , rgb_thresh : int = 220 , percentage : float = 0.2 ) -> bool : \"\"\"Check if a patch is white. Args: patch: An input patch. rgb_thresh: The threshold value for the RGB channels to be considered white. percentage: The percentage of pixels above the threshold to consider the patch as white. Returns: True if the patch is white, False otherwise. \"\"\" num_pixels = patch . size [ 0 ] * patch . size [ 1 ] return ( True if np . all ( np . array ( patch ) > rgb_thresh , axis = 2 ) . sum () > num_pixels * percentage else False ) local_average ( img , window_size ) Perform local averaging on the image for a given window size. Parameters: Name Type Description Default img ndarray An input image. required window_size int The window size to use for local averaging. required Returns: Type Description ndarray Grayscale image minus the local average on a window of size window_size of the input image. Source code in src/prismtoolbox/wsicore/core_utils.py 70 71 72 73 74 75 76 77 78 79 80 81 82 def local_average ( img : np . ndarray , window_size : int ) -> np . ndarray : \"\"\"Perform local averaging on the image for a given window size. Args: img: An input image. window_size: The window size to use for local averaging. Returns: Grayscale image minus the local average on a window of size window_size of the input image. \"\"\" window = np . ones (( window_size , window_size )) / ( window_size ** 2 ) img_grayscaled = cv2 . cvtColor ( img , cv2 . COLOR_RGB2GRAY ) return img_grayscaled - oaconvolve ( img_grayscaled , window , mode = \"same\" ) save_patches_with_hdf5 ( output_path , asset_dict , attr_dict = None ) Save patches to an HDF5 file. Parameters: Name Type Description Default output_path str The path to the output HDF5 file. required asset_dict dict [ str , ndarray ] A dictionary of patches to save (key: name of the dataset, value: array of patches coordinates). required attr_dict dict [ str , dict [ str , str | int | tuple [ int , int ]]] | None A dictionary of attributes to save (key: name of the dataset, value: dictionary of attributes to None set to the HDF5 dataset). Source code in src/prismtoolbox/wsicore/core_utils.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def save_patches_with_hdf5 ( output_path : str , asset_dict : dict [ str , np . ndarray ], attr_dict : dict [ str , dict [ str , str | int | tuple [ int , int ]]] | None = None , ) -> None : \"\"\"Save patches to an HDF5 file. Args: output_path: The path to the output HDF5 file. asset_dict: A dictionary of patches to save (key: name of the dataset, value: array of patches coordinates). attr_dict: A dictionary of attributes to save (key: name of the dataset, value: dictionary of attributes to set to the HDF5 dataset). \"\"\" file = h5py . File ( output_path , \"w\" ) for key , val in asset_dict . items (): data_shape = val . shape if key not in file : data_type = val . dtype chunk_shape = ( 1 ,) + data_shape [ 1 :] maxshape = ( None ,) + data_shape [ 1 :] dset = file . create_dataset ( key , shape = data_shape , maxshape = maxshape , chunks = chunk_shape , dtype = data_type , ) dset [:] = val if attr_dict is not None : if key in attr_dict . keys (): for attr_key , attr_val in attr_dict [ key ] . items (): dset . attrs [ attr_key ] = attr_val else : dset = file [ key ] assert isinstance ( dset , h5py . Dataset ), f \"Expected dataset, got { type ( dset ) } for key { key } .\" dset . resize ( len ( dset ) + data_shape [ 0 ], axis = 0 ) dset [ - data_shape [ 0 ] :] = val file . close () select_roi_on_thumbnail ( img , scale_factor ) Select a region of interest on the thumbnail of the slide using an interactive window. Parameters: Name Type Description Default img ndarray A thumbnail of the slide. required scale_factor int The scale factor to apply to convert the coordinates to the original slide dimensions. required Returns: Type Description ndarray The coordinates of the selected region of interest. Source code in src/prismtoolbox/wsicore/core_utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def select_roi_on_thumbnail ( img : np . ndarray , scale_factor : int ) -> np . ndarray : \"\"\"Select a region of interest on the thumbnail of the slide using an interactive window. Args: img: A thumbnail of the slide. scale_factor: The scale factor to apply to convert the coordinates to the original slide dimensions. Returns: The coordinates of the selected region of interest. \"\"\" drawing = False roi = [ 0 , 0 , 0 , 0 ] def _draw_rectangle ( event , x , y , flags , param ): nonlocal roi , drawing if event == cv2 . EVENT_LBUTTONDOWN : drawing = True roi [ 0 ], roi [ 1 ] = x , y elif event == cv2 . EVENT_MOUSEMOVE : if drawing : roi [ 2 ], roi [ 3 ] = x , y elif event == cv2 . EVENT_LBUTTONUP : drawing = False roi [ 2 ], roi [ 3 ] = x , y window_name = \"Select ROI (beginning by the top left corner) - press ESC to exit\" cv2 . namedWindow ( window_name , cv2 . WINDOW_NORMAL ) cv2 . resizeWindow ( window_name , 800 , 800 ) cv2 . setMouseCallback ( window_name , _draw_rectangle ) while True : temp_img = img . copy () if roi [ 2 ] and roi [ 3 ]: cv2 . rectangle ( temp_img , ( roi [ 0 ], roi [ 1 ]), ( roi [ 2 ], roi [ 3 ]), ( 0 , 255 , 0 ), 5 ) width = ( roi [ 2 ] - roi [ 0 ]) * scale_factor height = ( roi [ 3 ] - roi [ 1 ]) * scale_factor cv2 . putText ( temp_img , f \"Width: { width } - Height: { height } \" , ( roi [ 0 ], roi [ 1 ] - 10 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 0 , 255 , 0 ), 2 , ) cv2 . imshow ( window_name , temp_img ) k = cv2 . waitKey ( 1 ) & 0xFF if k == 27 : break cv2 . destroyAllWindows () return np . array ( roi )","title":"prismtoolbox.wsicore.core_utils"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.IsInContour","text":"The IsInContour class checks if a patch is inside a contour. Parameters: Name Type Description Default contour ndarray The contour to check if the patch is inside. required patch_size int The size of the patches that will be checked. required center_shift float The shift of the center of the patch. 0.5 mode str The mode to use for the contour checking. Possible values are: \"center\" mode checks if the center of the patch is within the contour. \"four_pt\" mode checks if one out of the four corners of the patch are within the contour. \"four_pt_hard\" mode checks if the four corners of the patch are within the contour. 'center' Source code in src/prismtoolbox/wsicore/core_utils.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def __init__ ( self , contour : np . ndarray , patch_size : int , center_shift : float = 0.5 , mode : str = \"center\" , ): \"\"\"The IsInContour class checks if a patch is inside a contour. Args: contour: The contour to check if the patch is inside. patch_size: The size of the patches that will be checked. center_shift: The shift of the center of the patch. mode: The mode to use for the contour checking. Possible values are: - \"center\" mode checks if the center of the patch is within the contour. - \"four_pt\" mode checks if one out of the four corners of the patch are within the contour. - \"four_pt_hard\" mode checks if the four corners of the patch are within the contour. \"\"\" self . cont = contour self . patch_size = patch_size self . shift = int ( patch_size // 2 * center_shift ) self . mode = mode","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;IsInContour"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.IsInContour.__call__","text":"Check if a point is inside the contour. Parameters: Name Type Description Default pt tuple [ int , int ] The top left coordinate of the patch. required Returns: Type Description int 1 if the patch is inside the contour, 0 otherwise. Source code in src/prismtoolbox/wsicore/core_utils.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def __call__ ( self , pt : tuple [ int , int ]) -> int : \"\"\"Check if a point is inside the contour. Args: pt: The top left coordinate of the patch. Returns: 1 if the patch is inside the contour, 0 otherwise. \"\"\" center = ( int ( pt [ 0 ] + self . patch_size // 2 ), int ( pt [ 1 ] + self . patch_size // 2 )) if self . mode == \"center\" : return 1 if cv2 . pointPolygonTest ( self . cont , center , False ) >= 0 else 0 else : if self . shift > 0 : all_points = [ ( center [ 0 ] - self . shift , center [ 1 ] - self . shift ), ( center [ 0 ] + self . shift , center [ 1 ] + self . shift ), ( center [ 0 ] + self . shift , center [ 1 ] - self . shift ), ( center [ 0 ] - self . shift , center [ 1 ] + self . shift ), ] else : all_points = [ center ] # Easy version of 4pt contour checking function - 1 of 4 points need to be in the contour for test to pass if self . mode == \"four_pt\" : for points in all_points : if cv2 . pointPolygonTest ( self . cont , points , False ) >= 0 : return 1 return 0 # Hard version of 4pt contour checking function - all 4 points need to be in the contour for test to pass elif self . mode == \"four_pt_hard\" : for points in all_points : if cv2 . pointPolygonTest ( self . cont , points , False ) < 0 : return 0 return 1 else : raise ValueError ( f \"mode { self . mode } not recognized\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;__call__"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.apply_bilateral_filter","text":"Apply a bilateral filter on a grayscale image. Parameters: Name Type Description Default img ndarray An input grayscale image. required Returns: Type Description ndarray The input image after applying a bilateral filter of parameters 3, 3*2, 3/2. Source code in src/prismtoolbox/wsicore/core_utils.py 150 151 152 153 154 155 156 157 158 159 160 161 def apply_bilateral_filter ( img : np . ndarray ) -> np . ndarray : \"\"\"Apply a bilateral filter on a grayscale image. Args: img: An input grayscale image. Returns: The input image after applying a bilateral filter of parameters 3, 3*2, 3/2. \"\"\" assert len ( img . shape ) == 2 , f \"Input image should be grayscale, got shape { img . shape } \" img_filtered = cv2 . bilateralFilter ( img , 3 , 3 * 2 , 3 / 2 ) return img_filtered","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;apply_bilateral_filter"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.apply_binary_thresh","text":"Apply a binary threshold on a grayscale image. Parameters: Name Type Description Default img ndarray An input grayscale image. required thresh int The threshold value. required inv bool Set to True to invert the binary threshold. required Returns: Type Description ndarray The input image after applying a binary threshold. Source code in src/prismtoolbox/wsicore/core_utils.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def apply_binary_thresh ( img : np . ndarray , thresh : int , inv : bool ) -> np . ndarray : \"\"\"Apply a binary threshold on a grayscale image. Args: img: An input grayscale image. thresh: The threshold value. inv: Set to True to invert the binary threshold. Returns: The input image after applying a binary threshold. \"\"\" if inv : _ , img_thresh = cv2 . threshold ( img , thresh , 255 , cv2 . THRESH_BINARY_INV ) else : _ , img_thresh = cv2 . threshold ( img , thresh , 255 , cv2 . THRESH_BINARY ) return img_thresh","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;apply_binary_thresh"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.compute_law_feats","text":"Compute the Law's texture energy for a given grayscale image and window size. Parameters: Name Type Description Default img ndarray An input grayscale image. required window_size int The window size to use for Law's texture energy computation. required only_s5 bool Set to True to only compute the Law's texture energy for the S5S5 filter. True Returns: Type Description list [ ndarray ] A list of Law's texture energy map for each filter extracted from the input image. Source code in src/prismtoolbox/wsicore/core_utils.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def compute_law_feats ( img : np . ndarray , window_size : int , only_s5 : bool = True ) -> list [ np . ndarray ]: \"\"\"Compute the Law's texture energy for a given grayscale image and window size. Args: img: An input grayscale image. window_size: The window size to use for Law's texture energy computation. only_s5: Set to True to only compute the Law's texture energy for the S5S5 filter. Returns: A list of Law's texture energy map for each filter extracted from the input image. \"\"\" L5 = np . array ([ 1 , 4 , 6 , 4 , 1 ]) E5 = np . array ([ - 1 , - 2 , 0 , 2 , 1 ]) S5 = np . array ([ - 1 , 0 , 2 , 0 , - 1 ]) R5 = np . array ([ 1 , - 4 , 6 , - 4 , 1 ]) vectors = [ L5 , E5 , S5 , R5 ] vectors_name = [ \"L5\" , \"E5\" , \"S5\" , \"R5\" ] if only_s5 : vectors = [ vectors [ 2 ]] vectors_name = [ vectors_name [ 2 ]] filters = [ np . expand_dims ( vectors [ i ], - 1 ) . dot ( np . expand_dims ( vectors [ j ], - 1 ) . T ) for i , j in product ( range ( len ( vectors )), range ( len ( vectors ))) ] filters_name = np . array ( [ vectors_name [ i ] + vectors_name [ j ] for i , j in product ( range ( len ( vectors )), range ( len ( vectors ))) ] ) imgs_filtered = [] for filt in filters : imgs_filtered . append ( oaconvolve ( img , filt , mode = \"same\" )) window = np . ones (( window_size , window_size )) imgs_energy = [] for img_filtered in imgs_filtered : imgs_energy . append ( oaconvolve ( np . abs ( img_filtered ), window , mode = \"same\" )) def _get_img_energy ( name ): return imgs_energy [ np . where ( filters_name == name )[ 0 ] . item ()] if only_s5 : imgs_feats = [ _get_img_energy ( \"S5S5\" )] else : imgs_feats = [ np . mean ( np . array ([ _get_img_energy ( \"L5E5\" ), _get_img_energy ( \"E5L5\" )]), axis = 0 ), np . mean ( np . array ([ _get_img_energy ( \"L5R5\" ), _get_img_energy ( \"R5L5\" )]), axis = 0 ), np . mean ( np . array ([ _get_img_energy ( \"E5S5\" ), _get_img_energy ( \"S5E5\" )]), axis = 0 ), _get_img_energy ( \"S5S5\" ), _get_img_energy ( \"R5R5\" ), np . mean ( np . array ([ _get_img_energy ( \"L5S5\" ), _get_img_energy ( \"S5L5\" )]), axis = 0 ), _get_img_energy ( \"E5E5\" ), np . mean ( np . array ([ _get_img_energy ( \"E5R5\" ), _get_img_energy ( \"R5E5\" )]), axis = 0 ), np . mean ( np . array ([ _get_img_energy ( \"S5R5\" ), _get_img_energy ( \"R5S5\" )]), axis = 0 ), ] return imgs_feats","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;compute_law_feats"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.contour_mask","text":"Find the contours in a binary image with opencv findContours. Parameters: Name Type Description Default mask ndarray A binary image. required Returns: Type Description list [ ndarray ] A list of contours found in the input image with opencv findContours. Source code in src/prismtoolbox/wsicore/core_utils.py 201 202 203 204 205 206 207 208 209 210 211 212 def contour_mask ( mask : np . ndarray ) -> list [ np . ndarray ]: \"\"\"Find the contours in a binary image with opencv findContours. Args: mask: A binary image. Returns: A list of contours found in the input image with opencv findContours. \"\"\" contours = cv2 . findContours ( mask , cv2 . RETR_TREE , cv2 . CHAIN_APPROX_SIMPLE ) contours = contours [ 0 ] if len ( contours ) == 2 else contours [ 1 ] return [ np . array ( contour ) for contour in contours ]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;contour_mask"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.floodfill_img","text":"Perform the floodfill algorithm on a binary image. Parameters: Name Type Description Default img ndarray A binary image. required start tuple [ int , int ] coordinates of the starting point for the floodfill algorithm. required Returns: Type Description ndarray The input image after applying the floodfill algorithm. Source code in src/prismtoolbox/wsicore/core_utils.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def floodfill_img ( img : np . ndarray , start : tuple [ int , int ]) -> np . ndarray : \"\"\"Perform the floodfill algorithm on a binary image. Args: img: A binary image. start: coordinates of the starting point for the floodfill algorithm. Returns: The input image after applying the floodfill algorithm. \"\"\" img_to_floodfill = img . copy () h , w = img . shape [: 2 ] mask = np . zeros (( h + 2 , w + 2 ), np . uint8 ) cv2 . floodFill ( img_to_floodfill , mask , start , 255 ) # type: ignore im_floodfill_inv = cv2 . bitwise_not ( img_to_floodfill ) img_out = img | im_floodfill_inv return img_out [ 1 : - 1 , 1 : - 1 ]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;floodfill_img"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.isBlackPatch","text":"Check if a patch is black. Parameters: Name Type Description Default patch Image An input patch. required rgb_thresh int The threshold value for the RGB channels to be considered black. 20 percentage float The percentage of pixels below the threshold to consider the patch as black. 0.05 Returns: Type Description bool True if the patch is black, False otherwise. Source code in src/prismtoolbox/wsicore/core_utils.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def isBlackPatch ( patch : Image . Image , rgb_thresh : int = 20 , percentage : float = 0.05 ) -> bool : \"\"\"Check if a patch is black. Args: patch: An input patch. rgb_thresh: The threshold value for the RGB channels to be considered black. percentage: The percentage of pixels below the threshold to consider the patch as black. Returns: True if the patch is black, False otherwise. \"\"\" num_pixels = patch . size [ 0 ] * patch . size [ 1 ] return ( True if np . all ( np . array ( patch ) < rgb_thresh , axis = 2 ) . sum () > num_pixels * percentage else False )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;isBlackPatch"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.isWhitePatch","text":"Check if a patch is white. Parameters: Name Type Description Default patch Image An input patch. required rgb_thresh int The threshold value for the RGB channels to be considered white. 220 percentage float The percentage of pixels above the threshold to consider the patch as white. 0.2 Returns: Type Description bool True if the patch is white, False otherwise. Source code in src/prismtoolbox/wsicore/core_utils.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def isWhitePatch ( patch : Image . Image , rgb_thresh : int = 220 , percentage : float = 0.2 ) -> bool : \"\"\"Check if a patch is white. Args: patch: An input patch. rgb_thresh: The threshold value for the RGB channels to be considered white. percentage: The percentage of pixels above the threshold to consider the patch as white. Returns: True if the patch is white, False otherwise. \"\"\" num_pixels = patch . size [ 0 ] * patch . size [ 1 ] return ( True if np . all ( np . array ( patch ) > rgb_thresh , axis = 2 ) . sum () > num_pixels * percentage else False )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;isWhitePatch"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.local_average","text":"Perform local averaging on the image for a given window size. Parameters: Name Type Description Default img ndarray An input image. required window_size int The window size to use for local averaging. required Returns: Type Description ndarray Grayscale image minus the local average on a window of size window_size of the input image. Source code in src/prismtoolbox/wsicore/core_utils.py 70 71 72 73 74 75 76 77 78 79 80 81 82 def local_average ( img : np . ndarray , window_size : int ) -> np . ndarray : \"\"\"Perform local averaging on the image for a given window size. Args: img: An input image. window_size: The window size to use for local averaging. Returns: Grayscale image minus the local average on a window of size window_size of the input image. \"\"\" window = np . ones (( window_size , window_size )) / ( window_size ** 2 ) img_grayscaled = cv2 . cvtColor ( img , cv2 . COLOR_RGB2GRAY ) return img_grayscaled - oaconvolve ( img_grayscaled , window , mode = \"same\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;local_average"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.save_patches_with_hdf5","text":"Save patches to an HDF5 file. Parameters: Name Type Description Default output_path str The path to the output HDF5 file. required asset_dict dict [ str , ndarray ] A dictionary of patches to save (key: name of the dataset, value: array of patches coordinates). required attr_dict dict [ str , dict [ str , str | int | tuple [ int , int ]]] | None A dictionary of attributes to save (key: name of the dataset, value: dictionary of attributes to None set to the HDF5 dataset). Source code in src/prismtoolbox/wsicore/core_utils.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def save_patches_with_hdf5 ( output_path : str , asset_dict : dict [ str , np . ndarray ], attr_dict : dict [ str , dict [ str , str | int | tuple [ int , int ]]] | None = None , ) -> None : \"\"\"Save patches to an HDF5 file. Args: output_path: The path to the output HDF5 file. asset_dict: A dictionary of patches to save (key: name of the dataset, value: array of patches coordinates). attr_dict: A dictionary of attributes to save (key: name of the dataset, value: dictionary of attributes to set to the HDF5 dataset). \"\"\" file = h5py . File ( output_path , \"w\" ) for key , val in asset_dict . items (): data_shape = val . shape if key not in file : data_type = val . dtype chunk_shape = ( 1 ,) + data_shape [ 1 :] maxshape = ( None ,) + data_shape [ 1 :] dset = file . create_dataset ( key , shape = data_shape , maxshape = maxshape , chunks = chunk_shape , dtype = data_type , ) dset [:] = val if attr_dict is not None : if key in attr_dict . keys (): for attr_key , attr_val in attr_dict [ key ] . items (): dset . attrs [ attr_key ] = attr_val else : dset = file [ key ] assert isinstance ( dset , h5py . Dataset ), f \"Expected dataset, got { type ( dset ) } for key { key } .\" dset . resize ( len ( dset ) + data_shape [ 0 ], axis = 0 ) dset [ - data_shape [ 0 ] :] = val file . close ()","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;save_patches_with_hdf5"},{"location":"reference/prismtoolbox/wsicore/core_utils/#prismtoolbox.wsicore.core_utils.select_roi_on_thumbnail","text":"Select a region of interest on the thumbnail of the slide using an interactive window. Parameters: Name Type Description Default img ndarray A thumbnail of the slide. required scale_factor int The scale factor to apply to convert the coordinates to the original slide dimensions. required Returns: Type Description ndarray The coordinates of the selected region of interest. Source code in src/prismtoolbox/wsicore/core_utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def select_roi_on_thumbnail ( img : np . ndarray , scale_factor : int ) -> np . ndarray : \"\"\"Select a region of interest on the thumbnail of the slide using an interactive window. Args: img: A thumbnail of the slide. scale_factor: The scale factor to apply to convert the coordinates to the original slide dimensions. Returns: The coordinates of the selected region of interest. \"\"\" drawing = False roi = [ 0 , 0 , 0 , 0 ] def _draw_rectangle ( event , x , y , flags , param ): nonlocal roi , drawing if event == cv2 . EVENT_LBUTTONDOWN : drawing = True roi [ 0 ], roi [ 1 ] = x , y elif event == cv2 . EVENT_MOUSEMOVE : if drawing : roi [ 2 ], roi [ 3 ] = x , y elif event == cv2 . EVENT_LBUTTONUP : drawing = False roi [ 2 ], roi [ 3 ] = x , y window_name = \"Select ROI (beginning by the top left corner) - press ESC to exit\" cv2 . namedWindow ( window_name , cv2 . WINDOW_NORMAL ) cv2 . resizeWindow ( window_name , 800 , 800 ) cv2 . setMouseCallback ( window_name , _draw_rectangle ) while True : temp_img = img . copy () if roi [ 2 ] and roi [ 3 ]: cv2 . rectangle ( temp_img , ( roi [ 0 ], roi [ 1 ]), ( roi [ 2 ], roi [ 3 ]), ( 0 , 255 , 0 ), 5 ) width = ( roi [ 2 ] - roi [ 0 ]) * scale_factor height = ( roi [ 3 ] - roi [ 1 ]) * scale_factor cv2 . putText ( temp_img , f \"Width: { width } - Height: { height } \" , ( roi [ 0 ], roi [ 1 ] - 10 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 0 , 255 , 0 ), 2 , ) cv2 . imshow ( window_name , temp_img ) k = cv2 . waitKey ( 1 ) & 0xFF if k == 27 : break cv2 . destroyAllWindows () return np . array ( roi )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;select_roi_on_thumbnail"},{"location":"reference/prismtoolbox/wsicore/wsi/","text":"WSI ( slide_path , engine = 'openslide' ) The WSI (Whole Slide Image) class is responsible for handling operations related to whole slide images. Parameters: Name Type Description Default slide_path str The path to the slide image file. required engine str The engine used to read the slide image. 'openslide' Attributes: Name Type Description slide_path str The path to the slide image file. engine str The engine used to read the slide image. slide_name str The name of the slide image file. Retrieved from the slide path using retrieve_slide_name_ext method. slide_ext str The extension of the slide image file. Retrieved from the slide path using retrieve_slide_name_ext method. slide OpenSlide | TiffSlide The wsi read from the file using engine. dimensions list [ tuple [ int , int ]] The dimensions of the slide image. Set by the set_slide_attributes method. level_dimensions list [ tuple [ int , int ]] The dimensions of the different levels of the slide image. Set by the set_slide_attributes method. level_downsamples list [ tuple [ int , int ]] The downsampling factors of the different levels of the slide image. Set by the set_slide_attributes method. properties dict The properties of the slide image. Set by the set_slide_attributes method. offset tuple [ int , int ] The offset of the slide image. Set by the set_slide_attributes method. ROI ndarray | None The region of interest in the slide image. Please use the set_roi method to set the ROI. ROI_width int | None The width of the region of interest. Set by the set_roi method. ROI_height int | None The height of the region of interest. Set by the set_roi method. tissue_contours list [ ndarray ] | None The contours of the tissue in the slide image. Please use the detect_tissue method to detect the tissue contours. coords ndarray | None The coordinates of patches extracted from slide image. Please use the extract_patches method to extract patches. coords_attrs dict | None The attributes of the coordinates. Set by the extract_patches method. Source code in src/prismtoolbox/wsicore/wsi.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , slide_path : str , engine : str = \"openslide\" ): \"\"\"The WSI (Whole Slide Image) class is responsible for handling operations related to whole slide images. Args: slide_path: The path to the slide image file. engine: The engine used to read the slide image. Attributes: slide_path (str): The path to the slide image file. engine (str): The engine used to read the slide image. slide_name (str): The name of the slide image file. Retrieved from the slide path using [retrieve_slide_name_ext][prismtoolbox.wsicore.WSI.retrieve_slide_name_ext] method. slide_ext (str): The extension of the slide image file. Retrieved from the slide path using [retrieve_slide_name_ext][prismtoolbox.wsicore.WSI.retrieve_slide_name_ext] method. slide (OpenSlide | TiffSlide): The wsi read from the file using engine. dimensions (list[tuple[int, int]]): The dimensions of the slide image. Set by the set_slide_attributes method. level_dimensions (list[tuple[int, int]]): The dimensions of the different levels of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. level_downsamples (list[tuple[int, int]]): The downsampling factors of the different levels of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. properties (dict): The properties of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. offset (tuple[int, int]): The offset of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. ROI (ndarray | None): The region of interest in the slide image. Please use the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method to set the ROI. ROI_width (int | None): The width of the region of interest. Set by the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method. ROI_height (int | None): The height of the region of interest. Set by the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method. tissue_contours (list[ndarray] | None): The contours of the tissue in the slide image. Please use the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method to detect the tissue contours. coords (np.ndarray | None): The coordinates of patches extracted from slide image. Please use the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method to extract patches. coords_attrs (dict | None): The attributes of the coordinates. Set by the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method. \"\"\" self . slide_path = slide_path self . engine = engine self . slide_name , self . slide_ext = self . retrieve_slide_name_ext ( self . slide_path ) self . slide = self . read ( slide_path , engine ) self . offset = ( 0 , 0 ) self . set_slide_attributes () self . ROI = None self . ROI_width = None self . ROI_height = None self . tissue_contours = None self . coords = None self . coords_attrs = None apply_pathologist_annotations ( path , class_name = 'annotation' , column_to_select = 'objectType' ) Apply pathologist annotations to the tissue contours by intersecting the annotations with the tissue contours. Requires the tissue contours to be set for the slide beforehand with the detect_tissue method. Parameters: Name Type Description Default path str The path to the .geojson file containing the annotations extracted from QuPath. required class_name str The class name to use for selecting the annotations to apply. 'annotation' column_to_select str The column to select in the GeoDataFrame. 'objectType' Source code in src/prismtoolbox/wsicore/wsi.py 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 def apply_pathologist_annotations ( self , path : str , class_name : str = \"annotation\" , column_to_select : str = \"objectType\" , ) -> None : \"\"\"Apply pathologist annotations to the tissue contours by intersecting the annotations with the tissue contours. Requires the tissue contours to be set for the slide beforehand with the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method. Args: path: The path to the .geojson file containing the annotations extracted from QuPath. class_name: The class name to use for selecting the annotations to apply. column_to_select: The column to select in the GeoDataFrame. \"\"\" assert ( self . tissue_contours is not None ), \"No tissue contours found for the slide, please run the detect_tissue method first\" offset = ( - self . offset [ 0 ], - self . offset [ 1 ]) pathologist_annotations = read_qupath_annotations ( path , offset = offset , class_name = class_name , column_to_select = column_to_select ) polygons = contoursToPolygons ( self . tissue_contours , make_valid = True ) intersection = intersectionPolygons ( polygons , pathologist_annotations ) self . tissue_contours = PolygonsToContours ( intersection ) convert_micrometer_to_pixel ( value , level , axis = 'x' ) Convert a value from micrometer to pixel. Parameters: Name Type Description Default value float The value to convert (in micrometer). required level int The level at which the conversion should be performed. required axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description int The input value in pixel. Source code in src/prismtoolbox/wsicore/wsi.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def convert_micrometer_to_pixel ( self , value : float , level : int , axis : str = \"x\" , ) -> int : \"\"\"Convert a value from micrometer to pixel. Args: value: The value to convert (in micrometer). level: The level at which the conversion should be performed. axis: The axis to use for getting the conversion factor (x or y). Returns: The input value in pixel. \"\"\" return int ( value / float ( self . properties [ f \" { self . engine } .mpp- { axis } \" ])) // int ( self . level_downsamples [ level ] ) convert_pixel_to_micrometer ( value , level , axis = 'x' ) Convert a value from pixel to micrometer. Parameters: Name Type Description Default value float The value to convert (in pixel). required level int The level at which the conversion should be performed. required axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description float The input value in micrometer. Source code in src/prismtoolbox/wsicore/wsi.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def convert_pixel_to_micrometer ( self , value : float , level : int , axis : str = \"x\" , ) -> float : \"\"\"Convert a value from pixel to micrometer. Args: value: The value to convert (in pixel). level: The level at which the conversion should be performed. axis: The axis to use for getting the conversion factor (x or y). Returns: The input value in micrometer. \"\"\" return ( value * float ( self . properties [ f \" { self . engine } .mpp- { axis } \" ]) * self . level_downsamples [ level ] ) convert_units ( value , level , from_unit , to_unit = 'px' , axis = 'x' ) Convert a value from one unit to another. Parameters: Name Type Description Default value float The value to convert. required level int The level at which the conversion should be performed. required from_unit str The unit to convert from (px or micro). required to_unit str The unit to convert to (px or micro). 'px' axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description int | float The input value converted in the desired unit. Source code in src/prismtoolbox/wsicore/wsi.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 def convert_units ( self , value : float , level : int , from_unit : str , to_unit : str = \"px\" , axis : str = \"x\" , ) -> int | float : \"\"\"Convert a value from one unit to another. Args: value: The value to convert. level: The level at which the conversion should be performed. from_unit: The unit to convert from (px or micro). to_unit: The unit to convert to (px or micro). axis: The axis to use for getting the conversion factor (x or y). Returns: The input value converted in the desired unit. \"\"\" if from_unit == \"micro\" and to_unit == \"px\" : value = self . convert_micrometer_to_pixel ( value , level , axis ) elif from_unit == \"px\" and to_unit == \"micro\" : value = self . convert_pixel_to_micrometer ( value , level , axis ) elif from_unit == to_unit : pass else : raise ValueError ( f \"Conversion from { from_unit } to { to_unit } not supported.\" ) return value create_thumbnail ( level , crop_roi = False ) Create a thumbnail of the slide at a given level. Parameters: Name Type Description Default level int The level at which the thumbnail should be created. required crop_roi bool A boolean to crop the thumbnail to the region of interest defined for the slide (requires a ROI to be set for the slide beforehand with the set_roi method) False Returns: Type Description Image A thumbnail of the slide as a PIL image. Source code in src/prismtoolbox/wsicore/wsi.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 def create_thumbnail ( self , level : int , crop_roi : bool = False , ) -> Image . Image : \"\"\"Create a thumbnail of the slide at a given level. Args: level: The level at which the thumbnail should be created. crop_roi: A boolean to crop the thumbnail to the region of interest defined for the slide (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method) Returns: A thumbnail of the slide as a PIL image. \"\"\" thumb = self . read_region (( 0 , 0 ), level , self . level_dimensions [ level ]) . convert ( \"RGB\" ) if crop_roi : if self . ROI is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) log . info ( f \"Creating thumbnail with ROI { self . ROI } .\" ) coords_roi = ( self . ROI / self . level_downsamples [ level ]) . astype ( int ) thumb = thumb . crop ( tuple ( coords_roi )) return thumb detect_tissue ( seg_level , window_avg , window_eng , thresh , inside_roi = False , inv_thresh = False , area_min = 200000.0 , start = ( 0 , 0 )) Segment the tissue on the slide based on a threshold on the Law's texture energy spot map and floodfill algorithm to fill the holes in the mask. The tissue contours are stored in the tissue_contours attribute. Parameters: Name Type Description Default seg_level int The level at which the segmentation should be performed. required window_avg int The size of the window for local averaging. required window_eng int The size of the window for Law's texture energy computation. required thresh int The threshold for binarization on the Law's texture energy spot map. required inside_roi bool Set to True to identify the tissue only within a ROI (requires a ROI to be set for the slide beforehand with the set_roi method). False inv_thresh bool Set to True to invert the thresholding. False area_min float The minimum area for a contour to be kept. 200000.0 start tuple [ int , int ] The starting point for the floodfill algorithm (should be left at (0, 0) in most cases). (0, 0) Source code in src/prismtoolbox/wsicore/wsi.py 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def detect_tissue ( self , seg_level : int , window_avg : int , window_eng : int , thresh : int , inside_roi : bool = False , inv_thresh : bool = False , area_min : float = 2e5 , start : tuple [ int , int ] = ( 0 , 0 ), ) -> None : \"\"\"Segment the tissue on the slide based on a threshold on the Law's texture energy spot map and floodfill algorithm to fill the holes in the mask. The tissue contours are stored in the tissue_contours attribute. Args: seg_level: The level at which the segmentation should be performed. window_avg: The size of the window for local averaging. window_eng: The size of the window for Law's texture energy computation. thresh: The threshold for binarization on the Law's texture energy spot map. inside_roi: Set to True to identify the tissue only within a ROI (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). inv_thresh: Set to True to invert the thresholding. area_min: The minimum area for a contour to be kept. start: The starting point for the floodfill algorithm (should be left at (0, 0) in most cases). \"\"\" final_contours = [] img = np . array ( self . create_thumbnail ( seg_level , inside_roi )) img_avg = local_average ( np . asarray ( img ), window_avg ) law_feats = compute_law_feats ( img_avg , window_eng )[ 0 ] filterred_img = apply_bilateral_filter ( np . clip ( law_feats , 0 , 255 ) . astype ( \"uint8\" )) threshed_img = apply_binary_thresh ( filterred_img , thresh , inv_thresh ) flooded_img = floodfill_img ( np . pad ( threshed_img , 1 ), start ) contours = contour_mask ( flooded_img ) for contour in contours : c = contour . copy () area = cv2 . contourArea ( c ) if area > area_min : final_contours . append ( contour ) if len ( final_contours ) == 0 : self . tissue_contours = [] log . warning ( f \"No tissue contours found for the slide { self . slide_name } .\" ) else : scale = self . level_downsamples [ seg_level ] offset = np . array ( self . ROI [: 2 ]) if self . ROI is not None else np . array ([ 0 , 0 ]) final_contours = self . scale_contours ( final_contours , scale ) final_contours = [ cont + offset for cont in final_contours ] # Sanity check to ensure that the contours are all within a ROI is provided if self . ROI is not None : assert all ( [ np . all ( cont >= self . ROI [: 2 ]) and np . all ( cont <= self . ROI [ 2 :]) for cont in final_contours ] ) self . tissue_contours = final_contours log . info ( f \"Identified { len ( final_contours ) } contours for the slide { self . slide_name } .\" ) return extract_patches ( patch_size , patch_level , mode , step_size = None , overlap = None , units = ( 'px' , 'px' ), use_padding = True , contours_mode = None , rgb_threshs = ( 2 , 220 ), percentages = ( 0.6 , 0.9 )) Extract valid patches from the slide with different extraction modes. A patch is considered valid if it is not black or white and is within the region of interest or the tissue contours if relevant. The extracted patches are stored as coordinates in the coords attribute, and the attributes of the coordinates are stored in the coords_attrs attribute. Parameters: Name Type Description Default patch_size float The size of the patches to extract (assumed to be square). required patch_level int The level at which the patches should be extracted. required mode str The mode to use for the extraction: \"contours\" mode extracts patches within the tissue contours (requires the tissue contours to be set for the slide beforehand with the detect_tissue method). \"roi\" mode extracts patches within the region of interest (requires the ROI to be set for the slide beforehand with the set_roi method). \"all\" mode extracts patches from the entire slide required step_size float | None The step size for the sliding window (if set to None, the step size will be computed based on the overlap). None overlap float | None The overlap between patches as an absolute value (must be provided if step_size is set to None). None units tuple [ str , str ] The units for the patch size and step size/overlap values (pixels: px, micrometers: micro). ('px', 'px') use_padding bool Set to True to use padding for the extraction. True contours_mode str | None The mode to use for the contour checking function (must be provided if mode is set to contours). See IsInContour for more details. None rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Source code in src/prismtoolbox/wsicore/wsi.py 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 def extract_patches ( self , patch_size : float , patch_level : int , mode : str , step_size : float | None = None , overlap : float | None = None , units : tuple [ str , str ] = ( \"px\" , \"px\" ), use_padding : bool = True , contours_mode : str | None = None , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> None : \"\"\"Extract valid patches from the slide with different extraction modes. A patch is considered valid if it is not black or white and is within the region of interest or the tissue contours if relevant. The extracted patches are stored as coordinates in the coords attribute, and the attributes of the coordinates are stored in the coords_attrs attribute. Args: patch_size: The size of the patches to extract (assumed to be square). patch_level: The level at which the patches should be extracted. mode: The mode to use for the extraction: - \"contours\" mode extracts patches within the tissue contours (requires the tissue contours to be set for the slide beforehand with the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method). - \"roi\" mode extracts patches within the region of interest (requires the ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). - \"all\" mode extracts patches from the entire slide step_size: The step size for the sliding window (if set to None, the step size will be computed based on the overlap). overlap: The overlap between patches as an absolute value (must be provided if step_size is set to None). units: The units for the patch size and step size/overlap values (pixels: px, micrometers: micro). use_padding: Set to True to use padding for the extraction. contours_mode: The mode to use for the contour checking function (must be provided if mode is set to contours). See [IsInContour][prismtoolbox.wsicore.core_utils.IsInContour] for more details. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. \"\"\" assert all ( [ unit in [ \"micro\" , \"px\" ] for unit in units ] ), \"Units must be either 'micro' or 'px'\" patch_size = int ( self . convert_units ( patch_size , patch_level , units [ 0 ])) if step_size is None : if overlap is None : raise ValueError ( \"Either step_size or overlap must be provided if step_size is not set\" ) step_size = int ( patch_size - self . convert_units ( overlap , patch_level , units [ 1 ])) else : step_size = int ( self . convert_units ( step_size , patch_level , units [ 1 ])) log . info ( f \"Extracting patches of size { patch_size } at level { patch_level } with step size { step_size } .\" ) if mode == \"contours\" : log . info ( \"Extracting patches with 'contours' mode.\" ) assert self . tissue_contours is not None , \"Empty tissue contours vector for the slide, please run the detect_tissue method first.\" assert len ( self . tissue_contours ) > 0 , \"No tissue contours found for the slide.\" assert contours_mode is not None , \"Contours mode must be provided if mode is set to 'contours'.\" valid_coords = [] for cont in self . tissue_contours : roi_dim = cv2 . boundingRect ( cont ) # type: ignore log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords . extend ( self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , cont , contours_mode , rgb_threshs = rgb_threshs , percentages = percentages , ) ) valid_coords = np . array ( valid_coords ) elif mode == \"roi\" : log . info ( \"Extracting patches with 'roi' mode.\" ) if self . ROI is None or self . ROI_width is None or self . ROI_height is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) roi_dim = self . ROI [ 0 ], self . ROI [ 1 ], self . ROI_width , self . ROI_height log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords = self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , rgb_threshs = rgb_threshs , percentages = percentages , ) elif mode == \"all\" : roi_dim = 0 , 0 , self . level_dimensions [ 0 ][ 0 ], self . level_dimensions [ 0 ][ 1 ] log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords = self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , rgb_threshs = rgb_threshs , percentages = percentages , ) else : raise ValueError ( f \"Mode { mode } not supported\" ) attr = { \"patch_size\" : patch_size , \"patch_level\" : patch_level , \"downsample\" : self . level_downsamples [ patch_level ], \"downsampled_level_dim\" : tuple ( np . array ( self . level_dimensions [ patch_level ])), \"level_dim\" : self . level_dimensions [ patch_level ], \"name\" : self . slide_name , } if len ( valid_coords ) == 0 : log . warning ( f \"No valid coordinates found for the slide { self . slide_name } .\" ) else : log . info ( f \"Identified a total of { len ( valid_coords ) } valid coordinates for the slide { self . slide_name } .\" ) self . coords = valid_coords self . coords_attrs = attr extract_patches_roi ( patch_level , patch_size , step_size = None , roi_dim = None , use_padding = True , contour = None , contours_mode = None , coord_candidates = None , rgb_threshs = ( 2 , 220 ), percentages = ( 0.6 , 0.9 ), return_indices = False ) Extract valid patches from a region of interest, i.e if the patch is not black or white and is within the region of interest/contours if relevant). Parameters: Name Type Description Default patch_level int The level at which the patches should be extracted. required patch_size int The size of the patches to extract (assumed to be square). required step_size int | None The step size to use for the sliding window. None roi_dim tuple [ int , int , int , int ] | None The top-left corner coordinates and dimensions of the region of interest. Must be provided if coord_candidates is set to None. None use_padding bool Set to True to use padding for the extraction. True contour ndarray | None The tissue contour to use for the extraction. If set to None, will not check if patches are within a contour. None contours_mode str | None The mode for the contour checking function. See IsInContour for more details. Must be provided if mode is set to contours. Otherwise, will not check if patches are within the contours. None coord_candidates ndarray | None Precomputed candidate coordinates for the patches. If set to None, will compute the candidate coordinates based on the ROI dimensions and the step size. None rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) return_indices bool Set to True to return the indices of the valid coordinates. False Returns: Type Description ndarray | tuple [ ndarray , list [ int ]] An array of valid coordinates for the patches (i.e. coordinates of the top-left corner of the patches). Source code in src/prismtoolbox/wsicore/wsi.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 def extract_patches_roi ( self , patch_level : int , patch_size : int , step_size : int | None = None , roi_dim : tuple [ int , int , int , int ] | None = None , use_padding : bool = True , contour : np . ndarray | None = None , contours_mode : str | None = None , coord_candidates : np . ndarray | None = None , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), return_indices : bool = False , ) -> np . ndarray | tuple [ np . ndarray , list [ int ]]: \"\"\"Extract valid patches from a region of interest, i.e if the patch is not black or white and is within the region of interest/contours if relevant). Args: patch_level: The level at which the patches should be extracted. patch_size: The size of the patches to extract (assumed to be square). step_size: The step size to use for the sliding window. roi_dim: The top-left corner coordinates and dimensions of the region of interest. Must be provided if coord_candidates is set to None. use_padding: Set to True to use padding for the extraction. contour: The tissue contour to use for the extraction. If set to None, will not check if patches are within a contour. contours_mode: The mode for the contour checking function. See [IsInContour][prismtoolbox.wsicore.core_utils.IsInContour] for more details. Must be provided if mode is set to contours. Otherwise, will not check if patches are within the contours. coord_candidates: Precomputed candidate coordinates for the patches. If set to None, will compute the candidate coordinates based on the ROI dimensions and the step size. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. return_indices: Set to True to return the indices of the valid coordinates. Returns: An array of valid coordinates for the patches (i.e. coordinates of the top-left corner of the patches). \"\"\" patch_downsample = int ( self . level_downsamples [ patch_level ]) ref_patch_size = patch_size * patch_downsample if coord_candidates is None : if roi_dim is None or step_size is None : raise ValueError ( \"roi_dim and step_size must be provided if coord_candidates is not set.\" ) start_x , start_y , w , h = roi_dim img_w , img_h = self . level_dimensions [ 0 ] if use_padding : stop_y = start_y + h stop_x = start_x + w else : stop_y = min ( start_y + h , img_h - ref_patch_size + 1 ) stop_x = min ( start_x + w , img_w - ref_patch_size + 1 ) step_size = step_size * patch_downsample x_range = np . arange ( start_x , stop_x , step = step_size ) y_range = np . arange ( start_y , stop_y , step = step_size ) x_coords , y_coords = np . meshgrid ( x_range , y_range , indexing = \"ij\" ) coord_candidates = np . array ( [ x_coords . flatten (), y_coords . flatten ()] ) . transpose () if contour is not None : if contours_mode is None : raise ValueError ( \"A contour mode must be provided if patch extraction mode is set to contours.\" ) cont_check_fn = IsInContour ( contour , patch_size = ref_patch_size , center_shift = 0.5 , mode = contours_mode ) log . info ( f \"Extracting patches with contour checking function mode { contours_mode } .\" ) else : cont_check_fn = None num_workers = mp . cpu_count () pool = mp . Pool ( num_workers , initializer = WSI . worker_init , initargs = ( self . slide_path , self . engine , ), ) iterable = [ ( coord , cont_check_fn , patch_level , patch_size , rgb_threshs , percentages ) for coord in coord_candidates ] valid_coords = pool . starmap ( WSI . process_coord_candidate , iterable ) pool . close () valid_indices = [ i for i , coord in enumerate ( valid_coords ) if coord is not None ] valid_coords = np . array ([ coord_candidates [ i ] for i in valid_indices ]) log . info ( f \"Identified { len ( valid_coords ) } valid coordinates in the ROI { roi_dim } .\" ) if return_indices : return valid_coords , valid_indices else : return valid_coords is_black_white ( patch , rgb_threshs = ( 2 , 220 ), percentages = ( 0.6 , 0.9 )) staticmethod Check if a patch is black or white. Parameters: Name Type Description Default patch Image The input patch. required rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Returns: Type Description bool True if the patch is black or white, False otherwise. Source code in src/prismtoolbox/wsicore/wsi.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 @staticmethod def is_black_white ( patch : Image . Image , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> bool : \"\"\"Check if a patch is black or white. Args: patch: The input patch. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. Returns: True if the patch is black or white, False otherwise. \"\"\" return isBlackPatch ( patch , rgb_thresh = rgb_threshs [ 0 ], percentage = percentages [ 0 ] ) or isWhitePatch ( patch , rgb_thresh = rgb_threshs [ 1 ], percentage = percentages [ 1 ]) load_patches ( file_path ) Load the patches from a hdf5 file. Parameters: Name Type Description Default file_path str The path to the hdf5 file containing the patches. required Source code in src/prismtoolbox/wsicore/wsi.py 462 463 464 465 466 467 468 469 def load_patches ( self , file_path : str ) -> None : \"\"\"Load the patches from a hdf5 file. Args: file_path: The path to the hdf5 file containing the patches. \"\"\" log . info ( f \"Loading patches for slide { self . slide_name } from { file_path } .\" ) self . coords , self . coords_attrs = read_h5_file ( file_path , \"coords\" ) load_tissue_contours ( file_path ) Load the tissue contours from a pickle file. Parameters: Name Type Description Default file_path str The path to the pickle file containing the tissue contours. required Source code in src/prismtoolbox/wsicore/wsi.py 370 371 372 373 374 375 376 377 def load_tissue_contours ( self , file_path : str ) -> None : \"\"\"Load the tissue contours from a pickle file. Args: file_path: The path to the pickle file containing the tissue contours. \"\"\" log . info ( f \"Loading tissue contours for slide { self . slide_name } from { file_path } .\" ) self . tissue_contours = load_obj_with_pickle ( file_path ) process_coord_candidate ( coord , cont_check_fn , patch_level , patch_size , rgb_threshs = ( 2 , 220 ), percentages = ( 0.6 , 0.9 )) staticmethod Determine if a candidate coordinate is valid based on a contour checking function and/or black/white thresholding. Parameters: Name Type Description Default coord tuple [ int , int ] The candidate coordinate. required cont_check_fn IsInContour | None The contour checking function. required patch_level int The level at which the patch should be extracted. required patch_size int The size of the patch (assumed to be square). required rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Returns: Type Description tuple [ int , int ] | None The coordinate if it is valid, None otherwise. Source code in src/prismtoolbox/wsicore/wsi.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 @staticmethod def process_coord_candidate ( coord : tuple [ int , int ], cont_check_fn : IsInContour | None , patch_level : int , patch_size : int , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> tuple [ int , int ] | None : \"\"\"Determine if a candidate coordinate is valid based on a contour checking function and/or black/white thresholding. Args: coord: The candidate coordinate. cont_check_fn: The contour checking function. patch_level: The level at which the patch should be extracted. patch_size: The size of the patch (assumed to be square). rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. Returns: The coordinate if it is valid, None otherwise. \"\"\" if cont_check_fn is None or cont_check_fn ( coord ): patch = wsi . read_region ( coord , patch_level , ( patch_size , patch_size ) ) . convert ( \"RGB\" ) if not WSI . is_black_white ( patch , rgb_threshs , percentages ): return coord else : return None else : return None read ( slide_path , engine ) staticmethod Read a slide with a given engine. Parameters: Name Type Description Default slide_path str The path to the slide. required engine str The backend library to use for reading the slide (currently only openslide and tiffslide are supported). required Returns: Type Description A slide object. Source code in src/prismtoolbox/wsicore/wsi.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 @staticmethod def read ( slide_path : str , engine : str ): \"\"\"Read a slide with a given engine. Args: slide_path: The path to the slide. engine: The backend library to use for reading the slide (currently only openslide and tiffslide are supported). Returns: A slide object. \"\"\" if engine == \"openslide\" : import openslide slide = openslide . OpenSlide ( slide_path ) elif engine == \"tiffslide\" : import tiffslide slide = tiffslide . TiffSlide ( slide_path ) else : raise NotImplementedError ( f \"engine { engine } not supported\" ) return slide read_region ( location , level , size ) Read a region from the slide for a given level and size. Parameters: Name Type Description Default location tuple [ int , int ] The coordinates of the top left corner of the region (in pixels). required level int The level at which the region should be read. required size tuple [ int , int ] The size of the region (in pixels). required Returns: Type Description Image The desired region of the slide as a PIL image. Source code in src/prismtoolbox/wsicore/wsi.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def read_region ( self , location : tuple [ int , int ], level : int , size : tuple [ int , int ], ) -> Image . Image : \"\"\"Read a region from the slide for a given level and size. Args: location: The coordinates of the top left corner of the region (in pixels). level: The level at which the region should be read. size: The size of the region (in pixels). Returns: The desired region of the slide as a PIL image. \"\"\" return self . slide . read_region ( location , level , size ) . convert ( \"RGB\" ) retrieve_slide_name_ext ( slide_path ) staticmethod Retrieves slide name and slide extension from slide path. Parameters: Name Type Description Default slide_path str The path to the slide. required Returns: Type Description tuple [ str , str ] A tuple (slide name, slide ext). Source code in src/prismtoolbox/wsicore/wsi.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 @staticmethod def retrieve_slide_name_ext ( slide_path : str ) -> tuple [ str , str ]: \"\"\"Retrieves slide name and slide extension from slide path. Args: slide_path: The path to the slide. Returns: A tuple (slide name, slide ext). \"\"\" slide_name = re . search ( r \"([^/]+?)(?=\\.[\\w\\.]+$)\" , slide_path ) slide_ext = re . search ( r \"(?<=\\.)[\\w\\.]+$\" , slide_path ) if slide_ext is None or slide_name is None : raise ValueError ( f \"Could not retrieve slide name and extension from { slide_path } . \" \"Please check the file path.\" ) else : slide_name , slide_ext = slide_name . group ( 0 ), slide_ext . group ( 0 ) return slide_name , slide_ext save_patches ( save_dir , file_format = 'h5' , selected_idx = None , merge = False , label = None , color = ( 255 , 0 , 0 ), append_to_existing_file = False ) Save the patches in a hdf5 or geojson file. Parameters: Name Type Description Default save_dir str The path to the directory where the patches will be saved. required file_format str The format for the saving (h5 for python processing, geojson for QuPath processing). 'h5' selected_idx ndarray | None An array of indices of the patches to save (if set to None, all the patches will be saved). None merge bool Set to True to merge the patches into a single polygon (for geojson format only). False label str | None An optional label to assign to the patches (for geojson format only). None color tuple [ int , int , int ] An optional color to assign to the patches (for geojson format only). (255, 0, 0) append_to_existing_file bool Set to True to append the patches to an existing geojson file. False Source code in src/prismtoolbox/wsicore/wsi.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def save_patches ( self , save_dir : str , file_format : str = \"h5\" , selected_idx : np . ndarray | None = None , merge : bool = False , label : str | None = None , color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), append_to_existing_file : bool = False , ) -> None : \"\"\"Save the patches in a hdf5 or geojson file. Args: save_dir: The path to the directory where the patches will be saved. file_format: The format for the saving (h5 for python processing, geojson for QuPath processing). selected_idx: An array of indices of the patches to save (if set to None, all the patches will be saved). merge: Set to True to merge the patches into a single polygon (for geojson format only). label: An optional label to assign to the patches (for geojson format only). color: An optional color to assign to the patches (for geojson format only). append_to_existing_file: Set to True to append the patches to an existing geojson file. \"\"\" if self . coords is None : raise ValueError ( \"No patches found. Please check if patches were correctly extracted.\" ) if self . coords_attrs is None : raise ValueError ( \"No attributes set for the patches. \" \"Please check if patches were correctly extracted.\" ) if selected_idx is not None : coords = np . array ( self . coords [ selected_idx ]) else : coords = np . array ( self . coords ) if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if file_format == \"h5\" : asset_dict = { \"coords\" : coords } attr_dict = { \"coords\" : self . coords_attrs } file_path = os . path . join ( save_dir , f \" { self . slide_name } .h5\" ) log . info ( f \"Saving patches for slide { self . slide_name } at { file_path } as h5 file.\" ) save_patches_with_hdf5 ( file_path , asset_dict , attr_dict ) elif file_format == \"geojson\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .geojson\" ) patch_downsample = int ( self . level_downsamples [ self . coords_attrs [ \"patch_level\" ]] ) polygons = patchesToPolygons ( coords , self . coords_attrs [ \"patch_size\" ], patch_downsample , merge ) log . info ( f \"Saving { len ( coords ) } patches for slide { self . slide_name } at { file_path } as geojson file.\" ) export_polygons_to_qupath ( polygons , file_path , \"annotation\" , offset = self . offset , label = label , color = color , append_to_existing_file = append_to_existing_file , ) elif file_format == \"jpg\" or file_format == \"png\" : save_dir = os . path . join ( save_dir , self . slide_name ) if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) log . info ( f \"Saving { len ( coords ) } patches for slide { self . slide_name } at { save_dir } with { file_format } .\" ) for coord in coords : patch = self . read_region ( coord , self . coords_attrs [ \"patch_level\" ], ( self . coords_attrs [ \"patch_size\" ], self . coords_attrs [ \"patch_size\" ]), ) . convert ( \"RGB\" ) patch . save ( os . path . join ( save_dir , f \" { coord [ 0 ] } _ { coord [ 1 ] } . { file_format } \" )) else : raise ValueError ( f \"Format { file_format } not supported.\" ) save_tissue_contours ( save_dir , selected_idx = None , file_format = 'pickle' , merge = False , label = None , color = ( 255 , 0 , 0 ), append_to_existing_file = False , make_valid = False ) Save the tissue contours in a pickle or geojson file. Parameters: Name Type Description Default save_dir str The path to the directory where the contours will be saved. required selected_idx ndarray | None An array of indices of the contours to save (if set to None, all the contours will be saved). None file_format str The file format for saving the contours (pickle for python processing, geojson for QuPath processing). 'pickle' merge bool Set to True to merge the contours into a single polygon (for geojson format only). False label str | None An optional label to assign to the tissue contours (for geojson format only). None color tuple [ int , int , int ] An optional color to assign to the tissue contours (for geojson format only). (255, 0, 0) append_to_existing_file bool Set to True to append the contours to an existing geojson file. False make_valid bool Set to True to make the polygons valid (for geojson format only). False Source code in src/prismtoolbox/wsicore/wsi.py 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def save_tissue_contours ( self , save_dir : str , selected_idx : np . ndarray | None = None , file_format : str = \"pickle\" , merge : bool = False , label : str | None = None , color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), append_to_existing_file : bool = False , make_valid : bool = False , ) -> None : \"\"\"Save the tissue contours in a pickle or geojson file. Args: save_dir: The path to the directory where the contours will be saved. selected_idx: An array of indices of the contours to save (if set to None, all the contours will be saved). file_format: The file format for saving the contours (pickle for python processing, geojson for QuPath processing). merge: Set to True to merge the contours into a single polygon (for geojson format only). label: An optional label to assign to the tissue contours (for geojson format only). color: An optional color to assign to the tissue contours (for geojson format only). append_to_existing_file: Set to True to append the contours to an existing geojson file. make_valid: Set to True to make the polygons valid (for geojson format only). \"\"\" assert self . tissue_contours is not None , ( \"No tissue contours found for the slide, \" \"please run the detect_tissue method first\" ) if selected_idx is not None : tissue_contours = [ self . tissue_contours [ idx ] for idx in selected_idx ] else : tissue_contours = self . tissue_contours if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if file_format == \"pickle\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .pkl\" ) log . info ( f \"Saving tissue contours for slide { self . slide_name } at { file_path } as pickle file.\" ) save_obj_with_pickle ( tissue_contours , file_path ) elif file_format == \"geojson\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .geojson\" ) log . info ( f \"Saving { selected_idx } tissue contours for slide { self . slide_name } at { file_path } as geojson file.\" ) polygons = contoursToPolygons ( tissue_contours , merge , make_valid ) export_polygons_to_qupath ( polygons , file_path , \"annotation\" , offset = self . offset , label = label , color = color , append_to_existing_file = append_to_existing_file , ) else : raise ValueError ( f \"format { file_format } not supported\" ) scale_contours ( contours , scale ) staticmethod Scale the contours by a given factor. Parameters: Name Type Description Default contours list [ ndarray ] The contours to scale. required scale float The scale factor to apply. required Returns: Type Description list [ ndarray ] The input list with scaled contours. Source code in src/prismtoolbox/wsicore/wsi.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 @staticmethod def scale_contours ( contours : list [ np . ndarray ], scale : float , ) -> list [ np . ndarray ]: \"\"\"Scale the contours by a given factor. Args: contours: The contours to scale. scale: The scale factor to apply. Returns: The input list with scaled contours. \"\"\" scaled_contours = [ np . array ( cont * scale , dtype = \"int\" ) for cont in contours ] return scaled_contours set_roi ( roi = None , rois_df_path = None ) Set the region of interest for the slide. Can be set manually or by selecting a region on a thumbnail. The ROI is stored as a tuple in the ROI attribute. Parameters: Name Type Description Default roi tuple [ int , int , int , int ] | None Set the region of interest manually as a tuple (x1, y1, x2, y2). None rois_df_path str | None The path to dataframe containing the ROIs with a slide_id column identifying the slide, and the ROI coordinates as columns (x1, y1, x2, y2). If roi is not provided, the ROI will be set from this dataframe. None Returns: The region of interest set for the slide as a tuple (x1, y1, x2, y2). Source code in src/prismtoolbox/wsicore/wsi.py 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def set_roi ( self , roi : tuple [ int , int , int , int ] | None = None , rois_df_path : str | None = None , ) -> np . ndarray : \"\"\"Set the region of interest for the slide. Can be set manually or by selecting a region on a thumbnail. The ROI is stored as a tuple in the ROI attribute. Args: roi: Set the region of interest manually as a tuple (x1, y1, x2, y2). rois_df_path: The path to dataframe containing the ROIs with a slide_id column identifying the slide, and the ROI coordinates as columns (x1, y1, x2, y2). If roi is not provided, the ROI will be set from this dataframe. Returns: The region of interest set for the slide as a tuple (x1, y1, x2, y2). \"\"\" if roi is not None : ROI = np . array ( roi ) . astype ( int ) elif rois_df_path is not None : rois_df = pd . read_csv ( rois_df_path ) if \"slide_id\" not in rois_df . columns or len ( rois_df . columns ) < 5 : raise ValueError ( \"The provided dataframe does not contain a 'slide_id' column, or does not have enough columns for ROI coordinates.\" ) ROI = rois_df [ rois_df . slide_id == self . slide_name ] . values [ 0 ] . astype ( int ) else : log . info ( \"No ROI provided, prompting user to select one.\" ) level = input ( f \"No ROI was provided, please select a level at which the ROI should be created (max level: { len ( self . level_dimensions ) - 1 } ): \" ) if not level : log . info ( \"No level provided, setting the ROI at the highest level.\" ) level = len ( self . level_downsamples ) - 1 else : level = int ( level ) img = np . array ( self . create_thumbnail ( level )) ROI = select_roi_on_thumbnail ( img , int ( self . level_downsamples [ level ])) ROI = ( ROI * self . level_downsamples [ level ]) . astype ( int ) self . ROI = ROI self . ROI_width = ROI [ 2 ] - ROI [ 0 ] self . ROI_height = ROI [ 3 ] - ROI [ 1 ] log . info ( f \"ROI for slide { self . slide_name } has been set to { self . ROI } .\" ) return ROI set_slide_attributes () Set the slide attributes. Source code in src/prismtoolbox/wsicore/wsi.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def set_slide_attributes ( self ): \"\"\"Set the slide attributes.\"\"\" if self . engine == \"openslide\" : self . dimensions = self . slide . dimensions self . level_dimensions = self . slide . level_dimensions self . level_downsamples = self . slide . level_downsamples self . properties = self . slide . properties elif self . engine == \"tiffslide\" : self . dimensions = self . slide . dimensions self . level_dimensions = self . slide . level_dimensions self . level_downsamples = self . slide . level_downsamples self . properties = self . slide . properties else : raise NotImplementedError ( f \"Engine { self . engine } not supported.\" ) if ( f \" { self . engine } .bounds-x\" in self . properties . keys () and self . properties [ f \" { self . engine } .bounds-x\" ] is not None ): self . offset = ( - int ( self . properties [ f \" { self . engine } .bounds-x\" ]), - int ( self . properties [ f \" { self . engine } .bounds-y\" ]), ) stitch ( vis_level , selected_idx = None , colors = None , alpha = 0.6 , black_white = False , draw_grid = False , crop_roi = False , background_color = ( 0 , 0 , 0 )) Stitch the patches extracted on an image. The patches can be masked and colored depending on the mask and colors provided. Requires the coordinates of the patches to be set for the slide beforehand with the extract_patches method. Parameters: Name Type Description Default vis_level int The level at which the patches should be visualized. required selected_idx ndarray | None An array of indices of the patches to visualize (if set to None, all the patches will be visualized). None colors ndarray | None An array of RGB colors to apply to the patches (if set to None, the patches will be visualized as they are). None alpha float Set the transparency of the colors to apply to the patches. 0.6 black_white bool Set to True to visualize a binary mask of the patches extracted. False draw_grid bool Set to True to draw a grid on the stitched patches. False crop_roi bool Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the set_roi method). False background_color tuple [ int , int , int ] The color of the background. (0, 0, 0) Returns: Type Description Image A PIL image of the stitched patches. Source code in src/prismtoolbox/wsicore/wsi.py 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 def stitch ( self , vis_level : int , selected_idx : np . ndarray | None = None , colors : np . ndarray | None = None , alpha : float = 0.6 , black_white : bool = False , draw_grid : bool = False , crop_roi : bool = False , background_color : tuple [ int , int , int ] = ( 0 , 0 , 0 ), ) -> Image . Image : \"\"\"Stitch the patches extracted on an image. The patches can be masked and colored depending on the mask and colors provided. Requires the coordinates of the patches to be set for the slide beforehand with the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method. Args: vis_level: The level at which the patches should be visualized. selected_idx: An array of indices of the patches to visualize (if set to None, all the patches will be visualized). colors: An array of RGB colors to apply to the patches (if set to None, the patches will be visualized as they are). alpha: Set the transparency of the colors to apply to the patches. black_white: Set to True to visualize a binary mask of the patches extracted. draw_grid: Set to True to draw a grid on the stitched patches. crop_roi: Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). background_color: The color of the background. Returns: A PIL image of the stitched patches. \"\"\" if self . coords_attrs is None : raise RuntimeError ( f \"No attributes set for the patches of the slide { self . slide_name } , please check if patches were correctly extracted.\" ) assert self . coords is not None , ( \"No coordinates provided for the patches to visualize, please run the \" \"extract_patches method first or load the coordinates from a file.\" ) if crop_roi : if self . ROI is None or self . ROI_width is None or self . ROI_height is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) w , h = int ( np . ceil ( self . ROI_width / self . level_downsamples [ vis_level ])), int ( np . ceil ( self . ROI_height / self . level_downsamples [ vis_level ]) ) offset = np . array ( self . ROI [: 2 ]) else : w , h = self . level_dimensions [ vis_level ] offset = np . array ([ 0 , 0 ]) patch_size = self . coords_attrs [ \"patch_size\" ] patch_level = self . coords_attrs [ \"patch_level\" ] patch_size = int ( patch_size * self . level_downsamples [ patch_level ]) canvas = init_image ( w , h , mask = black_white , color_bakground = background_color ) downsample_vis = self . level_downsamples [ vis_level ] idxs = np . arange ( len ( self . coords )) if selected_idx is not None : idxs = idxs [ selected_idx ] patch_size = np . ceil ( patch_size / downsample_vis ) . astype ( int ) log . info ( f \"Stitching { len ( idxs ) } patches at level { vis_level } with patch size { patch_size } , \" f \"with colors { colors is not None } .\" ) for idx in idxs : coord = self . coords [ idx ] coord_downsampled = np . ceil ( np . abs ( coord - offset ) / downsample_vis ) . astype ( int ) patch_size_coord = ( min ( max ( w - coord_downsampled [ 0 ], 0 ), patch_size ), min ( max ( h - coord_downsampled [ 1 ], 0 ), patch_size ), ) if any ( val == 0 for val in patch_size_coord ): continue if black_white : patch = np . ones ( patch_size_coord , dtype = \"uint8\" ) colors = None else : patch = np . array ( self . slide . read_region ( tuple ( coord ), vis_level , patch_size_coord ) . convert ( \"RGB\" ) ) if colors is not None : assert len ( colors ) == len ( idxs ), ( \"The number of colors provided must match \" \"the number of selected coordinates.\" ) color = colors [ idx ] color_patch = ( np . ones (( patch_size_coord [ 1 ], patch_size_coord [ 0 ], 3 )) * color ) . astype ( \"uint8\" ) canvas [ coord_downsampled [ 1 ] : coord_downsampled [ 1 ] + patch_size_coord [ 1 ], coord_downsampled [ 0 ] : coord_downsampled [ 0 ] + patch_size_coord [ 0 ], :, ] = cv2 . addWeighted ( color_patch , alpha , patch , 1 - alpha , 0 , patch ) else : canvas [ coord_downsampled [ 1 ] : coord_downsampled [ 1 ] + patch_size_coord [ 1 ], coord_downsampled [ 0 ] : coord_downsampled [ 0 ] + patch_size_coord [ 0 ], :, ] = patch if draw_grid : cv2 . rectangle ( canvas , tuple ( np . maximum ([ 0 , 0 ], coord_downsampled - 1 )), tuple ( coord_downsampled + patch_size_coord ), ( 0 , 0 , 0 , 255 ), thickness = 2 , ) img = Image . fromarray ( canvas ) return img visualize ( vis_level , crop_roi = False , contours_color = ( 255 , 0 , 0 ), line_thickness = 500 , max_size = None , number_contours = False , black_white = False , view_slide_only = False ) Visualize the slide with or without the contours of the tissue. Parameters: Name Type Description Default vis_level int The level at which the visualization should be performed. required crop_roi bool Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the set_roi method). False contours_color tuple [ int , int , int ] The color to use for the contours. (255, 0, 0) line_thickness int The thickness to use for the contours 500 max_size int | None The maximum size for the visualization for the width or height of the image. None number_contours bool Set to True to number the contours. False black_white bool Set to True to visualize a binary mask of the contoured tissue. False view_slide_only bool Set to True to visualize the slide only (without the contours). False Returns: Type Description Image A PIL image of the visualization. Source code in src/prismtoolbox/wsicore/wsi.py 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 def visualize ( self , vis_level : int , crop_roi : bool = False , contours_color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), line_thickness : int = 500 , max_size : int | None = None , number_contours : bool = False , black_white : bool = False , view_slide_only : bool = False , ) -> Image . Image : \"\"\"Visualize the slide with or without the contours of the tissue. Args: vis_level: The level at which the visualization should be performed. crop_roi: Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). contours_color: The color to use for the contours. line_thickness: The thickness to use for the contours max_size: The maximum size for the visualization for the width or height of the image. number_contours: Set to True to number the contours. black_white: Set to True to visualize a binary mask of the contoured tissue. view_slide_only: Set to True to visualize the slide only (without the contours). Returns: A PIL image of the visualization. \"\"\" assert line_thickness > 0 , \"line_thickness must be greater than 0\" scale = 1 / self . level_downsamples [ vis_level ] if black_white : img = np . zeros_like ( self . create_thumbnail ( vis_level , crop_roi ), dtype = \"uint8\" ) line_thickness = - 1 contours_color = ( 1 , 1 , 1 ) else : img = np . array ( self . create_thumbnail ( vis_level , crop_roi )) line_thickness = int ( line_thickness * scale ) if not view_slide_only : if self . tissue_contours is None : raise RuntimeError ( f \"No tissue contours found for the slide { self . slide_name } , please run the detect_tissue method first.\" ) if crop_roi : if self . ROI is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) offset = self . ROI [: 2 ] else : offset = np . array ([ 0 , 0 ]) contours = [ cont - offset for cont in self . tissue_contours ] contours = self . scale_contours ( contours , scale ) if len ( contours ) > 0 : if not number_contours : cv2 . drawContours ( img , contours , - 1 , contours_color , line_thickness , lineType = cv2 . LINE_8 , ) else : # add numbering to each contour for idx , cont in enumerate ( contours ): M = cv2 . moments ( cont ) cX = int ( M [ \"m10\" ] / ( M [ \"m00\" ] + 1e-9 )) cY = int ( M [ \"m01\" ] / ( M [ \"m00\" ] + 1e-9 )) # draw the contour and put text next to center cv2 . drawContours ( img , [ cont ], - 1 , contours_color , line_thickness , lineType = cv2 . LINE_8 , ) cv2 . putText ( img , \" {} \" . format ( idx ), ( cX , cY ), cv2 . FONT_HERSHEY_SIMPLEX , 2 , ( 255 , 0 , 0 ), 10 , ) img = Image . fromarray ( img ) if black_white : img = img . convert ( \"L\" ) w , h = img . size if max_size is not None and ( w > max_size or h > max_size ): resizeFactor = max_size / w if w > h else max_size / h img = img . resize (( int ( w * resizeFactor ), int ( h * resizeFactor ))) return img worker_init ( slide_path , engine ) staticmethod Initialize the worker process with a wsi object. Parameters: Name Type Description Default slide_path str The path to the slide. required engine str The backend library to use for reading the slide (currently only openslide and tiffslide are supported) required Source code in src/prismtoolbox/wsicore/wsi.py 170 171 172 173 174 175 176 177 178 179 180 @staticmethod def worker_init ( slide_path : str , engine : str ) -> None : \"\"\"Initialize the worker process with a wsi object. Args: slide_path: The path to the slide. engine: The backend library to use for reading the slide (currently only openslide and tiffslide are supported) \"\"\" global wsi wsi = WSI . read ( slide_path , engine )","title":"prismtoolbox.wsicore.wsi"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI","text":"The WSI (Whole Slide Image) class is responsible for handling operations related to whole slide images. Parameters: Name Type Description Default slide_path str The path to the slide image file. required engine str The engine used to read the slide image. 'openslide' Attributes: Name Type Description slide_path str The path to the slide image file. engine str The engine used to read the slide image. slide_name str The name of the slide image file. Retrieved from the slide path using retrieve_slide_name_ext method. slide_ext str The extension of the slide image file. Retrieved from the slide path using retrieve_slide_name_ext method. slide OpenSlide | TiffSlide The wsi read from the file using engine. dimensions list [ tuple [ int , int ]] The dimensions of the slide image. Set by the set_slide_attributes method. level_dimensions list [ tuple [ int , int ]] The dimensions of the different levels of the slide image. Set by the set_slide_attributes method. level_downsamples list [ tuple [ int , int ]] The downsampling factors of the different levels of the slide image. Set by the set_slide_attributes method. properties dict The properties of the slide image. Set by the set_slide_attributes method. offset tuple [ int , int ] The offset of the slide image. Set by the set_slide_attributes method. ROI ndarray | None The region of interest in the slide image. Please use the set_roi method to set the ROI. ROI_width int | None The width of the region of interest. Set by the set_roi method. ROI_height int | None The height of the region of interest. Set by the set_roi method. tissue_contours list [ ndarray ] | None The contours of the tissue in the slide image. Please use the detect_tissue method to detect the tissue contours. coords ndarray | None The coordinates of patches extracted from slide image. Please use the extract_patches method to extract patches. coords_attrs dict | None The attributes of the coordinates. Set by the extract_patches method. Source code in src/prismtoolbox/wsicore/wsi.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , slide_path : str , engine : str = \"openslide\" ): \"\"\"The WSI (Whole Slide Image) class is responsible for handling operations related to whole slide images. Args: slide_path: The path to the slide image file. engine: The engine used to read the slide image. Attributes: slide_path (str): The path to the slide image file. engine (str): The engine used to read the slide image. slide_name (str): The name of the slide image file. Retrieved from the slide path using [retrieve_slide_name_ext][prismtoolbox.wsicore.WSI.retrieve_slide_name_ext] method. slide_ext (str): The extension of the slide image file. Retrieved from the slide path using [retrieve_slide_name_ext][prismtoolbox.wsicore.WSI.retrieve_slide_name_ext] method. slide (OpenSlide | TiffSlide): The wsi read from the file using engine. dimensions (list[tuple[int, int]]): The dimensions of the slide image. Set by the set_slide_attributes method. level_dimensions (list[tuple[int, int]]): The dimensions of the different levels of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. level_downsamples (list[tuple[int, int]]): The downsampling factors of the different levels of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. properties (dict): The properties of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. offset (tuple[int, int]): The offset of the slide image. Set by the [set_slide_attributes][prismtoolbox.wsicore.WSI.set_slide_attributes] method. ROI (ndarray | None): The region of interest in the slide image. Please use the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method to set the ROI. ROI_width (int | None): The width of the region of interest. Set by the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method. ROI_height (int | None): The height of the region of interest. Set by the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method. tissue_contours (list[ndarray] | None): The contours of the tissue in the slide image. Please use the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method to detect the tissue contours. coords (np.ndarray | None): The coordinates of patches extracted from slide image. Please use the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method to extract patches. coords_attrs (dict | None): The attributes of the coordinates. Set by the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method. \"\"\" self . slide_path = slide_path self . engine = engine self . slide_name , self . slide_ext = self . retrieve_slide_name_ext ( self . slide_path ) self . slide = self . read ( slide_path , engine ) self . offset = ( 0 , 0 ) self . set_slide_attributes () self . ROI = None self . ROI_width = None self . ROI_height = None self . tissue_contours = None self . coords = None self . coords_attrs = None","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;WSI"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.apply_pathologist_annotations","text":"Apply pathologist annotations to the tissue contours by intersecting the annotations with the tissue contours. Requires the tissue contours to be set for the slide beforehand with the detect_tissue method. Parameters: Name Type Description Default path str The path to the .geojson file containing the annotations extracted from QuPath. required class_name str The class name to use for selecting the annotations to apply. 'annotation' column_to_select str The column to select in the GeoDataFrame. 'objectType' Source code in src/prismtoolbox/wsicore/wsi.py 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 def apply_pathologist_annotations ( self , path : str , class_name : str = \"annotation\" , column_to_select : str = \"objectType\" , ) -> None : \"\"\"Apply pathologist annotations to the tissue contours by intersecting the annotations with the tissue contours. Requires the tissue contours to be set for the slide beforehand with the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method. Args: path: The path to the .geojson file containing the annotations extracted from QuPath. class_name: The class name to use for selecting the annotations to apply. column_to_select: The column to select in the GeoDataFrame. \"\"\" assert ( self . tissue_contours is not None ), \"No tissue contours found for the slide, please run the detect_tissue method first\" offset = ( - self . offset [ 0 ], - self . offset [ 1 ]) pathologist_annotations = read_qupath_annotations ( path , offset = offset , class_name = class_name , column_to_select = column_to_select ) polygons = contoursToPolygons ( self . tissue_contours , make_valid = True ) intersection = intersectionPolygons ( polygons , pathologist_annotations ) self . tissue_contours = PolygonsToContours ( intersection )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;apply_pathologist_annotations"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.convert_micrometer_to_pixel","text":"Convert a value from micrometer to pixel. Parameters: Name Type Description Default value float The value to convert (in micrometer). required level int The level at which the conversion should be performed. required axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description int The input value in pixel. Source code in src/prismtoolbox/wsicore/wsi.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def convert_micrometer_to_pixel ( self , value : float , level : int , axis : str = \"x\" , ) -> int : \"\"\"Convert a value from micrometer to pixel. Args: value: The value to convert (in micrometer). level: The level at which the conversion should be performed. axis: The axis to use for getting the conversion factor (x or y). Returns: The input value in pixel. \"\"\" return int ( value / float ( self . properties [ f \" { self . engine } .mpp- { axis } \" ])) // int ( self . level_downsamples [ level ] )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;convert_micrometer_to_pixel"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.convert_pixel_to_micrometer","text":"Convert a value from pixel to micrometer. Parameters: Name Type Description Default value float The value to convert (in pixel). required level int The level at which the conversion should be performed. required axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description float The input value in micrometer. Source code in src/prismtoolbox/wsicore/wsi.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def convert_pixel_to_micrometer ( self , value : float , level : int , axis : str = \"x\" , ) -> float : \"\"\"Convert a value from pixel to micrometer. Args: value: The value to convert (in pixel). level: The level at which the conversion should be performed. axis: The axis to use for getting the conversion factor (x or y). Returns: The input value in micrometer. \"\"\" return ( value * float ( self . properties [ f \" { self . engine } .mpp- { axis } \" ]) * self . level_downsamples [ level ] )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;convert_pixel_to_micrometer"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.convert_units","text":"Convert a value from one unit to another. Parameters: Name Type Description Default value float The value to convert. required level int The level at which the conversion should be performed. required from_unit str The unit to convert from (px or micro). required to_unit str The unit to convert to (px or micro). 'px' axis str The axis to use for getting the conversion factor (x or y). 'x' Returns: Type Description int | float The input value converted in the desired unit. Source code in src/prismtoolbox/wsicore/wsi.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 def convert_units ( self , value : float , level : int , from_unit : str , to_unit : str = \"px\" , axis : str = \"x\" , ) -> int | float : \"\"\"Convert a value from one unit to another. Args: value: The value to convert. level: The level at which the conversion should be performed. from_unit: The unit to convert from (px or micro). to_unit: The unit to convert to (px or micro). axis: The axis to use for getting the conversion factor (x or y). Returns: The input value converted in the desired unit. \"\"\" if from_unit == \"micro\" and to_unit == \"px\" : value = self . convert_micrometer_to_pixel ( value , level , axis ) elif from_unit == \"px\" and to_unit == \"micro\" : value = self . convert_pixel_to_micrometer ( value , level , axis ) elif from_unit == to_unit : pass else : raise ValueError ( f \"Conversion from { from_unit } to { to_unit } not supported.\" ) return value","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;convert_units"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.create_thumbnail","text":"Create a thumbnail of the slide at a given level. Parameters: Name Type Description Default level int The level at which the thumbnail should be created. required crop_roi bool A boolean to crop the thumbnail to the region of interest defined for the slide (requires a ROI to be set for the slide beforehand with the set_roi method) False Returns: Type Description Image A thumbnail of the slide as a PIL image. Source code in src/prismtoolbox/wsicore/wsi.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 def create_thumbnail ( self , level : int , crop_roi : bool = False , ) -> Image . Image : \"\"\"Create a thumbnail of the slide at a given level. Args: level: The level at which the thumbnail should be created. crop_roi: A boolean to crop the thumbnail to the region of interest defined for the slide (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method) Returns: A thumbnail of the slide as a PIL image. \"\"\" thumb = self . read_region (( 0 , 0 ), level , self . level_dimensions [ level ]) . convert ( \"RGB\" ) if crop_roi : if self . ROI is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) log . info ( f \"Creating thumbnail with ROI { self . ROI } .\" ) coords_roi = ( self . ROI / self . level_downsamples [ level ]) . astype ( int ) thumb = thumb . crop ( tuple ( coords_roi )) return thumb","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;create_thumbnail"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.detect_tissue","text":"Segment the tissue on the slide based on a threshold on the Law's texture energy spot map and floodfill algorithm to fill the holes in the mask. The tissue contours are stored in the tissue_contours attribute. Parameters: Name Type Description Default seg_level int The level at which the segmentation should be performed. required window_avg int The size of the window for local averaging. required window_eng int The size of the window for Law's texture energy computation. required thresh int The threshold for binarization on the Law's texture energy spot map. required inside_roi bool Set to True to identify the tissue only within a ROI (requires a ROI to be set for the slide beforehand with the set_roi method). False inv_thresh bool Set to True to invert the thresholding. False area_min float The minimum area for a contour to be kept. 200000.0 start tuple [ int , int ] The starting point for the floodfill algorithm (should be left at (0, 0) in most cases). (0, 0) Source code in src/prismtoolbox/wsicore/wsi.py 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def detect_tissue ( self , seg_level : int , window_avg : int , window_eng : int , thresh : int , inside_roi : bool = False , inv_thresh : bool = False , area_min : float = 2e5 , start : tuple [ int , int ] = ( 0 , 0 ), ) -> None : \"\"\"Segment the tissue on the slide based on a threshold on the Law's texture energy spot map and floodfill algorithm to fill the holes in the mask. The tissue contours are stored in the tissue_contours attribute. Args: seg_level: The level at which the segmentation should be performed. window_avg: The size of the window for local averaging. window_eng: The size of the window for Law's texture energy computation. thresh: The threshold for binarization on the Law's texture energy spot map. inside_roi: Set to True to identify the tissue only within a ROI (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). inv_thresh: Set to True to invert the thresholding. area_min: The minimum area for a contour to be kept. start: The starting point for the floodfill algorithm (should be left at (0, 0) in most cases). \"\"\" final_contours = [] img = np . array ( self . create_thumbnail ( seg_level , inside_roi )) img_avg = local_average ( np . asarray ( img ), window_avg ) law_feats = compute_law_feats ( img_avg , window_eng )[ 0 ] filterred_img = apply_bilateral_filter ( np . clip ( law_feats , 0 , 255 ) . astype ( \"uint8\" )) threshed_img = apply_binary_thresh ( filterred_img , thresh , inv_thresh ) flooded_img = floodfill_img ( np . pad ( threshed_img , 1 ), start ) contours = contour_mask ( flooded_img ) for contour in contours : c = contour . copy () area = cv2 . contourArea ( c ) if area > area_min : final_contours . append ( contour ) if len ( final_contours ) == 0 : self . tissue_contours = [] log . warning ( f \"No tissue contours found for the slide { self . slide_name } .\" ) else : scale = self . level_downsamples [ seg_level ] offset = np . array ( self . ROI [: 2 ]) if self . ROI is not None else np . array ([ 0 , 0 ]) final_contours = self . scale_contours ( final_contours , scale ) final_contours = [ cont + offset for cont in final_contours ] # Sanity check to ensure that the contours are all within a ROI is provided if self . ROI is not None : assert all ( [ np . all ( cont >= self . ROI [: 2 ]) and np . all ( cont <= self . ROI [ 2 :]) for cont in final_contours ] ) self . tissue_contours = final_contours log . info ( f \"Identified { len ( final_contours ) } contours for the slide { self . slide_name } .\" ) return","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;detect_tissue"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.extract_patches","text":"Extract valid patches from the slide with different extraction modes. A patch is considered valid if it is not black or white and is within the region of interest or the tissue contours if relevant. The extracted patches are stored as coordinates in the coords attribute, and the attributes of the coordinates are stored in the coords_attrs attribute. Parameters: Name Type Description Default patch_size float The size of the patches to extract (assumed to be square). required patch_level int The level at which the patches should be extracted. required mode str The mode to use for the extraction: \"contours\" mode extracts patches within the tissue contours (requires the tissue contours to be set for the slide beforehand with the detect_tissue method). \"roi\" mode extracts patches within the region of interest (requires the ROI to be set for the slide beforehand with the set_roi method). \"all\" mode extracts patches from the entire slide required step_size float | None The step size for the sliding window (if set to None, the step size will be computed based on the overlap). None overlap float | None The overlap between patches as an absolute value (must be provided if step_size is set to None). None units tuple [ str , str ] The units for the patch size and step size/overlap values (pixels: px, micrometers: micro). ('px', 'px') use_padding bool Set to True to use padding for the extraction. True contours_mode str | None The mode to use for the contour checking function (must be provided if mode is set to contours). See IsInContour for more details. None rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Source code in src/prismtoolbox/wsicore/wsi.py 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 def extract_patches ( self , patch_size : float , patch_level : int , mode : str , step_size : float | None = None , overlap : float | None = None , units : tuple [ str , str ] = ( \"px\" , \"px\" ), use_padding : bool = True , contours_mode : str | None = None , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> None : \"\"\"Extract valid patches from the slide with different extraction modes. A patch is considered valid if it is not black or white and is within the region of interest or the tissue contours if relevant. The extracted patches are stored as coordinates in the coords attribute, and the attributes of the coordinates are stored in the coords_attrs attribute. Args: patch_size: The size of the patches to extract (assumed to be square). patch_level: The level at which the patches should be extracted. mode: The mode to use for the extraction: - \"contours\" mode extracts patches within the tissue contours (requires the tissue contours to be set for the slide beforehand with the [detect_tissue][prismtoolbox.wsicore.WSI.detect_tissue] method). - \"roi\" mode extracts patches within the region of interest (requires the ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). - \"all\" mode extracts patches from the entire slide step_size: The step size for the sliding window (if set to None, the step size will be computed based on the overlap). overlap: The overlap between patches as an absolute value (must be provided if step_size is set to None). units: The units for the patch size and step size/overlap values (pixels: px, micrometers: micro). use_padding: Set to True to use padding for the extraction. contours_mode: The mode to use for the contour checking function (must be provided if mode is set to contours). See [IsInContour][prismtoolbox.wsicore.core_utils.IsInContour] for more details. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. \"\"\" assert all ( [ unit in [ \"micro\" , \"px\" ] for unit in units ] ), \"Units must be either 'micro' or 'px'\" patch_size = int ( self . convert_units ( patch_size , patch_level , units [ 0 ])) if step_size is None : if overlap is None : raise ValueError ( \"Either step_size or overlap must be provided if step_size is not set\" ) step_size = int ( patch_size - self . convert_units ( overlap , patch_level , units [ 1 ])) else : step_size = int ( self . convert_units ( step_size , patch_level , units [ 1 ])) log . info ( f \"Extracting patches of size { patch_size } at level { patch_level } with step size { step_size } .\" ) if mode == \"contours\" : log . info ( \"Extracting patches with 'contours' mode.\" ) assert self . tissue_contours is not None , \"Empty tissue contours vector for the slide, please run the detect_tissue method first.\" assert len ( self . tissue_contours ) > 0 , \"No tissue contours found for the slide.\" assert contours_mode is not None , \"Contours mode must be provided if mode is set to 'contours'.\" valid_coords = [] for cont in self . tissue_contours : roi_dim = cv2 . boundingRect ( cont ) # type: ignore log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords . extend ( self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , cont , contours_mode , rgb_threshs = rgb_threshs , percentages = percentages , ) ) valid_coords = np . array ( valid_coords ) elif mode == \"roi\" : log . info ( \"Extracting patches with 'roi' mode.\" ) if self . ROI is None or self . ROI_width is None or self . ROI_height is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) roi_dim = self . ROI [ 0 ], self . ROI [ 1 ], self . ROI_width , self . ROI_height log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords = self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , rgb_threshs = rgb_threshs , percentages = percentages , ) elif mode == \"all\" : roi_dim = 0 , 0 , self . level_dimensions [ 0 ][ 0 ], self . level_dimensions [ 0 ][ 1 ] log . info ( f \"Processing ROI of dimensions: { roi_dim } \" ) valid_coords = self . extract_patches_roi ( patch_level , patch_size , step_size , roi_dim , use_padding , rgb_threshs = rgb_threshs , percentages = percentages , ) else : raise ValueError ( f \"Mode { mode } not supported\" ) attr = { \"patch_size\" : patch_size , \"patch_level\" : patch_level , \"downsample\" : self . level_downsamples [ patch_level ], \"downsampled_level_dim\" : tuple ( np . array ( self . level_dimensions [ patch_level ])), \"level_dim\" : self . level_dimensions [ patch_level ], \"name\" : self . slide_name , } if len ( valid_coords ) == 0 : log . warning ( f \"No valid coordinates found for the slide { self . slide_name } .\" ) else : log . info ( f \"Identified a total of { len ( valid_coords ) } valid coordinates for the slide { self . slide_name } .\" ) self . coords = valid_coords self . coords_attrs = attr","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;extract_patches"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.extract_patches_roi","text":"Extract valid patches from a region of interest, i.e if the patch is not black or white and is within the region of interest/contours if relevant). Parameters: Name Type Description Default patch_level int The level at which the patches should be extracted. required patch_size int The size of the patches to extract (assumed to be square). required step_size int | None The step size to use for the sliding window. None roi_dim tuple [ int , int , int , int ] | None The top-left corner coordinates and dimensions of the region of interest. Must be provided if coord_candidates is set to None. None use_padding bool Set to True to use padding for the extraction. True contour ndarray | None The tissue contour to use for the extraction. If set to None, will not check if patches are within a contour. None contours_mode str | None The mode for the contour checking function. See IsInContour for more details. Must be provided if mode is set to contours. Otherwise, will not check if patches are within the contours. None coord_candidates ndarray | None Precomputed candidate coordinates for the patches. If set to None, will compute the candidate coordinates based on the ROI dimensions and the step size. None rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) return_indices bool Set to True to return the indices of the valid coordinates. False Returns: Type Description ndarray | tuple [ ndarray , list [ int ]] An array of valid coordinates for the patches (i.e. coordinates of the top-left corner of the patches). Source code in src/prismtoolbox/wsicore/wsi.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 def extract_patches_roi ( self , patch_level : int , patch_size : int , step_size : int | None = None , roi_dim : tuple [ int , int , int , int ] | None = None , use_padding : bool = True , contour : np . ndarray | None = None , contours_mode : str | None = None , coord_candidates : np . ndarray | None = None , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), return_indices : bool = False , ) -> np . ndarray | tuple [ np . ndarray , list [ int ]]: \"\"\"Extract valid patches from a region of interest, i.e if the patch is not black or white and is within the region of interest/contours if relevant). Args: patch_level: The level at which the patches should be extracted. patch_size: The size of the patches to extract (assumed to be square). step_size: The step size to use for the sliding window. roi_dim: The top-left corner coordinates and dimensions of the region of interest. Must be provided if coord_candidates is set to None. use_padding: Set to True to use padding for the extraction. contour: The tissue contour to use for the extraction. If set to None, will not check if patches are within a contour. contours_mode: The mode for the contour checking function. See [IsInContour][prismtoolbox.wsicore.core_utils.IsInContour] for more details. Must be provided if mode is set to contours. Otherwise, will not check if patches are within the contours. coord_candidates: Precomputed candidate coordinates for the patches. If set to None, will compute the candidate coordinates based on the ROI dimensions and the step size. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. return_indices: Set to True to return the indices of the valid coordinates. Returns: An array of valid coordinates for the patches (i.e. coordinates of the top-left corner of the patches). \"\"\" patch_downsample = int ( self . level_downsamples [ patch_level ]) ref_patch_size = patch_size * patch_downsample if coord_candidates is None : if roi_dim is None or step_size is None : raise ValueError ( \"roi_dim and step_size must be provided if coord_candidates is not set.\" ) start_x , start_y , w , h = roi_dim img_w , img_h = self . level_dimensions [ 0 ] if use_padding : stop_y = start_y + h stop_x = start_x + w else : stop_y = min ( start_y + h , img_h - ref_patch_size + 1 ) stop_x = min ( start_x + w , img_w - ref_patch_size + 1 ) step_size = step_size * patch_downsample x_range = np . arange ( start_x , stop_x , step = step_size ) y_range = np . arange ( start_y , stop_y , step = step_size ) x_coords , y_coords = np . meshgrid ( x_range , y_range , indexing = \"ij\" ) coord_candidates = np . array ( [ x_coords . flatten (), y_coords . flatten ()] ) . transpose () if contour is not None : if contours_mode is None : raise ValueError ( \"A contour mode must be provided if patch extraction mode is set to contours.\" ) cont_check_fn = IsInContour ( contour , patch_size = ref_patch_size , center_shift = 0.5 , mode = contours_mode ) log . info ( f \"Extracting patches with contour checking function mode { contours_mode } .\" ) else : cont_check_fn = None num_workers = mp . cpu_count () pool = mp . Pool ( num_workers , initializer = WSI . worker_init , initargs = ( self . slide_path , self . engine , ), ) iterable = [ ( coord , cont_check_fn , patch_level , patch_size , rgb_threshs , percentages ) for coord in coord_candidates ] valid_coords = pool . starmap ( WSI . process_coord_candidate , iterable ) pool . close () valid_indices = [ i for i , coord in enumerate ( valid_coords ) if coord is not None ] valid_coords = np . array ([ coord_candidates [ i ] for i in valid_indices ]) log . info ( f \"Identified { len ( valid_coords ) } valid coordinates in the ROI { roi_dim } .\" ) if return_indices : return valid_coords , valid_indices else : return valid_coords","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;extract_patches_roi"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.is_black_white","text":"Check if a patch is black or white. Parameters: Name Type Description Default patch Image The input patch. required rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Returns: Type Description bool True if the patch is black or white, False otherwise. Source code in src/prismtoolbox/wsicore/wsi.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 @staticmethod def is_black_white ( patch : Image . Image , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> bool : \"\"\"Check if a patch is black or white. Args: patch: The input patch. rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. Returns: True if the patch is black or white, False otherwise. \"\"\" return isBlackPatch ( patch , rgb_thresh = rgb_threshs [ 0 ], percentage = percentages [ 0 ] ) or isWhitePatch ( patch , rgb_thresh = rgb_threshs [ 1 ], percentage = percentages [ 1 ])","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;is_black_white"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.load_patches","text":"Load the patches from a hdf5 file. Parameters: Name Type Description Default file_path str The path to the hdf5 file containing the patches. required Source code in src/prismtoolbox/wsicore/wsi.py 462 463 464 465 466 467 468 469 def load_patches ( self , file_path : str ) -> None : \"\"\"Load the patches from a hdf5 file. Args: file_path: The path to the hdf5 file containing the patches. \"\"\" log . info ( f \"Loading patches for slide { self . slide_name } from { file_path } .\" ) self . coords , self . coords_attrs = read_h5_file ( file_path , \"coords\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;load_patches"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.load_tissue_contours","text":"Load the tissue contours from a pickle file. Parameters: Name Type Description Default file_path str The path to the pickle file containing the tissue contours. required Source code in src/prismtoolbox/wsicore/wsi.py 370 371 372 373 374 375 376 377 def load_tissue_contours ( self , file_path : str ) -> None : \"\"\"Load the tissue contours from a pickle file. Args: file_path: The path to the pickle file containing the tissue contours. \"\"\" log . info ( f \"Loading tissue contours for slide { self . slide_name } from { file_path } .\" ) self . tissue_contours = load_obj_with_pickle ( file_path )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;load_tissue_contours"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.process_coord_candidate","text":"Determine if a candidate coordinate is valid based on a contour checking function and/or black/white thresholding. Parameters: Name Type Description Default coord tuple [ int , int ] The candidate coordinate. required cont_check_fn IsInContour | None The contour checking function. required patch_level int The level at which the patch should be extracted. required patch_size int The size of the patch (assumed to be square). required rgb_threshs tuple [ int , int ] The tuple of thresholds for the RGB channels (black threshold, white threshold). (2, 220) percentages tuple [ float , float ] The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. (0.6, 0.9) Returns: Type Description tuple [ int , int ] | None The coordinate if it is valid, None otherwise. Source code in src/prismtoolbox/wsicore/wsi.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 @staticmethod def process_coord_candidate ( coord : tuple [ int , int ], cont_check_fn : IsInContour | None , patch_level : int , patch_size : int , rgb_threshs : tuple [ int , int ] = ( 2 , 220 ), percentages : tuple [ float , float ] = ( 0.6 , 0.9 ), ) -> tuple [ int , int ] | None : \"\"\"Determine if a candidate coordinate is valid based on a contour checking function and/or black/white thresholding. Args: coord: The candidate coordinate. cont_check_fn: The contour checking function. patch_level: The level at which the patch should be extracted. patch_size: The size of the patch (assumed to be square). rgb_threshs: The tuple of thresholds for the RGB channels (black threshold, white threshold). percentages: The tuple of percentages of pixels below/above the thresholds to consider the patch as black/white. Returns: The coordinate if it is valid, None otherwise. \"\"\" if cont_check_fn is None or cont_check_fn ( coord ): patch = wsi . read_region ( coord , patch_level , ( patch_size , patch_size ) ) . convert ( \"RGB\" ) if not WSI . is_black_white ( patch , rgb_threshs , percentages ): return coord else : return None else : return None","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;process_coord_candidate"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.read","text":"Read a slide with a given engine. Parameters: Name Type Description Default slide_path str The path to the slide. required engine str The backend library to use for reading the slide (currently only openslide and tiffslide are supported). required Returns: Type Description A slide object. Source code in src/prismtoolbox/wsicore/wsi.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 @staticmethod def read ( slide_path : str , engine : str ): \"\"\"Read a slide with a given engine. Args: slide_path: The path to the slide. engine: The backend library to use for reading the slide (currently only openslide and tiffslide are supported). Returns: A slide object. \"\"\" if engine == \"openslide\" : import openslide slide = openslide . OpenSlide ( slide_path ) elif engine == \"tiffslide\" : import tiffslide slide = tiffslide . TiffSlide ( slide_path ) else : raise NotImplementedError ( f \"engine { engine } not supported\" ) return slide","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;read"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.read_region","text":"Read a region from the slide for a given level and size. Parameters: Name Type Description Default location tuple [ int , int ] The coordinates of the top left corner of the region (in pixels). required level int The level at which the region should be read. required size tuple [ int , int ] The size of the region (in pixels). required Returns: Type Description Image The desired region of the slide as a PIL image. Source code in src/prismtoolbox/wsicore/wsi.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def read_region ( self , location : tuple [ int , int ], level : int , size : tuple [ int , int ], ) -> Image . Image : \"\"\"Read a region from the slide for a given level and size. Args: location: The coordinates of the top left corner of the region (in pixels). level: The level at which the region should be read. size: The size of the region (in pixels). Returns: The desired region of the slide as a PIL image. \"\"\" return self . slide . read_region ( location , level , size ) . convert ( \"RGB\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;read_region"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.retrieve_slide_name_ext","text":"Retrieves slide name and slide extension from slide path. Parameters: Name Type Description Default slide_path str The path to the slide. required Returns: Type Description tuple [ str , str ] A tuple (slide name, slide ext). Source code in src/prismtoolbox/wsicore/wsi.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 @staticmethod def retrieve_slide_name_ext ( slide_path : str ) -> tuple [ str , str ]: \"\"\"Retrieves slide name and slide extension from slide path. Args: slide_path: The path to the slide. Returns: A tuple (slide name, slide ext). \"\"\" slide_name = re . search ( r \"([^/]+?)(?=\\.[\\w\\.]+$)\" , slide_path ) slide_ext = re . search ( r \"(?<=\\.)[\\w\\.]+$\" , slide_path ) if slide_ext is None or slide_name is None : raise ValueError ( f \"Could not retrieve slide name and extension from { slide_path } . \" \"Please check the file path.\" ) else : slide_name , slide_ext = slide_name . group ( 0 ), slide_ext . group ( 0 ) return slide_name , slide_ext","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;retrieve_slide_name_ext"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.save_patches","text":"Save the patches in a hdf5 or geojson file. Parameters: Name Type Description Default save_dir str The path to the directory where the patches will be saved. required file_format str The format for the saving (h5 for python processing, geojson for QuPath processing). 'h5' selected_idx ndarray | None An array of indices of the patches to save (if set to None, all the patches will be saved). None merge bool Set to True to merge the patches into a single polygon (for geojson format only). False label str | None An optional label to assign to the patches (for geojson format only). None color tuple [ int , int , int ] An optional color to assign to the patches (for geojson format only). (255, 0, 0) append_to_existing_file bool Set to True to append the patches to an existing geojson file. False Source code in src/prismtoolbox/wsicore/wsi.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def save_patches ( self , save_dir : str , file_format : str = \"h5\" , selected_idx : np . ndarray | None = None , merge : bool = False , label : str | None = None , color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), append_to_existing_file : bool = False , ) -> None : \"\"\"Save the patches in a hdf5 or geojson file. Args: save_dir: The path to the directory where the patches will be saved. file_format: The format for the saving (h5 for python processing, geojson for QuPath processing). selected_idx: An array of indices of the patches to save (if set to None, all the patches will be saved). merge: Set to True to merge the patches into a single polygon (for geojson format only). label: An optional label to assign to the patches (for geojson format only). color: An optional color to assign to the patches (for geojson format only). append_to_existing_file: Set to True to append the patches to an existing geojson file. \"\"\" if self . coords is None : raise ValueError ( \"No patches found. Please check if patches were correctly extracted.\" ) if self . coords_attrs is None : raise ValueError ( \"No attributes set for the patches. \" \"Please check if patches were correctly extracted.\" ) if selected_idx is not None : coords = np . array ( self . coords [ selected_idx ]) else : coords = np . array ( self . coords ) if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if file_format == \"h5\" : asset_dict = { \"coords\" : coords } attr_dict = { \"coords\" : self . coords_attrs } file_path = os . path . join ( save_dir , f \" { self . slide_name } .h5\" ) log . info ( f \"Saving patches for slide { self . slide_name } at { file_path } as h5 file.\" ) save_patches_with_hdf5 ( file_path , asset_dict , attr_dict ) elif file_format == \"geojson\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .geojson\" ) patch_downsample = int ( self . level_downsamples [ self . coords_attrs [ \"patch_level\" ]] ) polygons = patchesToPolygons ( coords , self . coords_attrs [ \"patch_size\" ], patch_downsample , merge ) log . info ( f \"Saving { len ( coords ) } patches for slide { self . slide_name } at { file_path } as geojson file.\" ) export_polygons_to_qupath ( polygons , file_path , \"annotation\" , offset = self . offset , label = label , color = color , append_to_existing_file = append_to_existing_file , ) elif file_format == \"jpg\" or file_format == \"png\" : save_dir = os . path . join ( save_dir , self . slide_name ) if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) log . info ( f \"Saving { len ( coords ) } patches for slide { self . slide_name } at { save_dir } with { file_format } .\" ) for coord in coords : patch = self . read_region ( coord , self . coords_attrs [ \"patch_level\" ], ( self . coords_attrs [ \"patch_size\" ], self . coords_attrs [ \"patch_size\" ]), ) . convert ( \"RGB\" ) patch . save ( os . path . join ( save_dir , f \" { coord [ 0 ] } _ { coord [ 1 ] } . { file_format } \" )) else : raise ValueError ( f \"Format { file_format } not supported.\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;save_patches"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.save_tissue_contours","text":"Save the tissue contours in a pickle or geojson file. Parameters: Name Type Description Default save_dir str The path to the directory where the contours will be saved. required selected_idx ndarray | None An array of indices of the contours to save (if set to None, all the contours will be saved). None file_format str The file format for saving the contours (pickle for python processing, geojson for QuPath processing). 'pickle' merge bool Set to True to merge the contours into a single polygon (for geojson format only). False label str | None An optional label to assign to the tissue contours (for geojson format only). None color tuple [ int , int , int ] An optional color to assign to the tissue contours (for geojson format only). (255, 0, 0) append_to_existing_file bool Set to True to append the contours to an existing geojson file. False make_valid bool Set to True to make the polygons valid (for geojson format only). False Source code in src/prismtoolbox/wsicore/wsi.py 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def save_tissue_contours ( self , save_dir : str , selected_idx : np . ndarray | None = None , file_format : str = \"pickle\" , merge : bool = False , label : str | None = None , color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), append_to_existing_file : bool = False , make_valid : bool = False , ) -> None : \"\"\"Save the tissue contours in a pickle or geojson file. Args: save_dir: The path to the directory where the contours will be saved. selected_idx: An array of indices of the contours to save (if set to None, all the contours will be saved). file_format: The file format for saving the contours (pickle for python processing, geojson for QuPath processing). merge: Set to True to merge the contours into a single polygon (for geojson format only). label: An optional label to assign to the tissue contours (for geojson format only). color: An optional color to assign to the tissue contours (for geojson format only). append_to_existing_file: Set to True to append the contours to an existing geojson file. make_valid: Set to True to make the polygons valid (for geojson format only). \"\"\" assert self . tissue_contours is not None , ( \"No tissue contours found for the slide, \" \"please run the detect_tissue method first\" ) if selected_idx is not None : tissue_contours = [ self . tissue_contours [ idx ] for idx in selected_idx ] else : tissue_contours = self . tissue_contours if not os . path . isdir ( save_dir ): log . info ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if file_format == \"pickle\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .pkl\" ) log . info ( f \"Saving tissue contours for slide { self . slide_name } at { file_path } as pickle file.\" ) save_obj_with_pickle ( tissue_contours , file_path ) elif file_format == \"geojson\" : file_path = os . path . join ( save_dir , f \" { self . slide_name } .geojson\" ) log . info ( f \"Saving { selected_idx } tissue contours for slide { self . slide_name } at { file_path } as geojson file.\" ) polygons = contoursToPolygons ( tissue_contours , merge , make_valid ) export_polygons_to_qupath ( polygons , file_path , \"annotation\" , offset = self . offset , label = label , color = color , append_to_existing_file = append_to_existing_file , ) else : raise ValueError ( f \"format { file_format } not supported\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;save_tissue_contours"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.scale_contours","text":"Scale the contours by a given factor. Parameters: Name Type Description Default contours list [ ndarray ] The contours to scale. required scale float The scale factor to apply. required Returns: Type Description list [ ndarray ] The input list with scaled contours. Source code in src/prismtoolbox/wsicore/wsi.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 @staticmethod def scale_contours ( contours : list [ np . ndarray ], scale : float , ) -> list [ np . ndarray ]: \"\"\"Scale the contours by a given factor. Args: contours: The contours to scale. scale: The scale factor to apply. Returns: The input list with scaled contours. \"\"\" scaled_contours = [ np . array ( cont * scale , dtype = \"int\" ) for cont in contours ] return scaled_contours","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;scale_contours"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.set_roi","text":"Set the region of interest for the slide. Can be set manually or by selecting a region on a thumbnail. The ROI is stored as a tuple in the ROI attribute. Parameters: Name Type Description Default roi tuple [ int , int , int , int ] | None Set the region of interest manually as a tuple (x1, y1, x2, y2). None rois_df_path str | None The path to dataframe containing the ROIs with a slide_id column identifying the slide, and the ROI coordinates as columns (x1, y1, x2, y2). If roi is not provided, the ROI will be set from this dataframe. None Returns: The region of interest set for the slide as a tuple (x1, y1, x2, y2). Source code in src/prismtoolbox/wsicore/wsi.py 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def set_roi ( self , roi : tuple [ int , int , int , int ] | None = None , rois_df_path : str | None = None , ) -> np . ndarray : \"\"\"Set the region of interest for the slide. Can be set manually or by selecting a region on a thumbnail. The ROI is stored as a tuple in the ROI attribute. Args: roi: Set the region of interest manually as a tuple (x1, y1, x2, y2). rois_df_path: The path to dataframe containing the ROIs with a slide_id column identifying the slide, and the ROI coordinates as columns (x1, y1, x2, y2). If roi is not provided, the ROI will be set from this dataframe. Returns: The region of interest set for the slide as a tuple (x1, y1, x2, y2). \"\"\" if roi is not None : ROI = np . array ( roi ) . astype ( int ) elif rois_df_path is not None : rois_df = pd . read_csv ( rois_df_path ) if \"slide_id\" not in rois_df . columns or len ( rois_df . columns ) < 5 : raise ValueError ( \"The provided dataframe does not contain a 'slide_id' column, or does not have enough columns for ROI coordinates.\" ) ROI = rois_df [ rois_df . slide_id == self . slide_name ] . values [ 0 ] . astype ( int ) else : log . info ( \"No ROI provided, prompting user to select one.\" ) level = input ( f \"No ROI was provided, please select a level at which the ROI should be created (max level: { len ( self . level_dimensions ) - 1 } ): \" ) if not level : log . info ( \"No level provided, setting the ROI at the highest level.\" ) level = len ( self . level_downsamples ) - 1 else : level = int ( level ) img = np . array ( self . create_thumbnail ( level )) ROI = select_roi_on_thumbnail ( img , int ( self . level_downsamples [ level ])) ROI = ( ROI * self . level_downsamples [ level ]) . astype ( int ) self . ROI = ROI self . ROI_width = ROI [ 2 ] - ROI [ 0 ] self . ROI_height = ROI [ 3 ] - ROI [ 1 ] log . info ( f \"ROI for slide { self . slide_name } has been set to { self . ROI } .\" ) return ROI","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;set_roi"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.set_slide_attributes","text":"Set the slide attributes. Source code in src/prismtoolbox/wsicore/wsi.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def set_slide_attributes ( self ): \"\"\"Set the slide attributes.\"\"\" if self . engine == \"openslide\" : self . dimensions = self . slide . dimensions self . level_dimensions = self . slide . level_dimensions self . level_downsamples = self . slide . level_downsamples self . properties = self . slide . properties elif self . engine == \"tiffslide\" : self . dimensions = self . slide . dimensions self . level_dimensions = self . slide . level_dimensions self . level_downsamples = self . slide . level_downsamples self . properties = self . slide . properties else : raise NotImplementedError ( f \"Engine { self . engine } not supported.\" ) if ( f \" { self . engine } .bounds-x\" in self . properties . keys () and self . properties [ f \" { self . engine } .bounds-x\" ] is not None ): self . offset = ( - int ( self . properties [ f \" { self . engine } .bounds-x\" ]), - int ( self . properties [ f \" { self . engine } .bounds-y\" ]), )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;set_slide_attributes"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.stitch","text":"Stitch the patches extracted on an image. The patches can be masked and colored depending on the mask and colors provided. Requires the coordinates of the patches to be set for the slide beforehand with the extract_patches method. Parameters: Name Type Description Default vis_level int The level at which the patches should be visualized. required selected_idx ndarray | None An array of indices of the patches to visualize (if set to None, all the patches will be visualized). None colors ndarray | None An array of RGB colors to apply to the patches (if set to None, the patches will be visualized as they are). None alpha float Set the transparency of the colors to apply to the patches. 0.6 black_white bool Set to True to visualize a binary mask of the patches extracted. False draw_grid bool Set to True to draw a grid on the stitched patches. False crop_roi bool Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the set_roi method). False background_color tuple [ int , int , int ] The color of the background. (0, 0, 0) Returns: Type Description Image A PIL image of the stitched patches. Source code in src/prismtoolbox/wsicore/wsi.py 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 def stitch ( self , vis_level : int , selected_idx : np . ndarray | None = None , colors : np . ndarray | None = None , alpha : float = 0.6 , black_white : bool = False , draw_grid : bool = False , crop_roi : bool = False , background_color : tuple [ int , int , int ] = ( 0 , 0 , 0 ), ) -> Image . Image : \"\"\"Stitch the patches extracted on an image. The patches can be masked and colored depending on the mask and colors provided. Requires the coordinates of the patches to be set for the slide beforehand with the [extract_patches][prismtoolbox.wsicore.WSI.extract_patches] method. Args: vis_level: The level at which the patches should be visualized. selected_idx: An array of indices of the patches to visualize (if set to None, all the patches will be visualized). colors: An array of RGB colors to apply to the patches (if set to None, the patches will be visualized as they are). alpha: Set the transparency of the colors to apply to the patches. black_white: Set to True to visualize a binary mask of the patches extracted. draw_grid: Set to True to draw a grid on the stitched patches. crop_roi: Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). background_color: The color of the background. Returns: A PIL image of the stitched patches. \"\"\" if self . coords_attrs is None : raise RuntimeError ( f \"No attributes set for the patches of the slide { self . slide_name } , please check if patches were correctly extracted.\" ) assert self . coords is not None , ( \"No coordinates provided for the patches to visualize, please run the \" \"extract_patches method first or load the coordinates from a file.\" ) if crop_roi : if self . ROI is None or self . ROI_width is None or self . ROI_height is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) w , h = int ( np . ceil ( self . ROI_width / self . level_downsamples [ vis_level ])), int ( np . ceil ( self . ROI_height / self . level_downsamples [ vis_level ]) ) offset = np . array ( self . ROI [: 2 ]) else : w , h = self . level_dimensions [ vis_level ] offset = np . array ([ 0 , 0 ]) patch_size = self . coords_attrs [ \"patch_size\" ] patch_level = self . coords_attrs [ \"patch_level\" ] patch_size = int ( patch_size * self . level_downsamples [ patch_level ]) canvas = init_image ( w , h , mask = black_white , color_bakground = background_color ) downsample_vis = self . level_downsamples [ vis_level ] idxs = np . arange ( len ( self . coords )) if selected_idx is not None : idxs = idxs [ selected_idx ] patch_size = np . ceil ( patch_size / downsample_vis ) . astype ( int ) log . info ( f \"Stitching { len ( idxs ) } patches at level { vis_level } with patch size { patch_size } , \" f \"with colors { colors is not None } .\" ) for idx in idxs : coord = self . coords [ idx ] coord_downsampled = np . ceil ( np . abs ( coord - offset ) / downsample_vis ) . astype ( int ) patch_size_coord = ( min ( max ( w - coord_downsampled [ 0 ], 0 ), patch_size ), min ( max ( h - coord_downsampled [ 1 ], 0 ), patch_size ), ) if any ( val == 0 for val in patch_size_coord ): continue if black_white : patch = np . ones ( patch_size_coord , dtype = \"uint8\" ) colors = None else : patch = np . array ( self . slide . read_region ( tuple ( coord ), vis_level , patch_size_coord ) . convert ( \"RGB\" ) ) if colors is not None : assert len ( colors ) == len ( idxs ), ( \"The number of colors provided must match \" \"the number of selected coordinates.\" ) color = colors [ idx ] color_patch = ( np . ones (( patch_size_coord [ 1 ], patch_size_coord [ 0 ], 3 )) * color ) . astype ( \"uint8\" ) canvas [ coord_downsampled [ 1 ] : coord_downsampled [ 1 ] + patch_size_coord [ 1 ], coord_downsampled [ 0 ] : coord_downsampled [ 0 ] + patch_size_coord [ 0 ], :, ] = cv2 . addWeighted ( color_patch , alpha , patch , 1 - alpha , 0 , patch ) else : canvas [ coord_downsampled [ 1 ] : coord_downsampled [ 1 ] + patch_size_coord [ 1 ], coord_downsampled [ 0 ] : coord_downsampled [ 0 ] + patch_size_coord [ 0 ], :, ] = patch if draw_grid : cv2 . rectangle ( canvas , tuple ( np . maximum ([ 0 , 0 ], coord_downsampled - 1 )), tuple ( coord_downsampled + patch_size_coord ), ( 0 , 0 , 0 , 255 ), thickness = 2 , ) img = Image . fromarray ( canvas ) return img","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;stitch"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.visualize","text":"Visualize the slide with or without the contours of the tissue. Parameters: Name Type Description Default vis_level int The level at which the visualization should be performed. required crop_roi bool Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the set_roi method). False contours_color tuple [ int , int , int ] The color to use for the contours. (255, 0, 0) line_thickness int The thickness to use for the contours 500 max_size int | None The maximum size for the visualization for the width or height of the image. None number_contours bool Set to True to number the contours. False black_white bool Set to True to visualize a binary mask of the contoured tissue. False view_slide_only bool Set to True to visualize the slide only (without the contours). False Returns: Type Description Image A PIL image of the visualization. Source code in src/prismtoolbox/wsicore/wsi.py 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 def visualize ( self , vis_level : int , crop_roi : bool = False , contours_color : tuple [ int , int , int ] = ( 255 , 0 , 0 ), line_thickness : int = 500 , max_size : int | None = None , number_contours : bool = False , black_white : bool = False , view_slide_only : bool = False , ) -> Image . Image : \"\"\"Visualize the slide with or without the contours of the tissue. Args: vis_level: The level at which the visualization should be performed. crop_roi: Set to True to crop the visualization to the region of interest (requires a ROI to be set for the slide beforehand with the [set_roi][prismtoolbox.wsicore.WSI.set_roi] method). contours_color: The color to use for the contours. line_thickness: The thickness to use for the contours max_size: The maximum size for the visualization for the width or height of the image. number_contours: Set to True to number the contours. black_white: Set to True to visualize a binary mask of the contoured tissue. view_slide_only: Set to True to visualize the slide only (without the contours). Returns: A PIL image of the visualization. \"\"\" assert line_thickness > 0 , \"line_thickness must be greater than 0\" scale = 1 / self . level_downsamples [ vis_level ] if black_white : img = np . zeros_like ( self . create_thumbnail ( vis_level , crop_roi ), dtype = \"uint8\" ) line_thickness = - 1 contours_color = ( 1 , 1 , 1 ) else : img = np . array ( self . create_thumbnail ( vis_level , crop_roi )) line_thickness = int ( line_thickness * scale ) if not view_slide_only : if self . tissue_contours is None : raise RuntimeError ( f \"No tissue contours found for the slide { self . slide_name } , please run the detect_tissue method first.\" ) if crop_roi : if self . ROI is None : raise ValueError ( \"ROI is not set for the slide, please set the ROI with the set_roi method.\" ) offset = self . ROI [: 2 ] else : offset = np . array ([ 0 , 0 ]) contours = [ cont - offset for cont in self . tissue_contours ] contours = self . scale_contours ( contours , scale ) if len ( contours ) > 0 : if not number_contours : cv2 . drawContours ( img , contours , - 1 , contours_color , line_thickness , lineType = cv2 . LINE_8 , ) else : # add numbering to each contour for idx , cont in enumerate ( contours ): M = cv2 . moments ( cont ) cX = int ( M [ \"m10\" ] / ( M [ \"m00\" ] + 1e-9 )) cY = int ( M [ \"m01\" ] / ( M [ \"m00\" ] + 1e-9 )) # draw the contour and put text next to center cv2 . drawContours ( img , [ cont ], - 1 , contours_color , line_thickness , lineType = cv2 . LINE_8 , ) cv2 . putText ( img , \" {} \" . format ( idx ), ( cX , cY ), cv2 . FONT_HERSHEY_SIMPLEX , 2 , ( 255 , 0 , 0 ), 10 , ) img = Image . fromarray ( img ) if black_white : img = img . convert ( \"L\" ) w , h = img . size if max_size is not None and ( w > max_size or h > max_size ): resizeFactor = max_size / w if w > h else max_size / h img = img . resize (( int ( w * resizeFactor ), int ( h * resizeFactor ))) return img","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;visualize"},{"location":"reference/prismtoolbox/wsicore/wsi/#prismtoolbox.wsicore.wsi.WSI.worker_init","text":"Initialize the worker process with a wsi object. Parameters: Name Type Description Default slide_path str The path to the slide. required engine str The backend library to use for reading the slide (currently only openslide and tiffslide are supported) required Source code in src/prismtoolbox/wsicore/wsi.py 170 171 172 173 174 175 176 177 178 179 180 @staticmethod def worker_init ( slide_path : str , engine : str ) -> None : \"\"\"Initialize the worker process with a wsi object. Args: slide_path: The path to the slide. engine: The backend library to use for reading the slide (currently only openslide and tiffslide are supported) \"\"\" global wsi wsi = WSI . read ( slide_path , engine )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;worker_init"},{"location":"reference/prismtoolbox/wsiemb/","text":"","title":"Index"},{"location":"reference/prismtoolbox/wsiemb/emb_utils/","text":"compute_cell_features ( cells_df_with_classes ) Compute the features of the cells from an input geodataframe for each class. Parameters: Name Type Description Default cells_df_with_classes GeoDataFrame A geodataframe containing the cells and their classes. required Returns: Type Description ndarray The cells based features for each class. Source code in src/prismtoolbox/wsiemb/emb_utils.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def compute_cell_features ( cells_df_with_classes : gpd . GeoDataFrame ) -> np . ndarray : \"\"\"Compute the features of the cells from an input geodataframe for each class. Args: cells_df_with_classes: A geodataframe containing the cells and their classes. Returns: The cells based features for each class. \"\"\" assert ( \"classification\" in cells_df_with_classes . columns ), \"The input geodataframe must contain a 'classification' column\" cells_feats = np . ravel ( cells_df_with_classes . groupby ( \"classification\" , sort = False ) . apply ( get_cell_properties , include_groups = False ) . to_list () ) return cells_feats compute_optimal_number_clusters ( embeddings_matrix , model_name , metric_name = 'davies_bouldin' , min_clusters = 2 , max_clusters = 10 , ** kwargs ) Compute the optimal number of clusters to retrieve from the embedding matrix using the specified clustering model and metric. Parameters: Name Type Description Default embeddings_matrix ndarray An array of shape (n_samples, n_features) containing the embeddings. required model_name str The name of the clustering model. At the moment, possible models are: \" kmeans \" \" kmeans_mini_batch \" required metric_name str The name of the metric used to evaluate the clustering quality. At the moment, possible metrics are: \" silhouette \" \" calinski_harabasz \" \" davies_bouldin \" 'davies_bouldin' min_clusters int The minimum number of clusters to consider. 2 max_clusters int The maximum number of clusters to consider. 10 **kwargs Some additional arguments for the clustering model (see the documentation of the clustering model). {} Returns: Type Description tuple [ int , list [ float ]] A tuple containing the optimal number of clusters and the quality scores for each number of clusters. Source code in src/prismtoolbox/wsiemb/emb_utils.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def compute_optimal_number_clusters ( embeddings_matrix : np . ndarray , model_name : str , metric_name : str = \"davies_bouldin\" , min_clusters : int = 2 , max_clusters : int = 10 , ** kwargs , ) -> tuple [ int , list [ float ]]: \"\"\"Compute the optimal number of clusters to retrieve from the embedding matrix using the specified clustering model and metric. Args: embeddings_matrix: An array of shape (n_samples, n_features) containing the embeddings. model_name: The name of the clustering model. At the moment, possible models are: - \"[kmeans](https://scikit-learn.org/stable/modules/generated/ sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\" - \"[kmeans_mini_batch](https://scikit-learn.org/stable/modules/generated/ sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans)\" metric_name: The name of the metric used to evaluate the clustering quality. At the moment, possible metrics are: - \"[silhouette](https://scikit-learn.org/stable/modules/generated/ sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\" - \"[calinski_harabasz](https://scikit-learn.org/stable/modules/generated/ sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score)\" - \"[davies_bouldin](https://scikit-learn.org/stable/modules/generated/ sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score)\" min_clusters: The minimum number of clusters to consider. max_clusters: The maximum number of clusters to consider. **kwargs: Some additional arguments for the clustering model (see the documentation of the clustering model). Returns: A tuple containing the optimal number of clusters and the quality scores for each number of clusters. \"\"\" if metric_name not in clustering_metrics . keys (): raise ValueError ( f \"Metric { metric_name } not implemented\" ) metric = clustering_metrics [ metric_name ] if model_name == \"kmeans\" : cluster_model = skl_cluster . KMeans ( ** kwargs ) elif model_name == \"kmeans_mini_batch\" : cluster_model = skl_cluster . MiniBatchKMeans ( ** kwargs ) else : raise ValueError ( f \"Model { model_name } not implemented\" ) scores = [] for n_clusters in tqdm ( range ( min_clusters , max_clusters + 1 ), desc = \"Computing clustering quality \" \"scores for each number of clusters\" , ): cluster_model . set_params ( n_clusters = n_clusters ) cluster_assignments = cluster_model . fit_predict ( embeddings_matrix ) scores . append ( metric ( embeddings_matrix , cluster_assignments )) if metric_name == \"davies_bouldin\" : optimal_number = np . argmin ( scores ) + min_clusters else : optimal_number = np . argmax ( scores ) + min_clusters return optimal_number , scores create_model ( arch_name , pretrained_weights = None ) Create a model from the arch_name and load the pretrained weights if specified. Parameters: Name Type Description Default arch_name str The architecture name. At the moment, possible models are: torchvision based models : \"resnet18\", \"resnet50\", \"resnet101\". CLAM , i.e ResNet50 truncated after the third convolutional block: \"clam\". Pathoduet model : \"pathoduet\". Phikon model : \"phikon\". Conch model : \"conch\". Uni model : \"uni\". required pretrained_weights str | None The path to the pretrained weights or the name of the pretrained weights. For torchvision models and CLAM, possible weights are: \"IMAGENET1K_V1\", \"IMAGENET1K_V2\", and \" ciga \" for ResNet18. For Pathoduet, possible weights are: \"pathoduet_HE\", \"pathoduet_IHC\". For Phikon, Uni, and Conch, the weights are automatically downloaded from the Hugging Face model hub. None Returns: Type Description tuple [ Module , Compose | None] A tuple containing a torch model and the transforms used to preprocess the images (set to None if no pretrained weights were used). Source code in src/prismtoolbox/wsiemb/emb_utils.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def create_model ( arch_name : str , pretrained_weights : str | None = None ) -> tuple [ nn . Module , transformsv2 . Compose | None ]: \"\"\"Create a model from the arch_name and load the pretrained weights if specified. Args: arch_name: The architecture name. At the moment, possible models are: - [torchvision based models](https://pytorch.org/vision/stable/models.html): \"resnet18\", \"resnet50\", \"resnet101\". - [CLAM](https://github.com/mahmoodlab/CLAM), i.e ResNet50 truncated after the third convolutional block: \"clam\". - [Pathoduet model](https://github.com/openmedlab/PathoDuet): \"pathoduet\". - [Phikon model](https://huggingface.co/owkin/phikon): \"phikon\". - [Conch model](https://huggingface.co/MahmoodLab/CONCH): \"conch\". - [Uni model](https://huggingface.co/MahmoodLab/UNI): \"uni\". pretrained_weights: The path to the pretrained weights or the name of the pretrained weights. - For torchvision models and CLAM, possible weights are: \"IMAGENET1K_V1\", \"IMAGENET1K_V2\", and \"[ciga](https://github.com/ozanciga/self-supervised-histopathology)\" for ResNet18. - For Pathoduet, possible weights are: \"pathoduet_HE\", \"pathoduet_IHC\". - For Phikon, Uni, and Conch, the weights are automatically downloaded from the Hugging Face model hub. Returns: A tuple containing a torch model and the transforms used to preprocess the images (set to None if no pretrained weights were used). \"\"\" if arch_name not in arch_dict : raise ValueError ( f \"invalid model name. Possible models: { arch_dict . keys () } \" ) model , pretrained_transforms = arch_dict [ arch_name ]( weights = pretrained_weights ) return model , pretrained_transforms extract_stain_features ( patches , conv_matrix_name = 'HED' ) Extract the stain features from the images. Parameters: Name Type Description Default imgs A tensor of shape (n_samples, n_channels, height, width) containing the images. required conv_matrix_name str The name of the conversion matrix. At the moment, possible values are: \"HED\": Hematoxylin, Eosin and DAB. \"HD\": Hematoxylin and DAB. \"HD_custom\": Custom Hematoxylin and DAB matrix. 'HED' Returns: Type Description Tensor A tensor of shape (n_samples, n_channels, height, width) containing the stain features. Source code in src/prismtoolbox/wsiemb/emb_utils.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def extract_stain_features ( patches : torch . Tensor , conv_matrix_name : str = \"HED\" ) -> torch . Tensor : \"\"\"Extract the stain features from the images. Args: imgs: A tensor of shape (n_samples, n_channels, height, width) containing the images. conv_matrix_name: The name of the conversion matrix. At the moment, possible values are: - \"HED\": Hematoxylin, Eosin and DAB. - \"HD\": Hematoxylin and DAB. - \"HD_custom\": Custom Hematoxylin and DAB matrix. Returns: A tensor of shape (n_samples, n_channels, height, width) containing the stain features. \"\"\" conv_matrix = torch . tensor ( retrieve_conv_matrix ( conv_matrix_name ), dtype = torch . float32 ) eps = torch . tensor ([ 1e-6 ], dtype = torch . float32 ) patches = torch . maximum ( patches , eps ) log_adjust = torch . log ( eps ) stains = torch . einsum ( \"bcij,cd->bdij\" , torch . log ( patches ) / log_adjust , conv_matrix ) stains = torch . maximum ( stains , torch . tensor ([ 0.0 ], dtype = torch . float32 )) if conv_matrix_name . startswith ( \"HD\" ): stains = stains [:, : 2 , ... ] stains_flattened = stains . view ( stains . size ( 0 ), stains . size ( 1 ), - 1 ) feats = torch . concatenate ( [ stains . mean ( dim = ( 2 , 3 )), stains . std ( dim = ( 2 , 3 )), stains . amin ( dim = ( 2 , 3 )), stains . amax ( dim = ( 2 , 3 )), stains_flattened . median ( dim =- 1 ) . values , stains_flattened . quantile ( 0.25 , dim =- 1 ), stains_flattened . quantile ( 0.75 , dim =- 1 ), ], dim = 1 , ) return feats get_cell_properties ( cells_df ) Get the properties of the cells in the input dataframe. Parameters: Name Type Description Default cells_df GeoDataFrame A geodataframe containing the cells. required Returns: Type Description tuple [ list [ float ], list [ str ]] A list of 7 features describing the number of cells and their average morphological properties. Source code in src/prismtoolbox/wsiemb/emb_utils.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def get_cell_properties ( cells_df : gpd . GeoDataFrame ) -> tuple [ list [ float ], list [ str ]]: \"\"\"Get the properties of the cells in the input dataframe. Args: cells_df: A geodataframe containing the cells. Returns: A list of 7 features describing the number of cells and their average morphological properties. \"\"\" if cells_df . geometry . isnull () . all (): return [ 0.0 ] * 7 else : N_cells = len ( cells_df ) avg_cells_area = cells_df . area . mean () avg_cells_perimeter = cells_df . length . mean () avg_cells_compactness = ( ( 4 * np . pi * cells_df . area ) / ( cells_df . length ** 2 ) ) . mean () avg_cells_roundness = ( ( 4 * cells_df . area ) / ( cells_df . convex_hull . length ** 2 ) ) . mean () avg_cells_solidity = ( cells_df . area / cells_df . convex_hull . area ) . mean () avg_cells_elongation = ( ( cells_df . bounds [ \"maxx\" ] - cells_df . bounds [ \"minx\" ]) / ( cells_df . bounds [ \"maxy\" ] - cells_df . bounds [ \"miny\" ]) ) . mean () return [ N_cells , avg_cells_area , avg_cells_perimeter , avg_cells_compactness , avg_cells_roundness , avg_cells_solidity , avg_cells_elongation , ] get_cells_in_patch ( cells_df , coord , patch_size , cell_classes ) Get the cells whose centroids are within the patch defined by the input coordinates and size. Parameters: Name Type Description Default cells_df GeoDataFrame A geodataframe containing the cells. required coord Tensor A tensor of shape (2,) containing the coordinates of the patch. required patch_size int The size of the patch. required cell_classes list [ str ] A list of cell classes. required Returns: Type Description GeoDataFrame A geodataframe containing the cells in the patch ordered by cell classes. Source code in src/prismtoolbox/wsiemb/emb_utils.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def get_cells_in_patch ( cells_df : gpd . GeoDataFrame , coord : torch . Tensor , patch_size : int , cell_classes : list [ str ], ) -> gpd . GeoDataFrame : \"\"\"Get the cells whose centroids are within the patch defined by the input coordinates and size. Args: cells_df: A geodataframe containing the cells. coord: A tensor of shape (2,) containing the coordinates of the patch. patch_size: The size of the patch. cell_classes: A list of cell classes. Returns: A geodataframe containing the cells in the patch ordered by cell classes. \"\"\" x , y = coord . numpy () patch_polygon = box ( x , y , x + patch_size , y + patch_size , ccw = False ) cells_in_patch = cells_df . loc [ cells_df . centroid . within ( patch_polygon ), ] cells_in_patch_grouped = [] for cell_class in cell_classes : if cell_class in cells_in_patch [ \"classification\" ] . values : cell_class_in_patch_df = cells_in_patch [ cells_in_patch [ \"classification\" ] == cell_class ] else : cell_class_in_patch_df = gpd . GeoDataFrame ( columns = cells_df . columns , data = [ pd . Series ({ col : None for col in cells_df . columns })], ) cell_class_in_patch_df [ \"classification\" ] = cell_class cells_in_patch_grouped . append ( cell_class_in_patch_df ) result = pd . concat ( cells_in_patch_grouped , ignore_index = True ) return result","title":"prismtoolbox.wsiemb.emb_utils"},{"location":"reference/prismtoolbox/wsiemb/emb_utils/#prismtoolbox.wsiemb.emb_utils.compute_cell_features","text":"Compute the features of the cells from an input geodataframe for each class. Parameters: Name Type Description Default cells_df_with_classes GeoDataFrame A geodataframe containing the cells and their classes. required Returns: Type Description ndarray The cells based features for each class. Source code in src/prismtoolbox/wsiemb/emb_utils.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def compute_cell_features ( cells_df_with_classes : gpd . GeoDataFrame ) -> np . ndarray : \"\"\"Compute the features of the cells from an input geodataframe for each class. Args: cells_df_with_classes: A geodataframe containing the cells and their classes. Returns: The cells based features for each class. \"\"\" assert ( \"classification\" in cells_df_with_classes . columns ), \"The input geodataframe must contain a 'classification' column\" cells_feats = np . ravel ( cells_df_with_classes . groupby ( \"classification\" , sort = False ) . apply ( get_cell_properties , include_groups = False ) . to_list () ) return cells_feats","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;compute_cell_features"},{"location":"reference/prismtoolbox/wsiemb/emb_utils/#prismtoolbox.wsiemb.emb_utils.compute_optimal_number_clusters","text":"Compute the optimal number of clusters to retrieve from the embedding matrix using the specified clustering model and metric. Parameters: Name Type Description Default embeddings_matrix ndarray An array of shape (n_samples, n_features) containing the embeddings. required model_name str The name of the clustering model. At the moment, possible models are: \" kmeans \" \" kmeans_mini_batch \" required metric_name str The name of the metric used to evaluate the clustering quality. At the moment, possible metrics are: \" silhouette \" \" calinski_harabasz \" \" davies_bouldin \" 'davies_bouldin' min_clusters int The minimum number of clusters to consider. 2 max_clusters int The maximum number of clusters to consider. 10 **kwargs Some additional arguments for the clustering model (see the documentation of the clustering model). {} Returns: Type Description tuple [ int , list [ float ]] A tuple containing the optimal number of clusters and the quality scores for each number of clusters. Source code in src/prismtoolbox/wsiemb/emb_utils.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def compute_optimal_number_clusters ( embeddings_matrix : np . ndarray , model_name : str , metric_name : str = \"davies_bouldin\" , min_clusters : int = 2 , max_clusters : int = 10 , ** kwargs , ) -> tuple [ int , list [ float ]]: \"\"\"Compute the optimal number of clusters to retrieve from the embedding matrix using the specified clustering model and metric. Args: embeddings_matrix: An array of shape (n_samples, n_features) containing the embeddings. model_name: The name of the clustering model. At the moment, possible models are: - \"[kmeans](https://scikit-learn.org/stable/modules/generated/ sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\" - \"[kmeans_mini_batch](https://scikit-learn.org/stable/modules/generated/ sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans)\" metric_name: The name of the metric used to evaluate the clustering quality. At the moment, possible metrics are: - \"[silhouette](https://scikit-learn.org/stable/modules/generated/ sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\" - \"[calinski_harabasz](https://scikit-learn.org/stable/modules/generated/ sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score)\" - \"[davies_bouldin](https://scikit-learn.org/stable/modules/generated/ sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score)\" min_clusters: The minimum number of clusters to consider. max_clusters: The maximum number of clusters to consider. **kwargs: Some additional arguments for the clustering model (see the documentation of the clustering model). Returns: A tuple containing the optimal number of clusters and the quality scores for each number of clusters. \"\"\" if metric_name not in clustering_metrics . keys (): raise ValueError ( f \"Metric { metric_name } not implemented\" ) metric = clustering_metrics [ metric_name ] if model_name == \"kmeans\" : cluster_model = skl_cluster . KMeans ( ** kwargs ) elif model_name == \"kmeans_mini_batch\" : cluster_model = skl_cluster . MiniBatchKMeans ( ** kwargs ) else : raise ValueError ( f \"Model { model_name } not implemented\" ) scores = [] for n_clusters in tqdm ( range ( min_clusters , max_clusters + 1 ), desc = \"Computing clustering quality \" \"scores for each number of clusters\" , ): cluster_model . set_params ( n_clusters = n_clusters ) cluster_assignments = cluster_model . fit_predict ( embeddings_matrix ) scores . append ( metric ( embeddings_matrix , cluster_assignments )) if metric_name == \"davies_bouldin\" : optimal_number = np . argmin ( scores ) + min_clusters else : optimal_number = np . argmax ( scores ) + min_clusters return optimal_number , scores","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;compute_optimal_number_clusters"},{"location":"reference/prismtoolbox/wsiemb/emb_utils/#prismtoolbox.wsiemb.emb_utils.create_model","text":"Create a model from the arch_name and load the pretrained weights if specified. Parameters: Name Type Description Default arch_name str The architecture name. At the moment, possible models are: torchvision based models : \"resnet18\", \"resnet50\", \"resnet101\". CLAM , i.e ResNet50 truncated after the third convolutional block: \"clam\". Pathoduet model : \"pathoduet\". Phikon model : \"phikon\". Conch model : \"conch\". Uni model : \"uni\". required pretrained_weights str | None The path to the pretrained weights or the name of the pretrained weights. For torchvision models and CLAM, possible weights are: \"IMAGENET1K_V1\", \"IMAGENET1K_V2\", and \" ciga \" for ResNet18. For Pathoduet, possible weights are: \"pathoduet_HE\", \"pathoduet_IHC\". For Phikon, Uni, and Conch, the weights are automatically downloaded from the Hugging Face model hub. None Returns: Type Description tuple [ Module , Compose | None] A tuple containing a torch model and the transforms used to preprocess the images (set to None if no pretrained weights were used). Source code in src/prismtoolbox/wsiemb/emb_utils.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def create_model ( arch_name : str , pretrained_weights : str | None = None ) -> tuple [ nn . Module , transformsv2 . Compose | None ]: \"\"\"Create a model from the arch_name and load the pretrained weights if specified. Args: arch_name: The architecture name. At the moment, possible models are: - [torchvision based models](https://pytorch.org/vision/stable/models.html): \"resnet18\", \"resnet50\", \"resnet101\". - [CLAM](https://github.com/mahmoodlab/CLAM), i.e ResNet50 truncated after the third convolutional block: \"clam\". - [Pathoduet model](https://github.com/openmedlab/PathoDuet): \"pathoduet\". - [Phikon model](https://huggingface.co/owkin/phikon): \"phikon\". - [Conch model](https://huggingface.co/MahmoodLab/CONCH): \"conch\". - [Uni model](https://huggingface.co/MahmoodLab/UNI): \"uni\". pretrained_weights: The path to the pretrained weights or the name of the pretrained weights. - For torchvision models and CLAM, possible weights are: \"IMAGENET1K_V1\", \"IMAGENET1K_V2\", and \"[ciga](https://github.com/ozanciga/self-supervised-histopathology)\" for ResNet18. - For Pathoduet, possible weights are: \"pathoduet_HE\", \"pathoduet_IHC\". - For Phikon, Uni, and Conch, the weights are automatically downloaded from the Hugging Face model hub. Returns: A tuple containing a torch model and the transforms used to preprocess the images (set to None if no pretrained weights were used). \"\"\" if arch_name not in arch_dict : raise ValueError ( f \"invalid model name. Possible models: { arch_dict . keys () } \" ) model , pretrained_transforms = arch_dict [ arch_name ]( weights = pretrained_weights ) return model , pretrained_transforms","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;create_model"},{"location":"reference/prismtoolbox/wsiemb/emb_utils/#prismtoolbox.wsiemb.emb_utils.extract_stain_features","text":"Extract the stain features from the images. Parameters: Name Type Description Default imgs A tensor of shape (n_samples, n_channels, height, width) containing the images. required conv_matrix_name str The name of the conversion matrix. At the moment, possible values are: \"HED\": Hematoxylin, Eosin and DAB. \"HD\": Hematoxylin and DAB. \"HD_custom\": Custom Hematoxylin and DAB matrix. 'HED' Returns: Type Description Tensor A tensor of shape (n_samples, n_channels, height, width) containing the stain features. Source code in src/prismtoolbox/wsiemb/emb_utils.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def extract_stain_features ( patches : torch . Tensor , conv_matrix_name : str = \"HED\" ) -> torch . Tensor : \"\"\"Extract the stain features from the images. Args: imgs: A tensor of shape (n_samples, n_channels, height, width) containing the images. conv_matrix_name: The name of the conversion matrix. At the moment, possible values are: - \"HED\": Hematoxylin, Eosin and DAB. - \"HD\": Hematoxylin and DAB. - \"HD_custom\": Custom Hematoxylin and DAB matrix. Returns: A tensor of shape (n_samples, n_channels, height, width) containing the stain features. \"\"\" conv_matrix = torch . tensor ( retrieve_conv_matrix ( conv_matrix_name ), dtype = torch . float32 ) eps = torch . tensor ([ 1e-6 ], dtype = torch . float32 ) patches = torch . maximum ( patches , eps ) log_adjust = torch . log ( eps ) stains = torch . einsum ( \"bcij,cd->bdij\" , torch . log ( patches ) / log_adjust , conv_matrix ) stains = torch . maximum ( stains , torch . tensor ([ 0.0 ], dtype = torch . float32 )) if conv_matrix_name . startswith ( \"HD\" ): stains = stains [:, : 2 , ... ] stains_flattened = stains . view ( stains . size ( 0 ), stains . size ( 1 ), - 1 ) feats = torch . concatenate ( [ stains . mean ( dim = ( 2 , 3 )), stains . std ( dim = ( 2 , 3 )), stains . amin ( dim = ( 2 , 3 )), stains . amax ( dim = ( 2 , 3 )), stains_flattened . median ( dim =- 1 ) . values , stains_flattened . quantile ( 0.25 , dim =- 1 ), stains_flattened . quantile ( 0.75 , dim =- 1 ), ], dim = 1 , ) return feats","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;extract_stain_features"},{"location":"reference/prismtoolbox/wsiemb/emb_utils/#prismtoolbox.wsiemb.emb_utils.get_cell_properties","text":"Get the properties of the cells in the input dataframe. Parameters: Name Type Description Default cells_df GeoDataFrame A geodataframe containing the cells. required Returns: Type Description tuple [ list [ float ], list [ str ]] A list of 7 features describing the number of cells and their average morphological properties. Source code in src/prismtoolbox/wsiemb/emb_utils.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def get_cell_properties ( cells_df : gpd . GeoDataFrame ) -> tuple [ list [ float ], list [ str ]]: \"\"\"Get the properties of the cells in the input dataframe. Args: cells_df: A geodataframe containing the cells. Returns: A list of 7 features describing the number of cells and their average morphological properties. \"\"\" if cells_df . geometry . isnull () . all (): return [ 0.0 ] * 7 else : N_cells = len ( cells_df ) avg_cells_area = cells_df . area . mean () avg_cells_perimeter = cells_df . length . mean () avg_cells_compactness = ( ( 4 * np . pi * cells_df . area ) / ( cells_df . length ** 2 ) ) . mean () avg_cells_roundness = ( ( 4 * cells_df . area ) / ( cells_df . convex_hull . length ** 2 ) ) . mean () avg_cells_solidity = ( cells_df . area / cells_df . convex_hull . area ) . mean () avg_cells_elongation = ( ( cells_df . bounds [ \"maxx\" ] - cells_df . bounds [ \"minx\" ]) / ( cells_df . bounds [ \"maxy\" ] - cells_df . bounds [ \"miny\" ]) ) . mean () return [ N_cells , avg_cells_area , avg_cells_perimeter , avg_cells_compactness , avg_cells_roundness , avg_cells_solidity , avg_cells_elongation , ]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;get_cell_properties"},{"location":"reference/prismtoolbox/wsiemb/emb_utils/#prismtoolbox.wsiemb.emb_utils.get_cells_in_patch","text":"Get the cells whose centroids are within the patch defined by the input coordinates and size. Parameters: Name Type Description Default cells_df GeoDataFrame A geodataframe containing the cells. required coord Tensor A tensor of shape (2,) containing the coordinates of the patch. required patch_size int The size of the patch. required cell_classes list [ str ] A list of cell classes. required Returns: Type Description GeoDataFrame A geodataframe containing the cells in the patch ordered by cell classes. Source code in src/prismtoolbox/wsiemb/emb_utils.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def get_cells_in_patch ( cells_df : gpd . GeoDataFrame , coord : torch . Tensor , patch_size : int , cell_classes : list [ str ], ) -> gpd . GeoDataFrame : \"\"\"Get the cells whose centroids are within the patch defined by the input coordinates and size. Args: cells_df: A geodataframe containing the cells. coord: A tensor of shape (2,) containing the coordinates of the patch. patch_size: The size of the patch. cell_classes: A list of cell classes. Returns: A geodataframe containing the cells in the patch ordered by cell classes. \"\"\" x , y = coord . numpy () patch_polygon = box ( x , y , x + patch_size , y + patch_size , ccw = False ) cells_in_patch = cells_df . loc [ cells_df . centroid . within ( patch_polygon ), ] cells_in_patch_grouped = [] for cell_class in cell_classes : if cell_class in cells_in_patch [ \"classification\" ] . values : cell_class_in_patch_df = cells_in_patch [ cells_in_patch [ \"classification\" ] == cell_class ] else : cell_class_in_patch_df = gpd . GeoDataFrame ( columns = cells_df . columns , data = [ pd . Series ({ col : None for col in cells_df . columns })], ) cell_class_in_patch_df [ \"classification\" ] = cell_class cells_in_patch_grouped . append ( cell_class_in_patch_df ) result = pd . concat ( cells_in_patch_grouped , ignore_index = True ) return result","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;get_cells_in_patch"},{"location":"reference/prismtoolbox/wsiemb/embedder/","text":"PatchEmbedder ( batch_size , num_workers , arch_name = None , custom_model = None , pretrained_weights = None , transforms_dict = None , device = 'cuda' , hug_login = None ) Bases: BasePatchHandler The PatchEmbedder class is used to extract embeddings from patches extracted as images in a folder. Parameters: Name Type Description Default batch_size int The batch size to use for the dataloader. required num_workers int The number of workers to use for the dataloader. required arch_name str | None The name of the architecture to use. See create_model for available architectures. None custom_model Module | None A custom model to use for creating the embeddings. Should accept a batch of patches as input of shape (batch_size, channels, height, width) and return the embeddings of shape (batch_size, embedding_dim). None pretrained_weights str | None The path to the pretrained weights or the name of the pretrained weights. See create_model for available weights for each architecture. None transforms_dict dict [ str , dict [ str , Any ]] | None The dictionary of transforms to use. See create_transforms for more information. If None, the pretrained transforms will be used. None device str The device to use for the model. 'cuda' hug_login str | None The login to use for the HuggingFace Hub (for Uni and Conch models). None Attributes: Name Type Description batch_size The batch size to use for the dataloader. num_workers The number of workers to use for the dataloader. transforms_dict The dictionary of transforms to use. device The device to use for the model. model The model to use for creating the embeddings. pretrained_transforms The transforms used when pretraining the model. embeddings A dictionary containing the extracted embeddings for each image. Source code in src/prismtoolbox/wsiemb/embedder.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 def __init__ ( self , batch_size : int , num_workers : int , arch_name : str | None = None , custom_model : torch . nn . Module | None = None , pretrained_weights : str | None = None , transforms_dict : dict [ str , dict [ str , Any ]] | None = None , device : str = \"cuda\" , hug_login : str | None = None , ): \"\"\"The PatchEmbedder class is used to extract embeddings from patches extracted as images in a folder. Args: batch_size: The batch size to use for the dataloader. num_workers: The number of workers to use for the dataloader. arch_name: The name of the architecture to use. See [create_model][prismtoolbox.wsiemb.emb_utils.create_model] for available architectures. custom_model: A custom model to use for creating the embeddings. Should accept a batch of patches as input of shape (batch_size, channels, height, width) and return the embeddings of shape (batch_size, embedding_dim). pretrained_weights: The path to the pretrained weights or the name of the pretrained weights. See [create_model][prismtoolbox.wsiemb.emb_utils.create_model] for available weights for each architecture. transforms_dict: The dictionary of transforms to use. See [create_transforms][prismtoolbox.utils.torch_utils.create_transforms] for more information. If None, the pretrained transforms will be used. device: The device to use for the model. hug_login: The login to use for the HuggingFace Hub (for Uni and Conch models). Attributes: batch_size: The batch size to use for the dataloader. num_workers: The number of workers to use for the dataloader. transforms_dict: The dictionary of transforms to use. device: The device to use for the model. model: The model to use for creating the embeddings. pretrained_transforms: The transforms used when pretraining the model. embeddings: A dictionary containing the extracted embeddings for each image. \"\"\" super () . __init__ ( batch_size , num_workers , transforms_dict ) self . device = device if hug_login is not None : from huggingface_hub import login login ( hug_login ) self . arch_name = arch_name if arch_name is not None : self . model , self . pretrained_transforms = create_model ( arch_name , pretrained_weights ) log . info ( f \"Model { arch_name } loaded with pretrained weights { pretrained_weights } .\" ) self . model . eval () self . model . to ( self . device ) elif custom_model is not None : self . model = custom_model self . pretrained_transforms = None self . arch_name = custom_model . __class__ . __name__ self . model . eval () self . model . to ( self . device ) else : log . warning ( \"No architecture name or custom model provided. Please provide an architecture name or a custom model.\" ) self . model = None self . pretrained_transforms = None self . embeddings = {} extract_embeddings ( img_folder , show_progress = True ) Extract embeddings from the images in the img_folder. Parameters: Name Type Description Default img_folder A folder containing a series of subfolders, each containing images. For example, img_folder could be a folder where the subfolders correspond to different slides. required show_progress bool Whether to show the progress bar. True Source code in src/prismtoolbox/wsiemb/embedder.py 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def extract_embeddings ( self , img_folder , show_progress : bool = True ): \"\"\"Extract embeddings from the images in the img_folder. Args: img_folder: A folder containing a series of subfolders, each containing images. For example, img_folder could be a folder where the subfolders correspond to different slides. show_progress: Whether to show the progress bar. \"\"\" assert self . model is not None , \"Model not found. Please provide an architecture name when initializing the PatchEmbedder.\" log . info ( f \"Extracting embeddings from images in { img_folder } .\" ) dataset = self . create_dataset ( img_folder = img_folder ) dataloader = self . create_dataloader ( dataset ) start_time = time . time () embeddings = [[] for _ in range ( len ( dataset . classes ))] img_ids = [] for i in range ( len ( dataset . classes )): img_ids . append ( np . array ( dataset . imgs )[ np . array ( dataset . targets ) == i ][:, 0 ]) for imgs , folder_id in tqdm ( dataloader , desc = f \"Extracting embeddings from images in { img_folder } \" , disable = not show_progress , ): imgs = imgs . to ( self . device ) with torch . no_grad (): output = self . model ( imgs ) for i in range ( len ( dataset . classes )): embeddings [ i ] . append ( output [ folder_id == i ] . cpu ()) log . info ( f \"Embedding time: { time . time () - start_time } .\" ) log . info ( f \"Extracted { len ( embeddings ) } from images in { img_folder } .\" ) for img_id , embedding in zip ( img_ids , embeddings ): self . embeddings [ img_id ] = torch . cat ( embedding , dim = 0 ) get_transforms () Get the transforms to use for creating the embeddings. Returns: Type Description The transforms to use when loading the patches. Source code in src/prismtoolbox/wsiemb/embedder.py 548 549 550 551 552 553 554 555 556 557 558 559 560 def get_transforms ( self ): \"\"\"Get the transforms to use for creating the embeddings. Returns: The transforms to use when loading the patches. \"\"\" if self . pretrained_transforms is None : log . info ( \"No pretrained transforms found, using transforms dict.\" ) super () . get_transforms () else : log . info ( \"No transforms dict found, using pretrained transforms.\" ) transforms = self . pretrained_transforms return transforms SlideEmbedder ( slide_dir , batch_size , num_workers , arch_name = None , custom_model = None , custom_model_name = None , pretrained_weights = None , transforms_dict = None , device = 'cuda' , engine = 'openslide' , coords_dir = None , patch_size = None , patch_level = None , patch_downsample = None , hug_login = None ) Bases: BaseSlideHandler The SlideEmbedder class is used to extract embeddings from patches extracted direclty from the slides. Parameters: Name Type Description Default slide_dir str The directory containing the slides. required batch_size int The batch size to use for the dataloader. required num_workers int The number of workers to use for the dataloader. required arch_name str | None The name of the architecture to use. See create_model for available architectures. None custom_model Module | None A custom model to use for creating the embeddings. Should accept a batch of patches as input of shape (batch_size, channels, height, width) and return the embeddings of shape (batch_size, embedding_dim). None custom_model_name str | None The name of the custom model (if used). None pretrained_weights str | None The path to the pretrained weights or the name of the pretrained weights. See create_model for available weights for each architecture. None transforms_dict dict [ str , dict [ str , Any ]] | None The dictionary of transforms to use. See create_transforms for more information. If None, the pretrained transforms will be used. None device str The device to use for the model. 'cuda' engine str The engine to use for reading the slides. 'openslide' coords_dir str | None The directory containing the coordinates of the patches as hdf5 files. If None, the patch_size and patch_level must be provided. None patch_size int | None The size of the patches. If None, it will be extracted from the hdf5 files. None patch_level int | None The level of the patches. If None, it will be extracted from the hdf5 files. None patch_downsample int | None The downsample of the patches. If None, it will be extracted from the hdf5 files. None hug_login str | None The login to use for the HuggingFace Hub (for Uni and Conch models). None Attributes: Name Type Description slide_dir The directory containing the slides. batch_size The batch size to use for the dataloader. num_workers The number of workers to use for the dataloader. transforms_dict The dictionary of transforms to use. device The device to use for the model. engine The engine to use for reading the slides. coords_dir The directory containing the coordinates of the patches as hdf5 files. patch_size The size of the patches. patch_level The level of the patches. patch_downsample The downsample of the patches. arch_name The name of the architecture to use. If custom_model was provided, the name of the custom model. If no custom_model_name was provided, the name of the custom model will be the class name of the custom model. model The model to use for creating the embeddings. pretrained_transforms The transforms used for the pretrained model. model_based_embeddings A dictionary containing the extracted embeddings for each slide with the pretrained model. stain_based_embeddings A dictionary containing the extracted embeddings for each slide with the stain based features. cell_based_embeddings A dictionary containing the extracted embeddings for each slide with the cell based features. model_based_embedding_names The names of the features extracted with the pretrained model. stain_based_embedding_names The names of the features extracted with the stain based features. cell_based_embedding_names The names of the features extracted with the cell based features. Source code in src/prismtoolbox/wsiemb/embedder.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def __init__ ( self , slide_dir : str , batch_size : int , num_workers : int , arch_name : str | None = None , custom_model : torch . nn . Module | None = None , custom_model_name : str | None = None , pretrained_weights : str | None = None , transforms_dict : dict [ str , dict [ str , Any ]] | None = None , device : str = \"cuda\" , engine : str = \"openslide\" , coords_dir : str | None = None , patch_size : int | None = None , patch_level : int | None = None , patch_downsample : int | None = None , hug_login : str | None = None , ): \"\"\"The SlideEmbedder class is used to extract embeddings from patches extracted direclty from the slides. Args: slide_dir: The directory containing the slides. batch_size: The batch size to use for the dataloader. num_workers: The number of workers to use for the dataloader. arch_name: The name of the architecture to use. See [create_model][prismtoolbox.wsiemb.emb_utils.create_model] for available architectures. custom_model: A custom model to use for creating the embeddings. Should accept a batch of patches as input of shape (batch_size, channels, height, width) and return the embeddings of shape (batch_size, embedding_dim). custom_model_name: The name of the custom model (if used). pretrained_weights: The path to the pretrained weights or the name of the pretrained weights. See [create_model][prismtoolbox.wsiemb.emb_utils.create_model] for available weights for each architecture. transforms_dict: The dictionary of transforms to use. See [create_transforms][prismtoolbox.utils.torch_utils.create_transforms] for more information. If None, the pretrained transforms will be used. device: The device to use for the model. engine: The engine to use for reading the slides. coords_dir: The directory containing the coordinates of the patches as hdf5 files. If None, the patch_size and patch_level must be provided. patch_size: The size of the patches. If None, it will be extracted from the hdf5 files. patch_level: The level of the patches. If None, it will be extracted from the hdf5 files. patch_downsample: The downsample of the patches. If None, it will be extracted from the hdf5 files. hug_login: The login to use for the HuggingFace Hub (for Uni and Conch models). Attributes: slide_dir: The directory containing the slides. batch_size: The batch size to use for the dataloader. num_workers: The number of workers to use for the dataloader. transforms_dict: The dictionary of transforms to use. device: The device to use for the model. engine: The engine to use for reading the slides. coords_dir: The directory containing the coordinates of the patches as hdf5 files. patch_size: The size of the patches. patch_level: The level of the patches. patch_downsample: The downsample of the patches. arch_name: The name of the architecture to use. If custom_model was provided, the name of the custom model. If no custom_model_name was provided, the name of the custom model will be the class name of the custom model. model: The model to use for creating the embeddings. pretrained_transforms: The transforms used for the pretrained model. model_based_embeddings: A dictionary containing the extracted embeddings for each slide with the pretrained model. stain_based_embeddings: A dictionary containing the extracted embeddings for each slide with the stain based features. cell_based_embeddings: A dictionary containing the extracted embeddings for each slide with the cell based features. model_based_embedding_names: The names of the features extracted with the pretrained model. stain_based_embedding_names: The names of the features extracted with the stain based features. cell_based_embedding_names: The names of the features extracted with the cell based features. \"\"\" super () . __init__ ( slide_dir , batch_size , num_workers , transforms_dict , engine , coords_dir , patch_size , patch_level , patch_downsample , ) if hug_login is not None : from huggingface_hub import login login ( hug_login ) self . device = device self . arch_name = arch_name if self . arch_name is not None : self . model , self . pretrained_transforms = create_model ( arch_name , pretrained_weights ) log . info ( f \"Model { self . arch_name } loaded with pretrained weights { pretrained_weights } .\" ) self . model . eval () self . model . to ( self . device ) elif custom_model is not None : self . model = custom_model self . pretrained_transforms = None self . arch_name = custom_model_name if custom_model_name is not None else custom_model . __class__ . __name__ self . model . eval () self . model . to ( self . device ) else : log . warning ( \"No architecture name or custom model provided. Please provide an architecture name or a custom model.\" ) self . model = None self . pretrained_transforms = None self . model_based_embeddings = {} self . stain_based_embeddings = {} self . cell_based_embeddings = {} self . model_based_embedding_names = [] self . stain_based_embedding_names = [] self . cell_based_embedding_names = [] extract_cell_based_embeddings ( slide_name , slide_ext , cells_path , coords = None , cell_classes = None , with_offset = True , show_progress = True ) Extract embeddings from the patches of a slide using the cell based features. Parameters: Name Type Description Default slide_name str The name of the slide to extract the embeddings from (without the extension). required slide_ext str The extension of the slide. required coords ndarray | None The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. None cells_path str The path to the cells geojson file. required cell_classes list [ str ] | None The classes of the cells to extract the embeddings from. If None, all the classes will be used. None with_offset bool Whether to offset the coordinates of the cells by the slide offset. True show_progress bool Whether to show the progress bar. True Source code in src/prismtoolbox/wsiemb/embedder.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 def extract_cell_based_embeddings ( self , slide_name : str , slide_ext : str , cells_path : str , coords : np . ndarray | None = None , cell_classes : list [ str ] | None = None , with_offset : bool = True , show_progress : bool = True , ): \"\"\"Extract embeddings from the patches of a slide using the cell based features. Args: slide_name: The name of the slide to extract the embeddings from (without the extension). slide_ext: The extension of the slide. coords: The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. cells_path: The path to the cells geojson file. cell_classes: The classes of the cells to extract the embeddings from. If None, all the classes will be used. with_offset: Whether to offset the coordinates of the cells by the slide offset. show_progress: Whether to show the progress bar. \"\"\" dataset = self . create_dataset ( slide_name , slide_ext = slide_ext , coords = coords , coords_only = True ) dataloader = self . create_dataloader ( dataset , num_workers = 1 ) patch_size = dataset . patch_size patch_downsample = dataset . downsample ref_patch_size = patch_size * patch_downsample offset = dataset . slide_offset if with_offset else ( 0 , 0 ) offset = ( - offset [ 0 ], - offset [ 1 ]) cells_df = read_json_with_geopandas ( cells_path , offset = offset ) if \"classification\" not in cells_df . columns : raise ValueError ( \"The 'classification' column is missing in the cells dataframe.\" ) if cell_classes is None : log . info ( \"No cell classes provided, using all unique classifications from the cells dataframe.\" ) cell_classes = list ( cells_df . classification . unique ()) else : log . info ( f \"Using the provided cell classes: { cell_classes } . If you want to use all unique classifications from the cells dataframe, set cell_classes to None.\" ) self . cell_based_embedding_names = [ f \" { cell_class } _ { feature } \" for cell_class in cell_classes for feature in CELL_FEATURE_NAMES ] start_time = time . time () embeddings = [] if self . num_workers > 1 : pool = mp . Pool ( self . num_workers ) for coords in tqdm ( dataloader , desc = f \"Extracting cell based features from the patches of { slide_name } \" , disable = not show_progress , ): if coords is None : log . warning ( f \"No coordinates found for slide { slide_name } . Skipping cell based embedding extraction.\" ) continue if self . num_workers > 1 : cells_df_in_patches = pool . starmap ( get_cells_in_patch , [( cells_df , coord , ref_patch_size , cell_classes ) for coord in coords ], ) cells_features = np . array ( pool . map ( compute_cell_features , cells_df_in_patches ) ) else : cells_df_in_patches = [ get_cells_in_patch ( cells_df , coord , ref_patch_size , cell_classes ) for coord in coords ] cells_features = np . array ( [ compute_cell_features ( cells_df_in_patch ) for cells_df_in_patch in cells_df_in_patches ] ) embeddings . append ( torch . tensor ( cells_features )) if self . num_workers > 1 : pool . close () self . cell_based_embeddings [ slide_name ] = torch . cat ( embeddings , dim = 0 ) log . info ( f \"Embedding time: { time . time () - start_time } .\" ) log . info ( f \"Extracted embeddings from { len ( embeddings ) } patches of { slide_name } .\" ) extract_model_based_embeddings ( slide_name , slide_ext , coords = None , show_progress = True ) Extract embeddings from the patches of a slide using the pretrained model. Parameters: Name Type Description Default slide_name str The name of the slide to extract the embeddings from (without the extension). required slide_ext str The extension of the slide. required coords ndarray | None The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. None show_progress bool Whether to show the progress bar. True Source code in src/prismtoolbox/wsiemb/embedder.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def extract_model_based_embeddings ( self , slide_name : str , slide_ext : str , coords : np . ndarray | None = None , show_progress : bool = True , ): \"\"\"Extract embeddings from the patches of a slide using the pretrained model. Args: slide_name: The name of the slide to extract the embeddings from (without the extension). slide_ext: The extension of the slide. coords: The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. show_progress: Whether to show the progress bar. \"\"\" assert ( self . model is not None ), \"Model not found. Please provide an architecture name when initializing the SlideEmbedder.\" log . info ( f \"Extracting embeddings from the patches of { slide_name } .\" ) dataset = self . create_dataset ( slide_name , slide_ext = slide_ext , coords = coords ) dataloader = self . create_dataloader ( dataset ) start_time = time . time () embeddings = [] for patches , _ in tqdm ( dataloader , desc = f \"Extracting embeddings from the patches of { slide_name } \" , disable = not show_progress , ): patches = patches . to ( self . device ) with torch . no_grad (): output = self . model ( patches ) embeddings . append ( output . cpu ()) log . info ( f \"Embedding time: { time . time () - start_time } .\" ) self . model_based_embeddings [ slide_name ] = torch . cat ( embeddings , dim = 0 ) embeddings_dim = self . model_based_embeddings [ slide_name ] . shape [ 1 ] self . model_based_embedding_names = [ f \" { self . arch_name } _ { i } \" for i in range ( embeddings_dim ) ] log . info ( f \"Extracted embeddings from { len ( embeddings ) } patches of { slide_name } .\" ) extract_stain_based_embeddings ( slide_name , slide_ext , coords = None , conv_matrix_name = 'HED' , show_progress = True ) Extract embeddings from the patches of a slide using the stain based features. Parameters: Name Type Description Default slide_name str The name of the slide to extract the embeddings from (without the extension). required slide_ext str The extension of the slide. required coords ndarray | None The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. None conv_matrix_name str The name of the convolutional matrix to use for the stain based features. See extract_stain_features for available matrices. 'HED' show_progress bool Whether to show the progress bar. True Source code in src/prismtoolbox/wsiemb/embedder.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def extract_stain_based_embeddings ( self , slide_name : str , slide_ext : str , coords : np . ndarray | None = None , conv_matrix_name : str = \"HED\" , show_progress : bool = True , ): \"\"\"Extract embeddings from the patches of a slide using the stain based features. Args: slide_name: The name of the slide to extract the embeddings from (without the extension). slide_ext: The extension of the slide. coords: The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. conv_matrix_name: The name of the convolutional matrix to use for the stain based features. See [extract_stain_features][prismtoolbox.wsiemb.emb_utils.extract_stain_features] for available matrices. show_progress: Whether to show the progress bar. \"\"\" log . info ( f \"Extracting embeddings from the patches of { slide_name } .\" ) dataset = self . create_dataset ( slide_name , slide_ext = slide_ext , coords = coords , no_transforms = True ) dataloader = self . create_dataloader ( dataset ) if conv_matrix_name == \"HED\" : stain_names = [ \"Hematoxylin\" , \"Eosin\" , \"DAB\" ] else : stain_names = [ \"Hematoxylin\" , \"DAB\" ] self . stain_based_embedding_names = [ f \" { stain_name } _ { feature } \" for feature in STAIN_FEATURE_NAMES for stain_name in stain_names ] start_time = time . time () embeddings = [] for patches , _ in tqdm ( dataloader , desc = f \"Extracting stain based features from the patches of { slide_name } \" , disable = not show_progress , ): embeddings . append ( extract_stain_features ( patches , conv_matrix_name )) log . info ( f \"Embedding time: { time . time () - start_time } .\" ) self . stain_based_embeddings [ slide_name ] = torch . cat ( embeddings , dim = 0 ) log . info ( f \"Extracted embeddings from { len ( embeddings ) } patches of { slide_name } .\" ) get_transforms () Get the transforms to use for creating the dataset. Returns: Type Description The transforms to use when loading the patches. Source code in src/prismtoolbox/wsiemb/embedder.py 150 151 152 153 154 155 156 157 158 159 160 161 162 def get_transforms ( self ): \"\"\"Get the transforms to use for creating the dataset. Returns: The transforms to use when loading the patches. \"\"\" if self . pretrained_transforms is None : log . info ( \"No pretrained transforms found, using transforms dict.\" ) transforms = super () . get_transforms () else : log . info ( \"No transforms dict found, using pretrained transforms.\" ) transforms = self . pretrained_transforms return transforms save_embeddings ( save_dir , flush_memory = False , format = 'pt' , merge = False ) Save the extracted embeddings to the chosen format under slide_name.format. Parameters: Name Type Description Default save_dir str The path to the directory where to save the embeddings. required flush_memory bool Whether to remove the embeddings from self.embeddings after saving. False format str The format to save the embeddings in. Possible formats: ['pt', 'npy'] 'pt' merge bool Whether to merge the different types of embeddings before saving. False Source code in src/prismtoolbox/wsiemb/embedder.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def save_embeddings ( self , save_dir : str , flush_memory : bool = False , format : str = \"pt\" , merge : bool = False , ): \"\"\"Save the extracted embeddings to the chosen format under slide_name.format. Args: save_dir: The path to the directory where to save the embeddings. flush_memory: Whether to remove the embeddings from self.embeddings after saving. format: The format to save the embeddings in. Possible formats: ['pt', 'npy'] merge: Whether to merge the different types of embeddings before saving. \"\"\" if not os . path . isdir ( save_dir ): log . warning ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if format not in [ \"pt\" , \"npy\" ]: raise ValueError ( \"invalid format, possible formats: ['pt', 'npy']\" ) slides_processed = set ( self . model_based_embeddings . keys ()) . union ( self . stain_based_embeddings . keys (), self . cell_based_embeddings . keys () ) for slide_name in slides_processed : if merge : embeddings_list = [ ( \"\" , torch . cat ( [ emb_dict [ slide_name ] for emb_dict in [ self . model_based_embeddings , self . stain_based_embeddings , self . cell_based_embeddings , ] if slide_name in emb_dict . keys () ], dim = 1 , ), ) ] else : embeddings_list = [ ( emb_type , emb_dict [ slide_name ]) for emb_type , emb_dict in zip ( [ \"_model_based\" , \"_stain_based\" , \"_cell_based\" ], [ self . model_based_embeddings , self . stain_based_embeddings , self . cell_based_embeddings , ], ) if slide_name in emb_dict . keys () ] for emb_type , emb in embeddings_list : output_path = os . path . join ( save_dir , f \" { slide_name + emb_type } . { format } \" ) if format == \"pt\" : torch . save ( emb , output_path ) elif format == \"npy\" : emb = emb . numpy () np . save ( output_path , emb ) else : raise NotImplementedError log . info ( f \"Embeddings for slide { slide_name } saved at { output_path } .\" ) if flush_memory : self . model_based_embeddings = {} self . stain_based_embeddings = {} self . cell_based_embeddings = {} log . info ( \"Memory flushed.\" ) save_embeddings_names ( save_dir , format = 'csv' , flush_memory = False , merge = False ) Save the extracted embeddings names to the chosen format under slide_name.format. Parameters: Name Type Description Default save_dir str The path to the directory where to save the embeddings. required format str The format to save the embeddings in. Possible formats: ['csv'] 'csv' flush_memory bool Whether to remove the reset the embeddings names after saving. False merge bool Whether the embeddings were merged before saving. False Source code in src/prismtoolbox/wsiemb/embedder.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def save_embeddings_names ( self , save_dir : str , format : str = \"csv\" , flush_memory : bool = False , merge : bool = False , ): \"\"\"Save the extracted embeddings names to the chosen format under slide_name.format. Args: save_dir: The path to the directory where to save the embeddings. format: The format to save the embeddings in. Possible formats: ['csv'] flush_memory: Whether to remove the reset the embeddings names after saving. merge: Whether the embeddings were merged before saving. \"\"\" if not os . path . isdir ( save_dir ): log . warning ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if format not in [ \"csv\" ]: raise ValueError ( \"invalid format, possible formats: ['csv']\" ) if merge : output_path = os . path . join ( save_dir , f \"embeddings_names. { format } \" ) if os . path . exists ( output_path ): log . warning ( f \"File embeddings_names. { format } already exists in { save_dir } . Overwriting...\" ) embeddings_names = ( self . model_based_embedding_names + self . stain_based_embedding_names + self . cell_based_embedding_names ) df = pd . DataFrame ( embeddings_names , columns = [ \"feature_names\" ]) df . to_csv ( output_path , index = False ) else : for emb_type , emb_names in zip ( [ \"model_based\" , \"stain_based\" , \"cell_based\" ], [ self . model_based_embedding_names , self . stain_based_embedding_names , self . cell_based_embedding_names , ], ): if len ( emb_names ) == 0 : continue output_path = os . path . join ( save_dir , f \"embeddings_names_ { emb_type } . { format } \" ) if os . path . exists ( output_path ): log . warning ( f \"File embeddings_names_ { emb_type } . { format } already exists in { save_dir } . Overwriting...\" ) df = pd . DataFrame ( emb_names , columns = [ \"feature_names\" ]) df . to_csv ( output_path , index = False ) log . info ( f \"Embeddings names saved at { output_path } .\" ) if flush_memory : self . model_based_embedding_names = [] self . stain_based_embedding_names = [] self . cell_based_embedding_names = [] log . info ( \"Memory flushed.\" )","title":"prismtoolbox.wsiemb.embedder"},{"location":"reference/prismtoolbox/wsiemb/embedder/#prismtoolbox.wsiemb.embedder.PatchEmbedder","text":"Bases: BasePatchHandler The PatchEmbedder class is used to extract embeddings from patches extracted as images in a folder. Parameters: Name Type Description Default batch_size int The batch size to use for the dataloader. required num_workers int The number of workers to use for the dataloader. required arch_name str | None The name of the architecture to use. See create_model for available architectures. None custom_model Module | None A custom model to use for creating the embeddings. Should accept a batch of patches as input of shape (batch_size, channels, height, width) and return the embeddings of shape (batch_size, embedding_dim). None pretrained_weights str | None The path to the pretrained weights or the name of the pretrained weights. See create_model for available weights for each architecture. None transforms_dict dict [ str , dict [ str , Any ]] | None The dictionary of transforms to use. See create_transforms for more information. If None, the pretrained transforms will be used. None device str The device to use for the model. 'cuda' hug_login str | None The login to use for the HuggingFace Hub (for Uni and Conch models). None Attributes: Name Type Description batch_size The batch size to use for the dataloader. num_workers The number of workers to use for the dataloader. transforms_dict The dictionary of transforms to use. device The device to use for the model. model The model to use for creating the embeddings. pretrained_transforms The transforms used when pretraining the model. embeddings A dictionary containing the extracted embeddings for each image. Source code in src/prismtoolbox/wsiemb/embedder.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 def __init__ ( self , batch_size : int , num_workers : int , arch_name : str | None = None , custom_model : torch . nn . Module | None = None , pretrained_weights : str | None = None , transforms_dict : dict [ str , dict [ str , Any ]] | None = None , device : str = \"cuda\" , hug_login : str | None = None , ): \"\"\"The PatchEmbedder class is used to extract embeddings from patches extracted as images in a folder. Args: batch_size: The batch size to use for the dataloader. num_workers: The number of workers to use for the dataloader. arch_name: The name of the architecture to use. See [create_model][prismtoolbox.wsiemb.emb_utils.create_model] for available architectures. custom_model: A custom model to use for creating the embeddings. Should accept a batch of patches as input of shape (batch_size, channels, height, width) and return the embeddings of shape (batch_size, embedding_dim). pretrained_weights: The path to the pretrained weights or the name of the pretrained weights. See [create_model][prismtoolbox.wsiemb.emb_utils.create_model] for available weights for each architecture. transforms_dict: The dictionary of transforms to use. See [create_transforms][prismtoolbox.utils.torch_utils.create_transforms] for more information. If None, the pretrained transforms will be used. device: The device to use for the model. hug_login: The login to use for the HuggingFace Hub (for Uni and Conch models). Attributes: batch_size: The batch size to use for the dataloader. num_workers: The number of workers to use for the dataloader. transforms_dict: The dictionary of transforms to use. device: The device to use for the model. model: The model to use for creating the embeddings. pretrained_transforms: The transforms used when pretraining the model. embeddings: A dictionary containing the extracted embeddings for each image. \"\"\" super () . __init__ ( batch_size , num_workers , transforms_dict ) self . device = device if hug_login is not None : from huggingface_hub import login login ( hug_login ) self . arch_name = arch_name if arch_name is not None : self . model , self . pretrained_transforms = create_model ( arch_name , pretrained_weights ) log . info ( f \"Model { arch_name } loaded with pretrained weights { pretrained_weights } .\" ) self . model . eval () self . model . to ( self . device ) elif custom_model is not None : self . model = custom_model self . pretrained_transforms = None self . arch_name = custom_model . __class__ . __name__ self . model . eval () self . model . to ( self . device ) else : log . warning ( \"No architecture name or custom model provided. Please provide an architecture name or a custom model.\" ) self . model = None self . pretrained_transforms = None self . embeddings = {}","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;PatchEmbedder"},{"location":"reference/prismtoolbox/wsiemb/embedder/#prismtoolbox.wsiemb.embedder.PatchEmbedder.extract_embeddings","text":"Extract embeddings from the images in the img_folder. Parameters: Name Type Description Default img_folder A folder containing a series of subfolders, each containing images. For example, img_folder could be a folder where the subfolders correspond to different slides. required show_progress bool Whether to show the progress bar. True Source code in src/prismtoolbox/wsiemb/embedder.py 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def extract_embeddings ( self , img_folder , show_progress : bool = True ): \"\"\"Extract embeddings from the images in the img_folder. Args: img_folder: A folder containing a series of subfolders, each containing images. For example, img_folder could be a folder where the subfolders correspond to different slides. show_progress: Whether to show the progress bar. \"\"\" assert self . model is not None , \"Model not found. Please provide an architecture name when initializing the PatchEmbedder.\" log . info ( f \"Extracting embeddings from images in { img_folder } .\" ) dataset = self . create_dataset ( img_folder = img_folder ) dataloader = self . create_dataloader ( dataset ) start_time = time . time () embeddings = [[] for _ in range ( len ( dataset . classes ))] img_ids = [] for i in range ( len ( dataset . classes )): img_ids . append ( np . array ( dataset . imgs )[ np . array ( dataset . targets ) == i ][:, 0 ]) for imgs , folder_id in tqdm ( dataloader , desc = f \"Extracting embeddings from images in { img_folder } \" , disable = not show_progress , ): imgs = imgs . to ( self . device ) with torch . no_grad (): output = self . model ( imgs ) for i in range ( len ( dataset . classes )): embeddings [ i ] . append ( output [ folder_id == i ] . cpu ()) log . info ( f \"Embedding time: { time . time () - start_time } .\" ) log . info ( f \"Extracted { len ( embeddings ) } from images in { img_folder } .\" ) for img_id , embedding in zip ( img_ids , embeddings ): self . embeddings [ img_id ] = torch . cat ( embedding , dim = 0 )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;extract_embeddings"},{"location":"reference/prismtoolbox/wsiemb/embedder/#prismtoolbox.wsiemb.embedder.PatchEmbedder.get_transforms","text":"Get the transforms to use for creating the embeddings. Returns: Type Description The transforms to use when loading the patches. Source code in src/prismtoolbox/wsiemb/embedder.py 548 549 550 551 552 553 554 555 556 557 558 559 560 def get_transforms ( self ): \"\"\"Get the transforms to use for creating the embeddings. Returns: The transforms to use when loading the patches. \"\"\" if self . pretrained_transforms is None : log . info ( \"No pretrained transforms found, using transforms dict.\" ) super () . get_transforms () else : log . info ( \"No transforms dict found, using pretrained transforms.\" ) transforms = self . pretrained_transforms return transforms","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_transforms"},{"location":"reference/prismtoolbox/wsiemb/embedder/#prismtoolbox.wsiemb.embedder.SlideEmbedder","text":"Bases: BaseSlideHandler The SlideEmbedder class is used to extract embeddings from patches extracted direclty from the slides. Parameters: Name Type Description Default slide_dir str The directory containing the slides. required batch_size int The batch size to use for the dataloader. required num_workers int The number of workers to use for the dataloader. required arch_name str | None The name of the architecture to use. See create_model for available architectures. None custom_model Module | None A custom model to use for creating the embeddings. Should accept a batch of patches as input of shape (batch_size, channels, height, width) and return the embeddings of shape (batch_size, embedding_dim). None custom_model_name str | None The name of the custom model (if used). None pretrained_weights str | None The path to the pretrained weights or the name of the pretrained weights. See create_model for available weights for each architecture. None transforms_dict dict [ str , dict [ str , Any ]] | None The dictionary of transforms to use. See create_transforms for more information. If None, the pretrained transforms will be used. None device str The device to use for the model. 'cuda' engine str The engine to use for reading the slides. 'openslide' coords_dir str | None The directory containing the coordinates of the patches as hdf5 files. If None, the patch_size and patch_level must be provided. None patch_size int | None The size of the patches. If None, it will be extracted from the hdf5 files. None patch_level int | None The level of the patches. If None, it will be extracted from the hdf5 files. None patch_downsample int | None The downsample of the patches. If None, it will be extracted from the hdf5 files. None hug_login str | None The login to use for the HuggingFace Hub (for Uni and Conch models). None Attributes: Name Type Description slide_dir The directory containing the slides. batch_size The batch size to use for the dataloader. num_workers The number of workers to use for the dataloader. transforms_dict The dictionary of transforms to use. device The device to use for the model. engine The engine to use for reading the slides. coords_dir The directory containing the coordinates of the patches as hdf5 files. patch_size The size of the patches. patch_level The level of the patches. patch_downsample The downsample of the patches. arch_name The name of the architecture to use. If custom_model was provided, the name of the custom model. If no custom_model_name was provided, the name of the custom model will be the class name of the custom model. model The model to use for creating the embeddings. pretrained_transforms The transforms used for the pretrained model. model_based_embeddings A dictionary containing the extracted embeddings for each slide with the pretrained model. stain_based_embeddings A dictionary containing the extracted embeddings for each slide with the stain based features. cell_based_embeddings A dictionary containing the extracted embeddings for each slide with the cell based features. model_based_embedding_names The names of the features extracted with the pretrained model. stain_based_embedding_names The names of the features extracted with the stain based features. cell_based_embedding_names The names of the features extracted with the cell based features. Source code in src/prismtoolbox/wsiemb/embedder.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def __init__ ( self , slide_dir : str , batch_size : int , num_workers : int , arch_name : str | None = None , custom_model : torch . nn . Module | None = None , custom_model_name : str | None = None , pretrained_weights : str | None = None , transforms_dict : dict [ str , dict [ str , Any ]] | None = None , device : str = \"cuda\" , engine : str = \"openslide\" , coords_dir : str | None = None , patch_size : int | None = None , patch_level : int | None = None , patch_downsample : int | None = None , hug_login : str | None = None , ): \"\"\"The SlideEmbedder class is used to extract embeddings from patches extracted direclty from the slides. Args: slide_dir: The directory containing the slides. batch_size: The batch size to use for the dataloader. num_workers: The number of workers to use for the dataloader. arch_name: The name of the architecture to use. See [create_model][prismtoolbox.wsiemb.emb_utils.create_model] for available architectures. custom_model: A custom model to use for creating the embeddings. Should accept a batch of patches as input of shape (batch_size, channels, height, width) and return the embeddings of shape (batch_size, embedding_dim). custom_model_name: The name of the custom model (if used). pretrained_weights: The path to the pretrained weights or the name of the pretrained weights. See [create_model][prismtoolbox.wsiemb.emb_utils.create_model] for available weights for each architecture. transforms_dict: The dictionary of transforms to use. See [create_transforms][prismtoolbox.utils.torch_utils.create_transforms] for more information. If None, the pretrained transforms will be used. device: The device to use for the model. engine: The engine to use for reading the slides. coords_dir: The directory containing the coordinates of the patches as hdf5 files. If None, the patch_size and patch_level must be provided. patch_size: The size of the patches. If None, it will be extracted from the hdf5 files. patch_level: The level of the patches. If None, it will be extracted from the hdf5 files. patch_downsample: The downsample of the patches. If None, it will be extracted from the hdf5 files. hug_login: The login to use for the HuggingFace Hub (for Uni and Conch models). Attributes: slide_dir: The directory containing the slides. batch_size: The batch size to use for the dataloader. num_workers: The number of workers to use for the dataloader. transforms_dict: The dictionary of transforms to use. device: The device to use for the model. engine: The engine to use for reading the slides. coords_dir: The directory containing the coordinates of the patches as hdf5 files. patch_size: The size of the patches. patch_level: The level of the patches. patch_downsample: The downsample of the patches. arch_name: The name of the architecture to use. If custom_model was provided, the name of the custom model. If no custom_model_name was provided, the name of the custom model will be the class name of the custom model. model: The model to use for creating the embeddings. pretrained_transforms: The transforms used for the pretrained model. model_based_embeddings: A dictionary containing the extracted embeddings for each slide with the pretrained model. stain_based_embeddings: A dictionary containing the extracted embeddings for each slide with the stain based features. cell_based_embeddings: A dictionary containing the extracted embeddings for each slide with the cell based features. model_based_embedding_names: The names of the features extracted with the pretrained model. stain_based_embedding_names: The names of the features extracted with the stain based features. cell_based_embedding_names: The names of the features extracted with the cell based features. \"\"\" super () . __init__ ( slide_dir , batch_size , num_workers , transforms_dict , engine , coords_dir , patch_size , patch_level , patch_downsample , ) if hug_login is not None : from huggingface_hub import login login ( hug_login ) self . device = device self . arch_name = arch_name if self . arch_name is not None : self . model , self . pretrained_transforms = create_model ( arch_name , pretrained_weights ) log . info ( f \"Model { self . arch_name } loaded with pretrained weights { pretrained_weights } .\" ) self . model . eval () self . model . to ( self . device ) elif custom_model is not None : self . model = custom_model self . pretrained_transforms = None self . arch_name = custom_model_name if custom_model_name is not None else custom_model . __class__ . __name__ self . model . eval () self . model . to ( self . device ) else : log . warning ( \"No architecture name or custom model provided. Please provide an architecture name or a custom model.\" ) self . model = None self . pretrained_transforms = None self . model_based_embeddings = {} self . stain_based_embeddings = {} self . cell_based_embeddings = {} self . model_based_embedding_names = [] self . stain_based_embedding_names = [] self . cell_based_embedding_names = []","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;SlideEmbedder"},{"location":"reference/prismtoolbox/wsiemb/embedder/#prismtoolbox.wsiemb.embedder.SlideEmbedder.extract_cell_based_embeddings","text":"Extract embeddings from the patches of a slide using the cell based features. Parameters: Name Type Description Default slide_name str The name of the slide to extract the embeddings from (without the extension). required slide_ext str The extension of the slide. required coords ndarray | None The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. None cells_path str The path to the cells geojson file. required cell_classes list [ str ] | None The classes of the cells to extract the embeddings from. If None, all the classes will be used. None with_offset bool Whether to offset the coordinates of the cells by the slide offset. True show_progress bool Whether to show the progress bar. True Source code in src/prismtoolbox/wsiemb/embedder.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 def extract_cell_based_embeddings ( self , slide_name : str , slide_ext : str , cells_path : str , coords : np . ndarray | None = None , cell_classes : list [ str ] | None = None , with_offset : bool = True , show_progress : bool = True , ): \"\"\"Extract embeddings from the patches of a slide using the cell based features. Args: slide_name: The name of the slide to extract the embeddings from (without the extension). slide_ext: The extension of the slide. coords: The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. cells_path: The path to the cells geojson file. cell_classes: The classes of the cells to extract the embeddings from. If None, all the classes will be used. with_offset: Whether to offset the coordinates of the cells by the slide offset. show_progress: Whether to show the progress bar. \"\"\" dataset = self . create_dataset ( slide_name , slide_ext = slide_ext , coords = coords , coords_only = True ) dataloader = self . create_dataloader ( dataset , num_workers = 1 ) patch_size = dataset . patch_size patch_downsample = dataset . downsample ref_patch_size = patch_size * patch_downsample offset = dataset . slide_offset if with_offset else ( 0 , 0 ) offset = ( - offset [ 0 ], - offset [ 1 ]) cells_df = read_json_with_geopandas ( cells_path , offset = offset ) if \"classification\" not in cells_df . columns : raise ValueError ( \"The 'classification' column is missing in the cells dataframe.\" ) if cell_classes is None : log . info ( \"No cell classes provided, using all unique classifications from the cells dataframe.\" ) cell_classes = list ( cells_df . classification . unique ()) else : log . info ( f \"Using the provided cell classes: { cell_classes } . If you want to use all unique classifications from the cells dataframe, set cell_classes to None.\" ) self . cell_based_embedding_names = [ f \" { cell_class } _ { feature } \" for cell_class in cell_classes for feature in CELL_FEATURE_NAMES ] start_time = time . time () embeddings = [] if self . num_workers > 1 : pool = mp . Pool ( self . num_workers ) for coords in tqdm ( dataloader , desc = f \"Extracting cell based features from the patches of { slide_name } \" , disable = not show_progress , ): if coords is None : log . warning ( f \"No coordinates found for slide { slide_name } . Skipping cell based embedding extraction.\" ) continue if self . num_workers > 1 : cells_df_in_patches = pool . starmap ( get_cells_in_patch , [( cells_df , coord , ref_patch_size , cell_classes ) for coord in coords ], ) cells_features = np . array ( pool . map ( compute_cell_features , cells_df_in_patches ) ) else : cells_df_in_patches = [ get_cells_in_patch ( cells_df , coord , ref_patch_size , cell_classes ) for coord in coords ] cells_features = np . array ( [ compute_cell_features ( cells_df_in_patch ) for cells_df_in_patch in cells_df_in_patches ] ) embeddings . append ( torch . tensor ( cells_features )) if self . num_workers > 1 : pool . close () self . cell_based_embeddings [ slide_name ] = torch . cat ( embeddings , dim = 0 ) log . info ( f \"Embedding time: { time . time () - start_time } .\" ) log . info ( f \"Extracted embeddings from { len ( embeddings ) } patches of { slide_name } .\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;extract_cell_based_embeddings"},{"location":"reference/prismtoolbox/wsiemb/embedder/#prismtoolbox.wsiemb.embedder.SlideEmbedder.extract_model_based_embeddings","text":"Extract embeddings from the patches of a slide using the pretrained model. Parameters: Name Type Description Default slide_name str The name of the slide to extract the embeddings from (without the extension). required slide_ext str The extension of the slide. required coords ndarray | None The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. None show_progress bool Whether to show the progress bar. True Source code in src/prismtoolbox/wsiemb/embedder.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def extract_model_based_embeddings ( self , slide_name : str , slide_ext : str , coords : np . ndarray | None = None , show_progress : bool = True , ): \"\"\"Extract embeddings from the patches of a slide using the pretrained model. Args: slide_name: The name of the slide to extract the embeddings from (without the extension). slide_ext: The extension of the slide. coords: The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. show_progress: Whether to show the progress bar. \"\"\" assert ( self . model is not None ), \"Model not found. Please provide an architecture name when initializing the SlideEmbedder.\" log . info ( f \"Extracting embeddings from the patches of { slide_name } .\" ) dataset = self . create_dataset ( slide_name , slide_ext = slide_ext , coords = coords ) dataloader = self . create_dataloader ( dataset ) start_time = time . time () embeddings = [] for patches , _ in tqdm ( dataloader , desc = f \"Extracting embeddings from the patches of { slide_name } \" , disable = not show_progress , ): patches = patches . to ( self . device ) with torch . no_grad (): output = self . model ( patches ) embeddings . append ( output . cpu ()) log . info ( f \"Embedding time: { time . time () - start_time } .\" ) self . model_based_embeddings [ slide_name ] = torch . cat ( embeddings , dim = 0 ) embeddings_dim = self . model_based_embeddings [ slide_name ] . shape [ 1 ] self . model_based_embedding_names = [ f \" { self . arch_name } _ { i } \" for i in range ( embeddings_dim ) ] log . info ( f \"Extracted embeddings from { len ( embeddings ) } patches of { slide_name } .\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;extract_model_based_embeddings"},{"location":"reference/prismtoolbox/wsiemb/embedder/#prismtoolbox.wsiemb.embedder.SlideEmbedder.extract_stain_based_embeddings","text":"Extract embeddings from the patches of a slide using the stain based features. Parameters: Name Type Description Default slide_name str The name of the slide to extract the embeddings from (without the extension). required slide_ext str The extension of the slide. required coords ndarray | None The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. None conv_matrix_name str The name of the convolutional matrix to use for the stain based features. See extract_stain_features for available matrices. 'HED' show_progress bool Whether to show the progress bar. True Source code in src/prismtoolbox/wsiemb/embedder.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def extract_stain_based_embeddings ( self , slide_name : str , slide_ext : str , coords : np . ndarray | None = None , conv_matrix_name : str = \"HED\" , show_progress : bool = True , ): \"\"\"Extract embeddings from the patches of a slide using the stain based features. Args: slide_name: The name of the slide to extract the embeddings from (without the extension). slide_ext: The extension of the slide. coords: The coordinates of the patches to extract the embeddings from. If None, the coordinates will be loaded from the hdf5 file located in coords_dir. conv_matrix_name: The name of the convolutional matrix to use for the stain based features. See [extract_stain_features][prismtoolbox.wsiemb.emb_utils.extract_stain_features] for available matrices. show_progress: Whether to show the progress bar. \"\"\" log . info ( f \"Extracting embeddings from the patches of { slide_name } .\" ) dataset = self . create_dataset ( slide_name , slide_ext = slide_ext , coords = coords , no_transforms = True ) dataloader = self . create_dataloader ( dataset ) if conv_matrix_name == \"HED\" : stain_names = [ \"Hematoxylin\" , \"Eosin\" , \"DAB\" ] else : stain_names = [ \"Hematoxylin\" , \"DAB\" ] self . stain_based_embedding_names = [ f \" { stain_name } _ { feature } \" for feature in STAIN_FEATURE_NAMES for stain_name in stain_names ] start_time = time . time () embeddings = [] for patches , _ in tqdm ( dataloader , desc = f \"Extracting stain based features from the patches of { slide_name } \" , disable = not show_progress , ): embeddings . append ( extract_stain_features ( patches , conv_matrix_name )) log . info ( f \"Embedding time: { time . time () - start_time } .\" ) self . stain_based_embeddings [ slide_name ] = torch . cat ( embeddings , dim = 0 ) log . info ( f \"Extracted embeddings from { len ( embeddings ) } patches of { slide_name } .\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;extract_stain_based_embeddings"},{"location":"reference/prismtoolbox/wsiemb/embedder/#prismtoolbox.wsiemb.embedder.SlideEmbedder.get_transforms","text":"Get the transforms to use for creating the dataset. Returns: Type Description The transforms to use when loading the patches. Source code in src/prismtoolbox/wsiemb/embedder.py 150 151 152 153 154 155 156 157 158 159 160 161 162 def get_transforms ( self ): \"\"\"Get the transforms to use for creating the dataset. Returns: The transforms to use when loading the patches. \"\"\" if self . pretrained_transforms is None : log . info ( \"No pretrained transforms found, using transforms dict.\" ) transforms = super () . get_transforms () else : log . info ( \"No transforms dict found, using pretrained transforms.\" ) transforms = self . pretrained_transforms return transforms","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_transforms"},{"location":"reference/prismtoolbox/wsiemb/embedder/#prismtoolbox.wsiemb.embedder.SlideEmbedder.save_embeddings","text":"Save the extracted embeddings to the chosen format under slide_name.format. Parameters: Name Type Description Default save_dir str The path to the directory where to save the embeddings. required flush_memory bool Whether to remove the embeddings from self.embeddings after saving. False format str The format to save the embeddings in. Possible formats: ['pt', 'npy'] 'pt' merge bool Whether to merge the different types of embeddings before saving. False Source code in src/prismtoolbox/wsiemb/embedder.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def save_embeddings ( self , save_dir : str , flush_memory : bool = False , format : str = \"pt\" , merge : bool = False , ): \"\"\"Save the extracted embeddings to the chosen format under slide_name.format. Args: save_dir: The path to the directory where to save the embeddings. flush_memory: Whether to remove the embeddings from self.embeddings after saving. format: The format to save the embeddings in. Possible formats: ['pt', 'npy'] merge: Whether to merge the different types of embeddings before saving. \"\"\" if not os . path . isdir ( save_dir ): log . warning ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if format not in [ \"pt\" , \"npy\" ]: raise ValueError ( \"invalid format, possible formats: ['pt', 'npy']\" ) slides_processed = set ( self . model_based_embeddings . keys ()) . union ( self . stain_based_embeddings . keys (), self . cell_based_embeddings . keys () ) for slide_name in slides_processed : if merge : embeddings_list = [ ( \"\" , torch . cat ( [ emb_dict [ slide_name ] for emb_dict in [ self . model_based_embeddings , self . stain_based_embeddings , self . cell_based_embeddings , ] if slide_name in emb_dict . keys () ], dim = 1 , ), ) ] else : embeddings_list = [ ( emb_type , emb_dict [ slide_name ]) for emb_type , emb_dict in zip ( [ \"_model_based\" , \"_stain_based\" , \"_cell_based\" ], [ self . model_based_embeddings , self . stain_based_embeddings , self . cell_based_embeddings , ], ) if slide_name in emb_dict . keys () ] for emb_type , emb in embeddings_list : output_path = os . path . join ( save_dir , f \" { slide_name + emb_type } . { format } \" ) if format == \"pt\" : torch . save ( emb , output_path ) elif format == \"npy\" : emb = emb . numpy () np . save ( output_path , emb ) else : raise NotImplementedError log . info ( f \"Embeddings for slide { slide_name } saved at { output_path } .\" ) if flush_memory : self . model_based_embeddings = {} self . stain_based_embeddings = {} self . cell_based_embeddings = {} log . info ( \"Memory flushed.\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;save_embeddings"},{"location":"reference/prismtoolbox/wsiemb/embedder/#prismtoolbox.wsiemb.embedder.SlideEmbedder.save_embeddings_names","text":"Save the extracted embeddings names to the chosen format under slide_name.format. Parameters: Name Type Description Default save_dir str The path to the directory where to save the embeddings. required format str The format to save the embeddings in. Possible formats: ['csv'] 'csv' flush_memory bool Whether to remove the reset the embeddings names after saving. False merge bool Whether the embeddings were merged before saving. False Source code in src/prismtoolbox/wsiemb/embedder.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def save_embeddings_names ( self , save_dir : str , format : str = \"csv\" , flush_memory : bool = False , merge : bool = False , ): \"\"\"Save the extracted embeddings names to the chosen format under slide_name.format. Args: save_dir: The path to the directory where to save the embeddings. format: The format to save the embeddings in. Possible formats: ['csv'] flush_memory: Whether to remove the reset the embeddings names after saving. merge: Whether the embeddings were merged before saving. \"\"\" if not os . path . isdir ( save_dir ): log . warning ( f \"Folder { save_dir } does not exist, creating new folder...\" ) pathlib . Path ( save_dir ) . mkdir ( parents = True , exist_ok = True ) if format not in [ \"csv\" ]: raise ValueError ( \"invalid format, possible formats: ['csv']\" ) if merge : output_path = os . path . join ( save_dir , f \"embeddings_names. { format } \" ) if os . path . exists ( output_path ): log . warning ( f \"File embeddings_names. { format } already exists in { save_dir } . Overwriting...\" ) embeddings_names = ( self . model_based_embedding_names + self . stain_based_embedding_names + self . cell_based_embedding_names ) df = pd . DataFrame ( embeddings_names , columns = [ \"feature_names\" ]) df . to_csv ( output_path , index = False ) else : for emb_type , emb_names in zip ( [ \"model_based\" , \"stain_based\" , \"cell_based\" ], [ self . model_based_embedding_names , self . stain_based_embedding_names , self . cell_based_embedding_names , ], ): if len ( emb_names ) == 0 : continue output_path = os . path . join ( save_dir , f \"embeddings_names_ { emb_type } . { format } \" ) if os . path . exists ( output_path ): log . warning ( f \"File embeddings_names_ { emb_type } . { format } already exists in { save_dir } . Overwriting...\" ) df = pd . DataFrame ( emb_names , columns = [ \"feature_names\" ]) df . to_csv ( output_path , index = False ) log . info ( f \"Embeddings names saved at { output_path } .\" ) if flush_memory : self . model_based_embedding_names = [] self . stain_based_embedding_names = [] self . cell_based_embedding_names = [] log . info ( \"Memory flushed.\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;save_embeddings_names"},{"location":"reference/prismtoolbox/wsiemb/processing/","text":"EmbeddingProcessor ( embeddings , embeddings_names = None , slide_ids = None , cmap = 'Set1' , seed = 123 ) summary Parameters: Name Type Description Default embeddings list [ ndarray | Tensor | str ] The embeddings to process. Can be a list of numpy arrays, torch tensors or paths to embeddings. required embeddings_names list [ str ] | str | None The names of the embeddings. Should be a list of strings with the same length as the embeddings, None slide_ids list [ str ] | None The ids of the slides. None cmap str The colormap to use for visualizations. 'Set1' seed int The seed to use for reproducibility. 123 Attributes: Name Type Description embeddings The embeddings to process as a list of numpy arrays. embeddings_matrix The embeddings to process concatenated into a single numpy array. embeddings_stats The statistics of the embeddings (mean, std, min, max). slide_ids The ids of the slides. If not provided, the slide ids are generated. slide_ids_matrix The slide ids of each embedding concatenated into a single numpy array. cmap The colormap to use for visualizations. seed The seed to use for reproducibility. cluster_model The clustering model used to cluster the embeddings. It is set using the n_clusters The number of clusters in the clustering model. It is set using the cluster_colors The colors of the clusters in the clustering model. It is set using the eps A small value to avoid division by zero. Source code in src/prismtoolbox/wsiemb/processing.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , embeddings : list [ np . ndarray | torch . Tensor | str ], embeddings_names : list [ str ] | str | None = None , slide_ids : list [ str ] | None = None , cmap : str = \"Set1\" , seed : int = 123 , ): \"\"\"_summary_ Args: embeddings: The embeddings to process. Can be a list of numpy arrays, torch tensors or paths to embeddings. embeddings_names: The names of the embeddings. Should be a list of strings with the same length as the embeddings, or the path to a file containing the names. If not provided, the names are generated. slide_ids: The ids of the slides. cmap: The colormap to use for visualizations. seed: The seed to use for reproducibility. Attributes: embeddings: The embeddings to process as a list of numpy arrays. embeddings_matrix: The embeddings to process concatenated into a single numpy array. embeddings_stats: The statistics of the embeddings (mean, std, min, max). slide_ids: The ids of the slides. If not provided, the slide ids are generated. slide_ids_matrix: The slide ids of each embedding concatenated into a single numpy array. cmap: The colormap to use for visualizations. seed: The seed to use for reproducibility. cluster_model: The clustering model used to cluster the embeddings. It is set using the [create_cluster_model][prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model] method. n_clusters: The number of clusters in the clustering model. It is set using the [create_cluster_model][prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model] method. cluster_colors: The colors of the clusters in the clustering model. It is set using the [create_cluster_model][prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model] method. eps: A small value to avoid division by zero. \"\"\" self . embeddings = self . load_embeddings ( embeddings ) self . embeddings_names = self . load_embeddings_names ( embeddings_names ) self . embeddings_matrix = np . concatenate ( self . embeddings , axis = 0 ) self . embeddings_stats = self . compute_embeddings_stats ( self . embeddings_matrix ) self . slide_ids = ( np . array ( slide_ids ) if slide_ids is not None else np . arange ( len ( embeddings )) ) self . slide_ids_matrix = np . concatenate ( [ np . repeat ( slide_id , len ( emb )) for slide_id , emb in zip ( self . slide_ids , self . embeddings ) ], axis = 0 , ) self . cmap = cmap self . seed = seed self . cluster_model = None self . n_clusters = None self . cluster_colors = None self . eps = 1e-6 compute_embeddings_stats ( embeddings_matrix ) staticmethod Compute the statistics of an input embeddings matrix. Parameters: Name Type Description Default embeddings_matrix ndarray The embeddings matrix to compute the statistics of. required Returns: Type Description dict [ str , ndarray ] The statistics of the embeddings matrix (mean, std, min, max) as a dictionary. Source code in src/prismtoolbox/wsiemb/processing.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @staticmethod def compute_embeddings_stats ( embeddings_matrix : np . ndarray ) -> dict [ str , np . ndarray ]: \"\"\"Compute the statistics of an input embeddings matrix. Args: embeddings_matrix: The embeddings matrix to compute the statistics of. Returns: The statistics of the embeddings matrix (mean, std, min, max) as a dictionary. \"\"\" return { \"mean\" : np . mean ( embeddings_matrix , axis = 0 ), \"std\" : np . std ( embeddings_matrix , axis = 0 ), \"min\" : np . min ( embeddings_matrix , axis = 0 ), \"max\" : np . max ( embeddings_matrix , axis = 0 ), } create_cluster_model ( model_name , normalize = True , n_samples = None , selected_features = None , ** kwargs ) Create a clustering model trained on the embeddings matrix. The resulting model is stored in self.cluster_model. Parameters: Name Type Description Default model_name str The clustering model to use. Possible models are: \" kmeans \" \" kmeans_mini_batch \" required normalize bool Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. True n_samples int | float | None The number of samples to subsample. If None, the whole embeddings matrix is used. None selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None **kwargs Additional arguments for the clustering model (see the documentation of the clustering model). {} Source code in src/prismtoolbox/wsiemb/processing.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def create_cluster_model ( self , model_name : str , normalize : bool = True , n_samples : int | float | None = None , selected_features : list [ str ] | None = None , ** kwargs , ): \"\"\"Create a clustering model trained on the embeddings matrix. The resulting model is stored in self.cluster_model. Args: model_name: The clustering model to use. Possible models are: - \"[kmeans](https://scikit-learn.org/stable/modules/generated/ sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\" - \"[kmeans_mini_batch](https://scikit-learn.org/stable/modules/generated/ sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans)\" normalize: Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. n_samples: The number of samples to subsample. If None, the whole embeddings matrix is used. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. **kwargs: Additional arguments for the clustering model (see the documentation of the clustering model). \"\"\" if model_name == \"kmeans\" : self . cluster_model = skl_cluster . KMeans ( ** kwargs ) elif model_name == \"kmeans_mini_batch\" : self . cluster_model = skl_cluster . MiniBatchKMeans ( ** kwargs ) else : raise ValueError ( f \"model { model_name } not implemented\" ) emb_mean , emb_std = ( ( self . embeddings_stats [ \"mean\" ], self . embeddings_stats [ \"std\" ]) if normalize else ( 0 , 1 ) ) if n_samples is not None : embeddings_matrix = self . return_subsampled_embeddings ( n_samples ) else : embeddings_matrix = self . embeddings_matrix embeddings_matrix = ( embeddings_matrix - emb_mean ) / ( emb_std + self . eps ) if selected_features is not None : selected_feats = np . array ( [ i for i , name in enumerate ( self . embeddings_names ) if name in selected_features ] ) embeddings_matrix = embeddings_matrix [:, selected_feats ] self . cluster_model . fit ( embeddings_matrix ) self . n_clusters = self . cluster_model . n_clusters self . cluster_colors = get_colors_from_cmap ( self . cmap , self . n_clusters ) export_clusters_to_qupath ( WSI_object , save_dir , normalize = True , selected_features = None ) Export the clusters as polygons to a geojson file for a slide. Parameters: Name Type Description Default WSI_object WSI An instance of the WSI class created from the slide. required save_dir str The directory to save the geojson file to. required normalize bool Whether the embeddings were normalized or not when creating the cluster model. True selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None Source code in src/prismtoolbox/wsiemb/processing.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 def export_clusters_to_qupath ( self , WSI_object : WSI , save_dir : str , normalize : bool = True , selected_features : list [ str ] | None = None , ): \"\"\"Export the clusters as polygons to a geojson file for a slide. Args: WSI_object: An instance of the WSI class created from the slide. save_dir: The directory to save the geojson file to. normalize: Whether the embeddings were normalized or not when creating the cluster model. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. \"\"\" assert ( self . n_clusters is not None and self . cluster_model is not None and self . cluster_colors is not None ), \"no cluster model created, please create a cluster model first\" cluster_assignments = self . get_cluster_assignments_for_slide ( WSI_object . slide_name , normalize , selected_features , ) if WSI_object . coords is None : raise ValueError ( \"WSI object has no coordinates, please make sure the WSI object is properly initialized\" ) idx = np . arange ( len ( WSI_object . coords )) assert len ( cluster_assignments ) == len ( idx ), \"Number of cluster assignments and number of patches do not match\" for cluster in range ( self . n_clusters ): WSI_object . save_patches ( save_dir , file_format = \"geojson\" , selected_idx = idx [ cluster_assignments == cluster ], merge = True , label = f \"cluster_ { cluster } \" , color = self . cluster_colors [ cluster ] . tolist (), append_to_existing_file = True , ) get_cluster_assignments_for_slide ( slide_id , normalize = True , selected_features = None ) Get the cluster assignments for a specific slide. Requires a cluster model to be created first with the create_cluster_model method. Parameters: Name Type Description Default slide_id str The id of the slide to get the cluster assignments for. required normalize bool Whether the embeddings were normalized or not when creating the cluster model. True selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None Returns: Type Description ndarray The cluster assignments for the slide. Source code in src/prismtoolbox/wsiemb/processing.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def get_cluster_assignments_for_slide ( self , slide_id : str , normalize : bool = True , selected_features : list [ str ] | None = None , ) -> np . ndarray : \"\"\"Get the cluster assignments for a specific slide. Requires a cluster model to be created first with the [create_cluster_model][prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model] method. Args: slide_id: The id of the slide to get the cluster assignments for. normalize: Whether the embeddings were normalized or not when creating the cluster model. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. Returns: The cluster assignments for the slide. \"\"\" assert ( self . cluster_model is not None ), \"no cluster model created, please create a cluster model first\" if slide_id not in self . slide_ids : raise ValueError ( f \"slide { slide_id } not found in slide ids\" ) idx = np . where ( self . slide_ids == slide_id )[ 0 ] . item () embeddings = self . embeddings [ idx ] emb_mean , emb_std = ( ( self . embeddings_stats [ \"mean\" ], self . embeddings_stats [ \"std\" ]) if normalize else ( 0 , 1 ) ) embeddings = ( embeddings - emb_mean ) / ( emb_std + self . eps ) if selected_features is not None : selected_feats = np . array ( [ i for i , name in enumerate ( self . embeddings_names ) if name in selected_features ] ) embeddings = embeddings [:, selected_feats ] return self . cluster_model . predict ( embeddings ) get_cluster_percentages_for_slide ( slide_id , normalize = True , selected_features = None ) Get the cluster percentages for a specific slide. Requires a cluster model to be created first with the create_cluster_model method. Parameters: Name Type Description Default slide_id str The id of the slide to get the cluster percentages for. required normalize bool Whether the embeddings were normalized or not when creating the cluster model. True selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None Returns: Type Description dict [ str , float ] The cluster percentages for the slide. Source code in src/prismtoolbox/wsiemb/processing.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def get_cluster_percentages_for_slide ( self , slide_id : str , normalize : bool = True , selected_features : list [ str ] | None = None , ) -> dict [ str , float ]: \"\"\"Get the cluster percentages for a specific slide. Requires a cluster model to be created first with the [create_cluster_model][prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model] method. Args: slide_id: The id of the slide to get the cluster percentages for. normalize: Whether the embeddings were normalized or not when creating the cluster model. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. Returns: The cluster percentages for the slide. \"\"\" assert self . n_clusters is not None , \"no cluster model created, please create a cluster model first\" cluster_assignments = self . get_cluster_assignments_for_slide ( slide_id , normalize , selected_features ) cluster_percentage = {} for cluster in range ( self . n_clusters ): cluster_percentage [ f \"cluster_ { cluster } \" ] = ( cluster_assignments == cluster ) . sum () / len ( cluster_assignments ) return cluster_percentage get_embedding_for_slide ( slide_id , normalize = True ) Get the embeddings for a specific slide. Parameters: Name Type Description Default slide_id str The id of the slide to get the embeddings for. required normalize bool Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. True Returns: Type Description ndarray The embeddings for the slide. Source code in src/prismtoolbox/wsiemb/processing.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def get_embedding_for_slide ( self , slide_id : str , normalize : bool = True ) -> np . ndarray : \"\"\"Get the embeddings for a specific slide. Args: slide_id: The id of the slide to get the embeddings for. normalize: Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. Returns: The embeddings for the slide. \"\"\" if slide_id not in self . slide_ids : raise ValueError ( f \"slide { slide_id } not found in slide ids\" ) idx = np . where ( self . slide_ids == slide_id )[ 0 ] . item () emb_mean , emb_std = ( ( self . embeddings_stats [ \"mean\" ], self . embeddings_stats [ \"std\" ]) if normalize else ( 0 , 1 ) ) return ( self . embeddings [ idx ] - emb_mean ) / ( emb_std + self . eps ) get_optimal_number_clusters ( model_name , normalize = True , metric_name = 'davies_bouldin' , min_clusters = 2 , max_clusters = 10 , with_scores = True , n_samples = None , selected_features = None , ** kwargs ) Compute the optimal number of clusters for the embeddings. Parameters: Name Type Description Default model_name str The clustering model to use. See compute_optimal_number_clusters for the available models. required normalize bool Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. True metric_name str The metric to use to compute the optimal number of clusters. See compute_optimal_number_clusters for the available metrics. 'davies_bouldin' min_clusters int The minimum number of clusters to consider. 2 max_clusters int The maximum number of clusters to consider. 10 with_scores bool Whether to return the scores or not. True n_samples int | float | None The number of samples to subsample. If None, the whole embeddings matrix is used. None selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None **kwargs Additional arguments for the clustering model (see the documentation of the clustering model). {} Returns: Type Description int | tuple [ int , list [ float ]] The optimal number of clusters according to the metric. If with_scores, it also returns the scores. Source code in src/prismtoolbox/wsiemb/processing.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def get_optimal_number_clusters ( self , model_name : str , normalize : bool = True , metric_name : str = \"davies_bouldin\" , min_clusters : int = 2 , max_clusters : int = 10 , with_scores : bool = True , n_samples : int | float | None = None , selected_features : list [ str ] | None = None , ** kwargs , ) -> int | tuple [ int , list [ float ]]: \"\"\"Compute the optimal number of clusters for the embeddings. Args: model_name: The clustering model to use. See [compute_optimal_number_clusters][prismtoolbox.wsiemb.emb_utils.compute_optimal_number_clusters] for the available models. normalize: Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. metric_name: The metric to use to compute the optimal number of clusters. See [compute_optimal_number_clusters][prismtoolbox.wsiemb.emb_utils.compute_optimal_number_clusters] for the available metrics. min_clusters: The minimum number of clusters to consider. max_clusters: The maximum number of clusters to consider. with_scores: Whether to return the scores or not. n_samples: The number of samples to subsample. If None, the whole embeddings matrix is used. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. **kwargs: Additional arguments for the clustering model (see the documentation of the clustering model). Returns: The optimal number of clusters according to the metric. If with_scores, it also returns the scores. \"\"\" emb_mean , emb_std = ( ( self . embeddings_stats [ \"mean\" ], self . embeddings_stats [ \"std\" ]) if normalize else ( 0 , 1 ) ) if n_samples is not None : embeddings_matrix = self . return_subsampled_embeddings ( n_samples ) else : embeddings_matrix = self . embeddings_matrix embeddings_matrix = ( embeddings_matrix - emb_mean ) / ( emb_std + self . eps ) if selected_features is not None : selected_feats = np . array ( [ i for i , name in enumerate ( self . embeddings_names ) if name in selected_features ] ) embeddings_matrix = embeddings_matrix [:, selected_feats ] optimal_number , scores = compute_optimal_number_clusters ( embeddings_matrix , model_name , metric_name = metric_name , min_clusters = min_clusters , max_clusters = max_clusters , ** kwargs , ) if with_scores : return optimal_number , scores else : return optimal_number import_cluster_model ( input_path ) Import the clustering model from a pickle file. Parameters: Name Type Description Default input_path str The path to import the clustering model from. required Source code in src/prismtoolbox/wsiemb/processing.py 534 535 536 537 538 539 540 541 542 def import_cluster_model ( self , input_path : str ): \"\"\"Import the clustering model from a pickle file. Args: input_path: The path to import the clustering model from. \"\"\" self . cluster_model = load_obj_with_pickle ( input_path ) self . n_clusters = self . cluster_model . n_clusters self . cluster_colors = get_colors_from_cmap ( self . cmap , self . n_clusters ) load_embeddings ( embeddings ) staticmethod Process the embeddings to load them as numpy arrays. Parameters: Name Type Description Default embeddings list [ ndarray | Tensor | str ] The embeddings to process. Can be a list of numpy arrays, torch tensors or paths to embeddings. required Returns: Type Description list [ ndarray ] The embeddings loaded as numpy arrays. Source code in src/prismtoolbox/wsiemb/processing.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @staticmethod def load_embeddings ( embeddings : list [ np . ndarray | torch . Tensor | str ], ) -> list [ np . ndarray ]: \"\"\"Process the embeddings to load them as numpy arrays. Args: embeddings: The embeddings to process. Can be a list of numpy arrays, torch tensors or paths to embeddings. Returns: The embeddings loaded as numpy arrays. \"\"\" embeddings_loaded = [] for emb in embeddings : if isinstance ( emb , torch . Tensor ): emb = emb . numpy () elif isinstance ( emb , np . ndarray ): emb = emb elif isinstance ( emb , str ): emb = torch . load ( emb ) . numpy () else : raise ValueError ( \"embedding type not supported\" ) embeddings_loaded . append ( emb ) return embeddings_loaded return_subsampled_embeddings ( n_samples , by_slide = True , labels = None ) Creates a subsample version of the embeddings matrix. Parameters: Name Type Description Default n_samples int | float The number of samples to subsample. If float, it is the percentage of samples to subsample. If None, the whole embeddings matrix is used. required by_slide bool Whether to subsample by slide or not. True labels ndarray | None Labels to subsample along with the embeddings. None Returns: Type Description ndarray | tuple [ ndarray , ndarray ] The subsampled embeddings matrix. If labels, it also returns the subsampled labels. Source code in src/prismtoolbox/wsiemb/processing.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def return_subsampled_embeddings ( self , n_samples : int | float , by_slide : bool = True , labels : np . ndarray | None = None , ) -> np . ndarray | tuple [ np . ndarray , np . ndarray ]: \"\"\"Creates a subsample version of the embeddings matrix. Args: n_samples: The number of samples to subsample. If float, it is the percentage of samples to subsample. If None, the whole embeddings matrix is used. by_slide: Whether to subsample by slide or not. labels: Labels to subsample along with the embeddings. Returns: The subsampled embeddings matrix. If labels, it also returns the subsampled labels. \"\"\" rng = np . random . default_rng ( self . seed ) if by_slide : subsampled_idx = [] current_idx = 0 for emb in self . embeddings : if n_samples < 1 : n_samples_per_slide = int ( n_samples * len ( emb )) else : n_samples_per_slide = n_samples // len ( self . embeddings ) subsampled_idx . extend ( rng . choice ( len ( emb ), n_samples_per_slide , replace = False ) + current_idx ) current_idx += len ( emb ) log . info ( f \"Subsampled { n_samples } embeddings from each slide.\" ) else : if n_samples < 1 : n_samples = int ( n_samples * len ( self . embeddings_matrix )) subsampled_idx = rng . choice ( len ( self . embeddings_matrix ), n_samples , replace = False ) log . info ( f \"Subsampled { n_samples } embeddings.\" ) if labels is not None : assert len ( labels ) == len ( self . embeddings_matrix ), \"labels and embeddings have different lengths\" return ( self . embeddings_matrix [ subsampled_idx ], labels [ subsampled_idx ], ) else : return self . embeddings_matrix [ subsampled_idx ] save_cluster_model ( output_path ) Save the clustering model to a pickle file. Parameters: Name Type Description Default output_path str The path to save the clustering model to. required Source code in src/prismtoolbox/wsiemb/processing.py 526 527 528 529 530 531 532 def save_cluster_model ( self , output_path : str ): \"\"\"Save the clustering model to a pickle file. Args: output_path: The path to save the clustering model to. \"\"\" save_obj_with_pickle ( self . cluster_model , output_path ) scale_to_01_range ( x ) staticmethod Scale an array to the [0; 1] range. Parameters: Name Type Description Default x ndarray The array to scale. required Returns: Type Description ndarray The array scaled to the [0; 1] range. Source code in src/prismtoolbox/wsiemb/processing.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 @staticmethod def scale_to_01_range ( x : np . ndarray ) -> np . ndarray : \"\"\"Scale an array to the [0; 1] range. Args: x: The array to scale. Returns: The array scaled to the [0; 1] range. \"\"\" # compute the distribution range value_range = np . max ( x ) - np . min ( x ) # move the distribution so that it starts from zero # by extracting the minimal value from all its values starts_from_zero = x - np . min ( x ) # make the distribution fit [0; 1] by dividing by its range return starts_from_zero / value_range visualize ( model_name , labels = None , n_samples = None , selected_features = None , ** kwargs ) Visualize the embeddings using a dimensionality reduction model. Parameters: Name Type Description Default model_name str The name of the dimensionality reduction model to use. Possible models are: \" PCA \" \" TSNE \" \" UMAP \" required labels ndarray | None The labels to use for the visualization. If None, no labels are used. None n_samples int | float | None The number of samples to subsample. If None, the whole embeddings matrix is used. None selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None **kwargs Additional arguments for the dimensionality reduction model (see the documentation of the dimensionality reduction model). {} Source code in src/prismtoolbox/wsiemb/processing.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 def visualize ( self , model_name : str , labels : np . ndarray | None = None , n_samples : int | float | None = None , selected_features : list [ str ] | None = None , ** kwargs , ): \"\"\"Visualize the embeddings using a dimensionality reduction model. Args: model_name: The name of the dimensionality reduction model to use. Possible models are: - \"[PCA](https://scikit-learn.org/stable/modules/generated/ sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\" - \"[TSNE](https://scikit-learn.org/stable/modules/generated/ sklearn.manifold.TSNE.html#sklearn.manifold.TSNE)\" - \"[UMAP](https://umap-learn.readthedocs.io/en/latest/)\" labels: The labels to use for the visualization. If None, no labels are used. n_samples: The number of samples to subsample. If None, the whole embeddings matrix is used. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. **kwargs: Additional arguments for the dimensionality reduction model (see the documentation of the dimensionality reduction model). \"\"\" if model_name == \"PCA\" : dimensionality_reduction_model = skl_decomposition . PCA ( n_components = 2 , ** kwargs ) elif model_name == \"TSNE\" : dimensionality_reduction_model = skl_manifold . TSNE ( n_components = 2 , ** kwargs ) elif model_name == \"UMAP\" : dimensionality_reduction_model = umap . UMAP ( n_components = 2 , ** kwargs ) else : raise ValueError ( f \"model { model_name } not implemented\" ) if n_samples is not None : subsampled_arrays = self . return_subsampled_embeddings ( n_samples , labels = labels ) embeddings_matrix = ( subsampled_arrays if labels is None else subsampled_arrays [ 0 ] ) labels = subsampled_arrays [ 1 ] if labels is not None else None else : embeddings_matrix = self . embeddings_matrix if selected_features is not None : selected_feats = np . array ( [ i for i , name in enumerate ( self . embeddings_names ) if name in selected_features ] ) embeddings_matrix = embeddings_matrix [:, selected_feats ] embeddings_reduced = dimensionality_reduction_model . fit_transform ( embeddings_matrix ) plot_scatter ( self . scale_to_01_range ( embeddings_reduced [:, 0 ]), self . scale_to_01_range ( embeddings_reduced [:, 1 ]), self . cmap , labels , )","title":"prismtoolbox.wsiemb.processing"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor","text":"summary Parameters: Name Type Description Default embeddings list [ ndarray | Tensor | str ] The embeddings to process. Can be a list of numpy arrays, torch tensors or paths to embeddings. required embeddings_names list [ str ] | str | None The names of the embeddings. Should be a list of strings with the same length as the embeddings, None slide_ids list [ str ] | None The ids of the slides. None cmap str The colormap to use for visualizations. 'Set1' seed int The seed to use for reproducibility. 123 Attributes: Name Type Description embeddings The embeddings to process as a list of numpy arrays. embeddings_matrix The embeddings to process concatenated into a single numpy array. embeddings_stats The statistics of the embeddings (mean, std, min, max). slide_ids The ids of the slides. If not provided, the slide ids are generated. slide_ids_matrix The slide ids of each embedding concatenated into a single numpy array. cmap The colormap to use for visualizations. seed The seed to use for reproducibility. cluster_model The clustering model used to cluster the embeddings. It is set using the n_clusters The number of clusters in the clustering model. It is set using the cluster_colors The colors of the clusters in the clustering model. It is set using the eps A small value to avoid division by zero. Source code in src/prismtoolbox/wsiemb/processing.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , embeddings : list [ np . ndarray | torch . Tensor | str ], embeddings_names : list [ str ] | str | None = None , slide_ids : list [ str ] | None = None , cmap : str = \"Set1\" , seed : int = 123 , ): \"\"\"_summary_ Args: embeddings: The embeddings to process. Can be a list of numpy arrays, torch tensors or paths to embeddings. embeddings_names: The names of the embeddings. Should be a list of strings with the same length as the embeddings, or the path to a file containing the names. If not provided, the names are generated. slide_ids: The ids of the slides. cmap: The colormap to use for visualizations. seed: The seed to use for reproducibility. Attributes: embeddings: The embeddings to process as a list of numpy arrays. embeddings_matrix: The embeddings to process concatenated into a single numpy array. embeddings_stats: The statistics of the embeddings (mean, std, min, max). slide_ids: The ids of the slides. If not provided, the slide ids are generated. slide_ids_matrix: The slide ids of each embedding concatenated into a single numpy array. cmap: The colormap to use for visualizations. seed: The seed to use for reproducibility. cluster_model: The clustering model used to cluster the embeddings. It is set using the [create_cluster_model][prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model] method. n_clusters: The number of clusters in the clustering model. It is set using the [create_cluster_model][prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model] method. cluster_colors: The colors of the clusters in the clustering model. It is set using the [create_cluster_model][prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model] method. eps: A small value to avoid division by zero. \"\"\" self . embeddings = self . load_embeddings ( embeddings ) self . embeddings_names = self . load_embeddings_names ( embeddings_names ) self . embeddings_matrix = np . concatenate ( self . embeddings , axis = 0 ) self . embeddings_stats = self . compute_embeddings_stats ( self . embeddings_matrix ) self . slide_ids = ( np . array ( slide_ids ) if slide_ids is not None else np . arange ( len ( embeddings )) ) self . slide_ids_matrix = np . concatenate ( [ np . repeat ( slide_id , len ( emb )) for slide_id , emb in zip ( self . slide_ids , self . embeddings ) ], axis = 0 , ) self . cmap = cmap self . seed = seed self . cluster_model = None self . n_clusters = None self . cluster_colors = None self . eps = 1e-6","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;EmbeddingProcessor"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.compute_embeddings_stats","text":"Compute the statistics of an input embeddings matrix. Parameters: Name Type Description Default embeddings_matrix ndarray The embeddings matrix to compute the statistics of. required Returns: Type Description dict [ str , ndarray ] The statistics of the embeddings matrix (mean, std, min, max) as a dictionary. Source code in src/prismtoolbox/wsiemb/processing.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @staticmethod def compute_embeddings_stats ( embeddings_matrix : np . ndarray ) -> dict [ str , np . ndarray ]: \"\"\"Compute the statistics of an input embeddings matrix. Args: embeddings_matrix: The embeddings matrix to compute the statistics of. Returns: The statistics of the embeddings matrix (mean, std, min, max) as a dictionary. \"\"\" return { \"mean\" : np . mean ( embeddings_matrix , axis = 0 ), \"std\" : np . std ( embeddings_matrix , axis = 0 ), \"min\" : np . min ( embeddings_matrix , axis = 0 ), \"max\" : np . max ( embeddings_matrix , axis = 0 ), }","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;compute_embeddings_stats"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model","text":"Create a clustering model trained on the embeddings matrix. The resulting model is stored in self.cluster_model. Parameters: Name Type Description Default model_name str The clustering model to use. Possible models are: \" kmeans \" \" kmeans_mini_batch \" required normalize bool Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. True n_samples int | float | None The number of samples to subsample. If None, the whole embeddings matrix is used. None selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None **kwargs Additional arguments for the clustering model (see the documentation of the clustering model). {} Source code in src/prismtoolbox/wsiemb/processing.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def create_cluster_model ( self , model_name : str , normalize : bool = True , n_samples : int | float | None = None , selected_features : list [ str ] | None = None , ** kwargs , ): \"\"\"Create a clustering model trained on the embeddings matrix. The resulting model is stored in self.cluster_model. Args: model_name: The clustering model to use. Possible models are: - \"[kmeans](https://scikit-learn.org/stable/modules/generated/ sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\" - \"[kmeans_mini_batch](https://scikit-learn.org/stable/modules/generated/ sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans)\" normalize: Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. n_samples: The number of samples to subsample. If None, the whole embeddings matrix is used. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. **kwargs: Additional arguments for the clustering model (see the documentation of the clustering model). \"\"\" if model_name == \"kmeans\" : self . cluster_model = skl_cluster . KMeans ( ** kwargs ) elif model_name == \"kmeans_mini_batch\" : self . cluster_model = skl_cluster . MiniBatchKMeans ( ** kwargs ) else : raise ValueError ( f \"model { model_name } not implemented\" ) emb_mean , emb_std = ( ( self . embeddings_stats [ \"mean\" ], self . embeddings_stats [ \"std\" ]) if normalize else ( 0 , 1 ) ) if n_samples is not None : embeddings_matrix = self . return_subsampled_embeddings ( n_samples ) else : embeddings_matrix = self . embeddings_matrix embeddings_matrix = ( embeddings_matrix - emb_mean ) / ( emb_std + self . eps ) if selected_features is not None : selected_feats = np . array ( [ i for i , name in enumerate ( self . embeddings_names ) if name in selected_features ] ) embeddings_matrix = embeddings_matrix [:, selected_feats ] self . cluster_model . fit ( embeddings_matrix ) self . n_clusters = self . cluster_model . n_clusters self . cluster_colors = get_colors_from_cmap ( self . cmap , self . n_clusters )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;create_cluster_model"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.export_clusters_to_qupath","text":"Export the clusters as polygons to a geojson file for a slide. Parameters: Name Type Description Default WSI_object WSI An instance of the WSI class created from the slide. required save_dir str The directory to save the geojson file to. required normalize bool Whether the embeddings were normalized or not when creating the cluster model. True selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None Source code in src/prismtoolbox/wsiemb/processing.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 def export_clusters_to_qupath ( self , WSI_object : WSI , save_dir : str , normalize : bool = True , selected_features : list [ str ] | None = None , ): \"\"\"Export the clusters as polygons to a geojson file for a slide. Args: WSI_object: An instance of the WSI class created from the slide. save_dir: The directory to save the geojson file to. normalize: Whether the embeddings were normalized or not when creating the cluster model. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. \"\"\" assert ( self . n_clusters is not None and self . cluster_model is not None and self . cluster_colors is not None ), \"no cluster model created, please create a cluster model first\" cluster_assignments = self . get_cluster_assignments_for_slide ( WSI_object . slide_name , normalize , selected_features , ) if WSI_object . coords is None : raise ValueError ( \"WSI object has no coordinates, please make sure the WSI object is properly initialized\" ) idx = np . arange ( len ( WSI_object . coords )) assert len ( cluster_assignments ) == len ( idx ), \"Number of cluster assignments and number of patches do not match\" for cluster in range ( self . n_clusters ): WSI_object . save_patches ( save_dir , file_format = \"geojson\" , selected_idx = idx [ cluster_assignments == cluster ], merge = True , label = f \"cluster_ { cluster } \" , color = self . cluster_colors [ cluster ] . tolist (), append_to_existing_file = True , )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;export_clusters_to_qupath"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.get_cluster_assignments_for_slide","text":"Get the cluster assignments for a specific slide. Requires a cluster model to be created first with the create_cluster_model method. Parameters: Name Type Description Default slide_id str The id of the slide to get the cluster assignments for. required normalize bool Whether the embeddings were normalized or not when creating the cluster model. True selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None Returns: Type Description ndarray The cluster assignments for the slide. Source code in src/prismtoolbox/wsiemb/processing.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def get_cluster_assignments_for_slide ( self , slide_id : str , normalize : bool = True , selected_features : list [ str ] | None = None , ) -> np . ndarray : \"\"\"Get the cluster assignments for a specific slide. Requires a cluster model to be created first with the [create_cluster_model][prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model] method. Args: slide_id: The id of the slide to get the cluster assignments for. normalize: Whether the embeddings were normalized or not when creating the cluster model. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. Returns: The cluster assignments for the slide. \"\"\" assert ( self . cluster_model is not None ), \"no cluster model created, please create a cluster model first\" if slide_id not in self . slide_ids : raise ValueError ( f \"slide { slide_id } not found in slide ids\" ) idx = np . where ( self . slide_ids == slide_id )[ 0 ] . item () embeddings = self . embeddings [ idx ] emb_mean , emb_std = ( ( self . embeddings_stats [ \"mean\" ], self . embeddings_stats [ \"std\" ]) if normalize else ( 0 , 1 ) ) embeddings = ( embeddings - emb_mean ) / ( emb_std + self . eps ) if selected_features is not None : selected_feats = np . array ( [ i for i , name in enumerate ( self . embeddings_names ) if name in selected_features ] ) embeddings = embeddings [:, selected_feats ] return self . cluster_model . predict ( embeddings )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_cluster_assignments_for_slide"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.get_cluster_percentages_for_slide","text":"Get the cluster percentages for a specific slide. Requires a cluster model to be created first with the create_cluster_model method. Parameters: Name Type Description Default slide_id str The id of the slide to get the cluster percentages for. required normalize bool Whether the embeddings were normalized or not when creating the cluster model. True selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None Returns: Type Description dict [ str , float ] The cluster percentages for the slide. Source code in src/prismtoolbox/wsiemb/processing.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def get_cluster_percentages_for_slide ( self , slide_id : str , normalize : bool = True , selected_features : list [ str ] | None = None , ) -> dict [ str , float ]: \"\"\"Get the cluster percentages for a specific slide. Requires a cluster model to be created first with the [create_cluster_model][prismtoolbox.wsiemb.processing.EmbeddingProcessor.create_cluster_model] method. Args: slide_id: The id of the slide to get the cluster percentages for. normalize: Whether the embeddings were normalized or not when creating the cluster model. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. Returns: The cluster percentages for the slide. \"\"\" assert self . n_clusters is not None , \"no cluster model created, please create a cluster model first\" cluster_assignments = self . get_cluster_assignments_for_slide ( slide_id , normalize , selected_features ) cluster_percentage = {} for cluster in range ( self . n_clusters ): cluster_percentage [ f \"cluster_ { cluster } \" ] = ( cluster_assignments == cluster ) . sum () / len ( cluster_assignments ) return cluster_percentage","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_cluster_percentages_for_slide"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.get_embedding_for_slide","text":"Get the embeddings for a specific slide. Parameters: Name Type Description Default slide_id str The id of the slide to get the embeddings for. required normalize bool Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. True Returns: Type Description ndarray The embeddings for the slide. Source code in src/prismtoolbox/wsiemb/processing.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def get_embedding_for_slide ( self , slide_id : str , normalize : bool = True ) -> np . ndarray : \"\"\"Get the embeddings for a specific slide. Args: slide_id: The id of the slide to get the embeddings for. normalize: Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. Returns: The embeddings for the slide. \"\"\" if slide_id not in self . slide_ids : raise ValueError ( f \"slide { slide_id } not found in slide ids\" ) idx = np . where ( self . slide_ids == slide_id )[ 0 ] . item () emb_mean , emb_std = ( ( self . embeddings_stats [ \"mean\" ], self . embeddings_stats [ \"std\" ]) if normalize else ( 0 , 1 ) ) return ( self . embeddings [ idx ] - emb_mean ) / ( emb_std + self . eps )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_embedding_for_slide"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.get_optimal_number_clusters","text":"Compute the optimal number of clusters for the embeddings. Parameters: Name Type Description Default model_name str The clustering model to use. See compute_optimal_number_clusters for the available models. required normalize bool Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. True metric_name str The metric to use to compute the optimal number of clusters. See compute_optimal_number_clusters for the available metrics. 'davies_bouldin' min_clusters int The minimum number of clusters to consider. 2 max_clusters int The maximum number of clusters to consider. 10 with_scores bool Whether to return the scores or not. True n_samples int | float | None The number of samples to subsample. If None, the whole embeddings matrix is used. None selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None **kwargs Additional arguments for the clustering model (see the documentation of the clustering model). {} Returns: Type Description int | tuple [ int , list [ float ]] The optimal number of clusters according to the metric. If with_scores, it also returns the scores. Source code in src/prismtoolbox/wsiemb/processing.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def get_optimal_number_clusters ( self , model_name : str , normalize : bool = True , metric_name : str = \"davies_bouldin\" , min_clusters : int = 2 , max_clusters : int = 10 , with_scores : bool = True , n_samples : int | float | None = None , selected_features : list [ str ] | None = None , ** kwargs , ) -> int | tuple [ int , list [ float ]]: \"\"\"Compute the optimal number of clusters for the embeddings. Args: model_name: The clustering model to use. See [compute_optimal_number_clusters][prismtoolbox.wsiemb.emb_utils.compute_optimal_number_clusters] for the available models. normalize: Whether to normalize the embeddings or not according to the mean and std of self.embeddings_stats. metric_name: The metric to use to compute the optimal number of clusters. See [compute_optimal_number_clusters][prismtoolbox.wsiemb.emb_utils.compute_optimal_number_clusters] for the available metrics. min_clusters: The minimum number of clusters to consider. max_clusters: The maximum number of clusters to consider. with_scores: Whether to return the scores or not. n_samples: The number of samples to subsample. If None, the whole embeddings matrix is used. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. **kwargs: Additional arguments for the clustering model (see the documentation of the clustering model). Returns: The optimal number of clusters according to the metric. If with_scores, it also returns the scores. \"\"\" emb_mean , emb_std = ( ( self . embeddings_stats [ \"mean\" ], self . embeddings_stats [ \"std\" ]) if normalize else ( 0 , 1 ) ) if n_samples is not None : embeddings_matrix = self . return_subsampled_embeddings ( n_samples ) else : embeddings_matrix = self . embeddings_matrix embeddings_matrix = ( embeddings_matrix - emb_mean ) / ( emb_std + self . eps ) if selected_features is not None : selected_feats = np . array ( [ i for i , name in enumerate ( self . embeddings_names ) if name in selected_features ] ) embeddings_matrix = embeddings_matrix [:, selected_feats ] optimal_number , scores = compute_optimal_number_clusters ( embeddings_matrix , model_name , metric_name = metric_name , min_clusters = min_clusters , max_clusters = max_clusters , ** kwargs , ) if with_scores : return optimal_number , scores else : return optimal_number","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_optimal_number_clusters"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.import_cluster_model","text":"Import the clustering model from a pickle file. Parameters: Name Type Description Default input_path str The path to import the clustering model from. required Source code in src/prismtoolbox/wsiemb/processing.py 534 535 536 537 538 539 540 541 542 def import_cluster_model ( self , input_path : str ): \"\"\"Import the clustering model from a pickle file. Args: input_path: The path to import the clustering model from. \"\"\" self . cluster_model = load_obj_with_pickle ( input_path ) self . n_clusters = self . cluster_model . n_clusters self . cluster_colors = get_colors_from_cmap ( self . cmap , self . n_clusters )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;import_cluster_model"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.load_embeddings","text":"Process the embeddings to load them as numpy arrays. Parameters: Name Type Description Default embeddings list [ ndarray | Tensor | str ] The embeddings to process. Can be a list of numpy arrays, torch tensors or paths to embeddings. required Returns: Type Description list [ ndarray ] The embeddings loaded as numpy arrays. Source code in src/prismtoolbox/wsiemb/processing.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @staticmethod def load_embeddings ( embeddings : list [ np . ndarray | torch . Tensor | str ], ) -> list [ np . ndarray ]: \"\"\"Process the embeddings to load them as numpy arrays. Args: embeddings: The embeddings to process. Can be a list of numpy arrays, torch tensors or paths to embeddings. Returns: The embeddings loaded as numpy arrays. \"\"\" embeddings_loaded = [] for emb in embeddings : if isinstance ( emb , torch . Tensor ): emb = emb . numpy () elif isinstance ( emb , np . ndarray ): emb = emb elif isinstance ( emb , str ): emb = torch . load ( emb ) . numpy () else : raise ValueError ( \"embedding type not supported\" ) embeddings_loaded . append ( emb ) return embeddings_loaded","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;load_embeddings"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.return_subsampled_embeddings","text":"Creates a subsample version of the embeddings matrix. Parameters: Name Type Description Default n_samples int | float The number of samples to subsample. If float, it is the percentage of samples to subsample. If None, the whole embeddings matrix is used. required by_slide bool Whether to subsample by slide or not. True labels ndarray | None Labels to subsample along with the embeddings. None Returns: Type Description ndarray | tuple [ ndarray , ndarray ] The subsampled embeddings matrix. If labels, it also returns the subsampled labels. Source code in src/prismtoolbox/wsiemb/processing.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def return_subsampled_embeddings ( self , n_samples : int | float , by_slide : bool = True , labels : np . ndarray | None = None , ) -> np . ndarray | tuple [ np . ndarray , np . ndarray ]: \"\"\"Creates a subsample version of the embeddings matrix. Args: n_samples: The number of samples to subsample. If float, it is the percentage of samples to subsample. If None, the whole embeddings matrix is used. by_slide: Whether to subsample by slide or not. labels: Labels to subsample along with the embeddings. Returns: The subsampled embeddings matrix. If labels, it also returns the subsampled labels. \"\"\" rng = np . random . default_rng ( self . seed ) if by_slide : subsampled_idx = [] current_idx = 0 for emb in self . embeddings : if n_samples < 1 : n_samples_per_slide = int ( n_samples * len ( emb )) else : n_samples_per_slide = n_samples // len ( self . embeddings ) subsampled_idx . extend ( rng . choice ( len ( emb ), n_samples_per_slide , replace = False ) + current_idx ) current_idx += len ( emb ) log . info ( f \"Subsampled { n_samples } embeddings from each slide.\" ) else : if n_samples < 1 : n_samples = int ( n_samples * len ( self . embeddings_matrix )) subsampled_idx = rng . choice ( len ( self . embeddings_matrix ), n_samples , replace = False ) log . info ( f \"Subsampled { n_samples } embeddings.\" ) if labels is not None : assert len ( labels ) == len ( self . embeddings_matrix ), \"labels and embeddings have different lengths\" return ( self . embeddings_matrix [ subsampled_idx ], labels [ subsampled_idx ], ) else : return self . embeddings_matrix [ subsampled_idx ]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;return_subsampled_embeddings"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.save_cluster_model","text":"Save the clustering model to a pickle file. Parameters: Name Type Description Default output_path str The path to save the clustering model to. required Source code in src/prismtoolbox/wsiemb/processing.py 526 527 528 529 530 531 532 def save_cluster_model ( self , output_path : str ): \"\"\"Save the clustering model to a pickle file. Args: output_path: The path to save the clustering model to. \"\"\" save_obj_with_pickle ( self . cluster_model , output_path )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;save_cluster_model"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.scale_to_01_range","text":"Scale an array to the [0; 1] range. Parameters: Name Type Description Default x ndarray The array to scale. required Returns: Type Description ndarray The array scaled to the [0; 1] range. Source code in src/prismtoolbox/wsiemb/processing.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 @staticmethod def scale_to_01_range ( x : np . ndarray ) -> np . ndarray : \"\"\"Scale an array to the [0; 1] range. Args: x: The array to scale. Returns: The array scaled to the [0; 1] range. \"\"\" # compute the distribution range value_range = np . max ( x ) - np . min ( x ) # move the distribution so that it starts from zero # by extracting the minimal value from all its values starts_from_zero = x - np . min ( x ) # make the distribution fit [0; 1] by dividing by its range return starts_from_zero / value_range","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;scale_to_01_range"},{"location":"reference/prismtoolbox/wsiemb/processing/#prismtoolbox.wsiemb.processing.EmbeddingProcessor.visualize","text":"Visualize the embeddings using a dimensionality reduction model. Parameters: Name Type Description Default model_name str The name of the dimensionality reduction model to use. Possible models are: \" PCA \" \" TSNE \" \" UMAP \" required labels ndarray | None The labels to use for the visualization. If None, no labels are used. None n_samples int | float | None The number of samples to subsample. If None, the whole embeddings matrix is used. None selected_features list [ str ] | None The names of the embeddings to subsample. If None, all embeddings are used. None **kwargs Additional arguments for the dimensionality reduction model (see the documentation of the dimensionality reduction model). {} Source code in src/prismtoolbox/wsiemb/processing.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 def visualize ( self , model_name : str , labels : np . ndarray | None = None , n_samples : int | float | None = None , selected_features : list [ str ] | None = None , ** kwargs , ): \"\"\"Visualize the embeddings using a dimensionality reduction model. Args: model_name: The name of the dimensionality reduction model to use. Possible models are: - \"[PCA](https://scikit-learn.org/stable/modules/generated/ sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\" - \"[TSNE](https://scikit-learn.org/stable/modules/generated/ sklearn.manifold.TSNE.html#sklearn.manifold.TSNE)\" - \"[UMAP](https://umap-learn.readthedocs.io/en/latest/)\" labels: The labels to use for the visualization. If None, no labels are used. n_samples: The number of samples to subsample. If None, the whole embeddings matrix is used. selected_features: The names of the embeddings to subsample. If None, all embeddings are used. **kwargs: Additional arguments for the dimensionality reduction model (see the documentation of the dimensionality reduction model). \"\"\" if model_name == \"PCA\" : dimensionality_reduction_model = skl_decomposition . PCA ( n_components = 2 , ** kwargs ) elif model_name == \"TSNE\" : dimensionality_reduction_model = skl_manifold . TSNE ( n_components = 2 , ** kwargs ) elif model_name == \"UMAP\" : dimensionality_reduction_model = umap . UMAP ( n_components = 2 , ** kwargs ) else : raise ValueError ( f \"model { model_name } not implemented\" ) if n_samples is not None : subsampled_arrays = self . return_subsampled_embeddings ( n_samples , labels = labels ) embeddings_matrix = ( subsampled_arrays if labels is None else subsampled_arrays [ 0 ] ) labels = subsampled_arrays [ 1 ] if labels is not None else None else : embeddings_matrix = self . embeddings_matrix if selected_features is not None : selected_feats = np . array ( [ i for i , name in enumerate ( self . embeddings_names ) if name in selected_features ] ) embeddings_matrix = embeddings_matrix [:, selected_feats ] embeddings_reduced = dimensionality_reduction_model . fit_transform ( embeddings_matrix ) plot_scatter ( self . scale_to_01_range ( embeddings_reduced [:, 0 ]), self . scale_to_01_range ( embeddings_reduced [:, 1 ]), self . cmap , labels , )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;visualize"},{"location":"reference/prismtoolbox/wsiemb/models/","text":"","title":"Index"},{"location":"reference/prismtoolbox/wsiemb/models/clam/","text":"","title":"Clam"},{"location":"reference/prismtoolbox/wsiemb/models/generic/","text":"","title":"Generic"},{"location":"reference/prismtoolbox/wsiemb/models/pathoduet/","text":"","title":"Pathoduet"},{"location":"reference/prismtoolbox/wsiemb/models/phikon/","text":"","title":"Phikon"},{"location":"reference/prismtoolbox/wsiemb/models/utils/","text":"","title":"Utils"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/","text":"","title":"Index"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/coca_model/","text":"","title":"Coca model"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/conch/","text":"","title":"Conch"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/transformer/","text":"LayerNorm Bases: LayerNorm Subclass torch's LayerNorm (with cast back to input dtype). LayerNormFp32 Bases: LayerNorm Subclass torch's LayerNorm to handle fp16 (by casting to float32 and back). PatchDropout ( prob , exclude_first_token = True ) Bases: Module https://arxiv.org/abs/2212.00794 Source code in src/prismtoolbox/wsiemb/models/conch_model/transformer.py 53 54 55 56 57 def __init__ ( self , prob , exclude_first_token = True ): super () . __init__ () assert 0 <= prob < 1.0 self . prob = prob self . exclude_first_token = exclude_first_token # exclude CLS token","title":"Transformer"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/transformer/#prismtoolbox.wsiemb.models.conch_model.transformer.LayerNorm","text":"Bases: LayerNorm Subclass torch's LayerNorm (with cast back to input dtype).","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;LayerNorm"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/transformer/#prismtoolbox.wsiemb.models.conch_model.transformer.LayerNormFp32","text":"Bases: LayerNorm Subclass torch's LayerNorm to handle fp16 (by casting to float32 and back).","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;LayerNormFp32"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/transformer/#prismtoolbox.wsiemb.models.conch_model.transformer.PatchDropout","text":"Bases: Module https://arxiv.org/abs/2212.00794 Source code in src/prismtoolbox/wsiemb/models/conch_model/transformer.py 53 54 55 56 57 def __init__ ( self , prob , exclude_first_token = True ): super () . __init__ () assert 0 <= prob < 1.0 self . prob = prob self . exclude_first_token = exclude_first_token # exclude CLS token","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;PatchDropout"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/utils/","text":"freeze_batch_norm_2d ( module , module_match = {}, name = '' ) Converts all BatchNorm2d and SyncBatchNorm layers of provided module into FrozenBatchNorm2d . If module is itself an instance of either BatchNorm2d or SyncBatchNorm , it is converted into FrozenBatchNorm2d and returned. Otherwise, the module is walked recursively and submodules are converted in place. Parameters: Name Type Description Default module Module Any PyTorch module. required module_match dict Dictionary of full module names to freeze (all if empty) {} name str Full module name (prefix) '' Returns: Type Description torch.nn.Module: Resulting module Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762 Source code in src/prismtoolbox/wsiemb/models/conch_model/utils.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def freeze_batch_norm_2d ( module , module_match = {}, name = \"\" ): \"\"\" Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and returned. Otherwise, the module is walked recursively and submodules are converted in place. Args: module (torch.nn.Module): Any PyTorch module. module_match (dict): Dictionary of full module names to freeze (all if empty) name (str): Full module name (prefix) Returns: torch.nn.Module: Resulting module Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762 \"\"\" res = module is_match = True if module_match : is_match = name in module_match if is_match and isinstance ( module , ( nn . modules . batchnorm . BatchNorm2d , nn . modules . batchnorm . SyncBatchNorm ) ): res = FrozenBatchNorm2d ( module . num_features ) res . num_features = module . num_features res . affine = module . affine if module . affine : res . weight . data = module . weight . data . clone () . detach () res . bias . data = module . bias . data . clone () . detach () res . running_mean . data = module . running_mean . data res . running_var . data = module . running_var . data res . eps = module . eps else : for child_name , child in module . named_children (): full_child_name = \".\" . join ([ name , child_name ]) if name else child_name new_child = freeze_batch_norm_2d ( child , module_match , full_child_name ) if new_child is not child : res . add_module ( child_name , new_child ) return res","title":"Utils"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/utils/#prismtoolbox.wsiemb.models.conch_model.utils.freeze_batch_norm_2d","text":"Converts all BatchNorm2d and SyncBatchNorm layers of provided module into FrozenBatchNorm2d . If module is itself an instance of either BatchNorm2d or SyncBatchNorm , it is converted into FrozenBatchNorm2d and returned. Otherwise, the module is walked recursively and submodules are converted in place. Parameters: Name Type Description Default module Module Any PyTorch module. required module_match dict Dictionary of full module names to freeze (all if empty) {} name str Full module name (prefix) '' Returns: Type Description torch.nn.Module: Resulting module Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762 Source code in src/prismtoolbox/wsiemb/models/conch_model/utils.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def freeze_batch_norm_2d ( module , module_match = {}, name = \"\" ): \"\"\" Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and returned. Otherwise, the module is walked recursively and submodules are converted in place. Args: module (torch.nn.Module): Any PyTorch module. module_match (dict): Dictionary of full module names to freeze (all if empty) name (str): Full module name (prefix) Returns: torch.nn.Module: Resulting module Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762 \"\"\" res = module is_match = True if module_match : is_match = name in module_match if is_match and isinstance ( module , ( nn . modules . batchnorm . BatchNorm2d , nn . modules . batchnorm . SyncBatchNorm ) ): res = FrozenBatchNorm2d ( module . num_features ) res . num_features = module . num_features res . affine = module . affine if module . affine : res . weight . data = module . weight . data . clone () . detach () res . bias . data = module . bias . data . clone () . detach () res . running_mean . data = module . running_mean . data res . running_var . data = module . running_var . data res . eps = module . eps else : for child_name , child in module . named_children (): full_child_name = \".\" . join ([ name , child_name ]) if name else child_name new_child = freeze_batch_norm_2d ( child , module_match , full_child_name ) if new_child is not child : res . add_module ( child_name , new_child ) return res","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;freeze_batch_norm_2d"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/vision_tower/","text":"VisualModel ( embed_dim_contrast , embed_dim_caption , trunk , image_size = 224 , proj = '' , proj_bias = False , drop = 0.0 , global_average_pool = False , use_attentional_pool_contrast = False , use_attentional_pool_caption = False , n_queries_contrast = 1 , n_queries_caption = 256 , attn_pooler_heads = 8 , norm_layer = nn . LayerNorm , output_tokens = False , trunk_kwargs = {}) Bases: Module Source code in src/prismtoolbox/wsiemb/models/conch_model/vision_tower.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , embed_dim_contrast , embed_dim_caption , trunk , image_size = 224 , proj = \"\" , proj_bias = False , drop = 0.0 , global_average_pool = False , use_attentional_pool_contrast = False , use_attentional_pool_caption = False , n_queries_contrast = 1 , n_queries_caption = 256 , attn_pooler_heads = 8 , norm_layer = nn . LayerNorm , output_tokens = False , trunk_kwargs = {}, ): super () . __init__ () self . trunk = trunk self . trunk_kwargs = trunk_kwargs self . image_size = to_2tuple ( image_size ) prev_chs = self . trunk . num_features head_layers = OrderedDict () # whether to use attentional pooling self . use_attentional_pool_contrast = use_attentional_pool_contrast self . use_attentional_pool_caption = use_attentional_pool_caption self . global_average_pool = global_average_pool self . output_tokens = output_tokens if use_attentional_pool_contrast : scale = prev_chs **- 0.5 self . attn_pool_contrast = AttentionalPooler ( d_model = embed_dim_contrast , context_dim = prev_chs , n_head = attn_pooler_heads , n_queries = n_queries_contrast , ) self . ln_contrast = norm_layer ( embed_dim_contrast ) self . proj_contrast = nn . Parameter ( scale * torch . randn ( embed_dim_contrast , embed_dim_contrast ) ) else : assert proj , \"projection layer needed if not using attentional pooling.\" # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used if proj == \"linear\" : head_layers [ \"drop\" ] = nn . Dropout ( drop ) head_layers [ \"proj\" ] = nn . Linear ( prev_chs , embed_dim_contrast , bias = proj_bias ) elif proj == \"mlp\" : head_layers [ \"mlp\" ] = Mlp ( prev_chs , 2 * embed_dim_contrast , embed_dim_contrast , drop = ( drop , 0 ), bias = ( True , proj_bias ), ) self . head = nn . Sequential ( head_layers ) if use_attentional_pool_caption : self . attn_pool_caption = AttentionalPooler ( d_model = embed_dim_caption , context_dim = prev_chs , n_head = attn_pooler_heads , n_queries = n_queries_caption , ) self . ln_caption = norm_layer ( embed_dim_caption ) lock ( unlocked_groups = 0 , freeze_bn_stats = False ) lock modules Args: unlocked_groups (int): leave last n layer groups unlocked (default: 0) Source code in src/prismtoolbox/wsiemb/models/conch_model/vision_tower.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def lock ( self , unlocked_groups = 0 , freeze_bn_stats = False ): \"\"\"lock modules Args: unlocked_groups (int): leave last n layer groups unlocked (default: 0) \"\"\" if not unlocked_groups : # lock full model for param in self . trunk . parameters (): param . requires_grad = False if freeze_bn_stats : freeze_batch_norm_2d ( self . trunk ) else : from timm.models.helpers import group_modules , group_parameters matcher = self . trunk . group_matcher () gparams = group_parameters ( self . trunk , matcher ) max_layer_id = max ( gparams . keys ()) max_layer_id = max_layer_id - unlocked_groups for group_idx in range ( max_layer_id + 1 ): group = gparams [ group_idx ] for param in group : self . trunk . get_parameter ( param ) . requires_grad = False if freeze_bn_stats : gmodules = group_modules ( self . trunk , matcher , reverse = True ) gmodules = { k for k , v in gmodules . items () if v <= max_layer_id } freeze_batch_norm_2d ( self . trunk , gmodules )","title":"Vision tower"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/vision_tower/#prismtoolbox.wsiemb.models.conch_model.vision_tower.VisualModel","text":"Bases: Module Source code in src/prismtoolbox/wsiemb/models/conch_model/vision_tower.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , embed_dim_contrast , embed_dim_caption , trunk , image_size = 224 , proj = \"\" , proj_bias = False , drop = 0.0 , global_average_pool = False , use_attentional_pool_contrast = False , use_attentional_pool_caption = False , n_queries_contrast = 1 , n_queries_caption = 256 , attn_pooler_heads = 8 , norm_layer = nn . LayerNorm , output_tokens = False , trunk_kwargs = {}, ): super () . __init__ () self . trunk = trunk self . trunk_kwargs = trunk_kwargs self . image_size = to_2tuple ( image_size ) prev_chs = self . trunk . num_features head_layers = OrderedDict () # whether to use attentional pooling self . use_attentional_pool_contrast = use_attentional_pool_contrast self . use_attentional_pool_caption = use_attentional_pool_caption self . global_average_pool = global_average_pool self . output_tokens = output_tokens if use_attentional_pool_contrast : scale = prev_chs **- 0.5 self . attn_pool_contrast = AttentionalPooler ( d_model = embed_dim_contrast , context_dim = prev_chs , n_head = attn_pooler_heads , n_queries = n_queries_contrast , ) self . ln_contrast = norm_layer ( embed_dim_contrast ) self . proj_contrast = nn . Parameter ( scale * torch . randn ( embed_dim_contrast , embed_dim_contrast ) ) else : assert proj , \"projection layer needed if not using attentional pooling.\" # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used if proj == \"linear\" : head_layers [ \"drop\" ] = nn . Dropout ( drop ) head_layers [ \"proj\" ] = nn . Linear ( prev_chs , embed_dim_contrast , bias = proj_bias ) elif proj == \"mlp\" : head_layers [ \"mlp\" ] = Mlp ( prev_chs , 2 * embed_dim_contrast , embed_dim_contrast , drop = ( drop , 0 ), bias = ( True , proj_bias ), ) self . head = nn . Sequential ( head_layers ) if use_attentional_pool_caption : self . attn_pool_caption = AttentionalPooler ( d_model = embed_dim_caption , context_dim = prev_chs , n_head = attn_pooler_heads , n_queries = n_queries_caption , ) self . ln_caption = norm_layer ( embed_dim_caption )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;VisualModel"},{"location":"reference/prismtoolbox/wsiemb/models/conch_model/vision_tower/#prismtoolbox.wsiemb.models.conch_model.vision_tower.VisualModel.lock","text":"lock modules Args: unlocked_groups (int): leave last n layer groups unlocked (default: 0) Source code in src/prismtoolbox/wsiemb/models/conch_model/vision_tower.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def lock ( self , unlocked_groups = 0 , freeze_bn_stats = False ): \"\"\"lock modules Args: unlocked_groups (int): leave last n layer groups unlocked (default: 0) \"\"\" if not unlocked_groups : # lock full model for param in self . trunk . parameters (): param . requires_grad = False if freeze_bn_stats : freeze_batch_norm_2d ( self . trunk ) else : from timm.models.helpers import group_modules , group_parameters matcher = self . trunk . group_matcher () gparams = group_parameters ( self . trunk , matcher ) max_layer_id = max ( gparams . keys ()) max_layer_id = max_layer_id - unlocked_groups for group_idx in range ( max_layer_id + 1 ): group = gparams [ group_idx ] for param in group : self . trunk . get_parameter ( param ) . requires_grad = False if freeze_bn_stats : gmodules = group_modules ( self . trunk , matcher , reverse = True ) gmodules = { k for k , v in gmodules . items () if v <= max_layer_id } freeze_batch_norm_2d ( self . trunk , gmodules )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;lock"}]}